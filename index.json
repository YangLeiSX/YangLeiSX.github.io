
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":" Lei Yang received the B.Eng. degree from Shanghai Jiao Tong University in 2021. He is currently pursuing the Ph.D. degree with the SEIEE, SJTU. His research interests include computer vision, visual speech recognition, and semantics segmentation.\n","date":1710460800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1710460800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Lei Yang received the B.Eng. degree from Shanghai Jiao Tong University in 2021. He is currently pursuing the Ph.D. degree with the SEIEE, SJTU. His research interests include computer vision, visual speech recognition, and semantics segmentation.","tags":null,"title":"Lei Yang","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Hugo Blox Builder’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://yangleisx.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Hugo Blox Builder's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Yi He","Lei Yang","Hanyi Wang","Yun Zhu","Shilin Wang"],"categories":null,"content":" In this paper, we propose a novel speaker-adaptive lipreading model. For front-end network, conv-based LoRA modules are used to adapt to speaker’s space features. For back-end network, a plug-and-play TAWL module is designed to learn temporal characteristics. An Adapter module is finally employed to bridge the adaptation knowledge from front-end and back-end. The experiments show that the proposed method achieve the state-of-the-art performance on both word-level and sentence-level dataset with fewer training parameters.\n","date":1710460800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1710460800,"objectID":"df2a78da2aa4ef86955dcc5be7591d49","permalink":"https://yangleisx.github.io/publication/icassp-udp/","publishdate":"2024-01-19T00:00:00Z","relpermalink":"/publication/icassp-udp/","section":"publication","summary":"A novel speaker-adaptive lipreading model is proposed.","tags":null,"title":"Speaker-Adaptive LipReading via Spatio-Temporal Information Learning","type":"publication"},{"authors":["Lei Yang"],"categories":["论文阅读"],"content":"论文题目：Shape Robust Text Detection with Progressive Scale Expansion Network\n作者：Wenhai Wang, Enze Xie, Xiang Li, Wenbo Hou, Tong Lu, Gang Yu, Shuai Shao\n会议/时间：CVPR2019\n链接：arxiv\n论文目标 在当前的文字检测算法在应用时有两个问题：当前的算法通常得到一个四边形边界框，难以检测任意形状的文本；如果两行文本距离比较近，有可能会被框选为同一个边界框。因此提出了PSENet，可以有效的解决上述的两个问题。\n相关工作 现有的基于CNN的文本检测模型可以分为两类：基于回归的方案和基于分割的方案。前者生成四边形的边界框，无法处理任意形状的文本，后者使用像素级的分类得到目标区域，但是很难区分开相聚比较近的目标。\n基于回归的文本检测方案大多是基于通用的目标检测模型，包括Faster R-CNN等。其他的文本检测模型还有TextBoxes、EAST等。大多数这一类的模型都需要设计Anchor而且由多个处理阶段组成，可能会导致性能比较差。基于分割的文本检测方案主要使用FCN，例如通过FCN获得热力图等，再进行后处理获得文本位置。\n本文思路/解决方案 本文提出的PSENet是基于分割的方案，每一个预测的分割称为kernel，形状相似但是大小不同，最后设计了一个基于BFS的渐进扩展算法，将原本的kernel扩展得到最终的预测分割。由于使用渐近扩展式的算法，对于最小尺寸的kernel，可以区分开距离较近的文本，同时也可以解决小尺寸分割难以覆盖完整文本的问题。\n模型结构是基于ResNet的FPN结构，从中选取不同大小的特征图连接得到混合特征图，最后通过卷积等操作得到不同尺寸的多个分割图，再经过尺寸扩展得到预测结果。\n在这个尺寸扩展算法中，首先选择了尺寸最小的分割图进行连通域分析作为kernel，然后将其他的分割图作为输入，通过Scale Expansion算法计算新的扩展后的kernel，最终得到结果。作者提到对于位于多个文本之间的像素，使用先来先服务的方式合并到不同的标签中，同时由于使用了渐进式的方法，这些边界上的重合并不会影响最终的处理结果，这可能也是作者选择多个不同尺寸的分割图渐近处理的原因。\n在训练过程中，为了获得不同尺寸的分割图，需要提供对应的标签供学习，作者使用了Vatti clipping algorithm来将最初的文本框标签收缩一定的像素得到这些标签。算法中使用到的像素值通过下面的公式计算得到。其中m为最小的缩放比例，n为不同尺寸的分割图的个数。\n$$\\begin{aligned} d_i = \\frac{\\mathrm{Area}(p_n)\\times(1-r_i^2)}{\\mathrm{Perimeter}(p_n)}\\notag\\\\ r_i = 1 - \\frac{(1-m)\\times(n-i)}{n-1} \\end{aligned}$$ 在实验中作者使用了Dice Coefficient作为模型的评价指标，使用了完整尺寸的标签和缩放后的标签上的Dice系数作为损失函数来指导模型的学习。其中对于完整尺寸的标签，使用Online Hard Example Mining(OHEM)来获得一个mask协助训练。 $$\\begin{aligned} L \u0026amp;= \\lambda L_c + (1-\\lambda)L_s\\notag\\\\ L_c \u0026amp;= 1 - D(S_n\\cdot M, G_n \\cdot M)\\notag\\\\ L_s \u0026amp;= 1 - \\frac{\\sum\\limits^{n-1} D(S_i\\cdot W, G_i \\cdot W)}{n-1}\\notag\\\\ W_{x,y} \u0026amp;= \\left\\{ \\begin{align} 1,\u0026amp; \\quad if\\ S_{n,x,y} \\geq0.5;\\notag\\\\ 0,\u0026amp; \\quad otherwise\\notag\\\\ \\end{align} \\right. \\end{aligned}$$ 结果 实验包括了四个数据集：CTW1500、Total-Text、ICDAR2015和ICDAR2017MLT。模型使用预训练好的ResNet，在IC17-MLT上训练，而且没有使用另外的人造数据集。实验讨论的结果包括：\n最小尺寸的kernel并不能直接作为模型的输出，模型的F-measure结果比较差，而且文本框内容的识别结果也比较差。 对于最小缩放比例m的选择，选择太大或者太小都会导致性能的下降。 分割图的个数n增加时，性能会有一定的上升，但是不能无限制增大，在n大于5之后性能提升不大。 修改模型骨架，例如增加ResNet的层数也会提升模型的性能。 总结 使用了基于FPN的结构，将不同尺度的特征图上采样并连接在一起。\n获得不同尺度下的分割图，再从小到大渐进式的合并，不仅可以检测到任意形状的文本，也可以避免将距离比较近的文本识别为同一对象。\n","date":1705753217,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1705753217,"objectID":"49c381d1a8bc932014b892a571eedff6","permalink":"https://yangleisx.github.io/post/psenet/","publishdate":"2024-01-20T20:20:17+08:00","relpermalink":"/post/psenet/","section":"post","summary":"论文题目：Shape Robust Text Detection with Progressive Scale Expansion Network\n作者：Wenhai Wang, Enze Xie, Xiang Li, Wenbo Hou, Tong Lu, Gang Yu, Shuai Shao\n会议/时间：CVPR2019\n链接：arxiv\n","tags":["Text Detection","Deep Learning","Computer Vision"],"title":"【论文】Shape Robust Text Detection with Progressive Scale Expansion Network","type":"post"},{"authors":["Yi He","Lei Yang","ShilinWang","Alan Wee-Chung Liew"],"categories":null,"content":" In this paper, we proposed a visual speaker authentication system using the random prompt text scheme to meet the requirements of mobile applications. Lip features are disentangled into three parts, i.e. the static identity features, the dynamic identity features and the content features by the proposed TDVSA-Net. The experiment results show that the content features obtained the lowest WER in speaker- independent lip-reading compared with other lipreading models, and the identity features had comparable performance with the state-of-the-art methods on detecting human imposters and DeepFake imposters and exhibits more robustness when facing different image qualities. Therefore, our VSA system can be a feasible solution for today’s widely used mobile applications.\n","date":1704844800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704844800,"objectID":"5fd4b6501903f192356210148aa1e793","permalink":"https://yangleisx.github.io/publication/tcsvt-vsa/","publishdate":"2024-01-20T00:00:00Z","relpermalink":"/publication/tcsvt-vsa/","section":"publication","summary":"Lip features are disentangled into three parts, i.e. the static identity features, the dynamic identity features and the content features by the proposed TDVSA-Net.","tags":[],"title":"Lip Feature Disentanglement for Visual Speaker Authentication in Natural Scenes","type":"publication"},{"authors":["Lei Yang","ShilinWang","Alan Wee-Chung Liew"],"categories":null,"content":" In this paper, we proposed a new lip segmentation method based on fuzzy convolutional neural network with graph reasoning that can learn high-level semantics. The fuzzy learning module and the fuzzy graph reasoning module help the deep convolutional neural network to handle uncertainties around ambiguous boundaries, capture global information, and improve multi-class lip region segmentation. In addition, a fine-grained lip region dataset is released for multi-class segmentation studies. Our proposed approach achieved satisfactory performance on the test set, with 94.36% pixel accuracy and 74.89% mIoU. The experiment results have demonstrated that the proposed method can be applied in many lip-related applications to obtain accurate and robust lip region segmentation in natural scenes.\nHowever, the proposed network cannot achieve satisfactory results in certain scenarios, specifically in cases of occlusion or extreme lighting conditions. This limitation can be attributed to the FLRSeg dataset, which is derived from the VSA dataset that was collected for speaker authentication and thus required unobstructed lip movements. In our future work, we will further improve the segmentation performance in various situations and explore the potential of leveraging the segmentation results in downstream tasks, such as lip reading and visual speaker authentication, to enhance performance and accelerate converge.\n","date":1690156800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690156800,"objectID":"94f8e2e9184f2654bf74ce49a32462c5","permalink":"https://yangleisx.github.io/publication/tfs-lip-seg/","publishdate":"2024-01-20T00:00:00Z","relpermalink":"/publication/tfs-lip-seg/","section":"publication","summary":"A new lip segmentation method based on fuzzy convolutional neural network with graph reasoning that can learn high-level semantics is proposed.","tags":[],"title":"Fine-Grained Lip Image Segmentation using Fuzzy Logic and Graph Reasoning","type":"publication"},{"authors":["Fangqi Li","Lei Yang","Shilin Wang","Alan Wee-Chung Liew"],"categories":null,"content":" This paper presents MTLSign, an MTL-based DNN watermarking scheme. We examine the basic security requirements for the DNN watermark, especially the unambiguity, and propose to embed the watermark as an additional task.\nThe proposed scheme explicitly meets security requirements by corresponding regularizers. With a decentralized consensus protocol, MTLSign is secure against adaptive attacks. It is true that like any other white-box DNN watermarking scheme, MTLSign remains vulnerable to functionality equivalence attacks such as the neuron permutation. This is one of the aspects that require further effort to increase the applicability of DNN watermarks.\n","date":1646006400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646006400,"objectID":"4ef88e8dc053666bc23b5353aadd54ff","permalink":"https://yangleisx.github.io/publication/aaaiw-mtl-watermark/","publishdate":"2024-01-20T00:00:00Z","relpermalink":"/publication/aaaiw-mtl-watermark/","section":"publication","summary":"MTLSign, an MTL-based DNN watermarking scheme","tags":null,"title":"Leveraging Multi-task Learning for Unambiguous and Flexible Deep Neural Network Watermarking","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Hugo Blox Builder Hugo Blox Builder | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://yangleisx.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Hugo Blox Builder's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://yangleisx.github.io/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://yangleisx.github.io/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"}]