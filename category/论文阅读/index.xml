<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>论文阅读 | YangLeiSX</title>
    <link>https://yangleisx.github.io/category/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</link>
      <atom:link href="https://yangleisx.github.io/category/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/index.xml" rel="self" type="application/rss+xml" />
    <description>论文阅读</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Wed, 06 Jul 2022 10:45:27 +0800</lastBuildDate>
    <image>
      <url>https://yangleisx.github.io/media/icon_hu9d67daea6e408fd17d2331b8d809e90a_61652_512x512_fill_lanczos_center_3.png</url>
      <title>论文阅读</title>
      <link>https://yangleisx.github.io/category/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</link>
    </image>
    
    <item>
      <title>【论文】Class-wise Dynamic Graph Convolution for Semantic Segmentation</title>
      <link>https://yangleisx.github.io/post/paper-cdgc/</link>
      <pubDate>Wed, 06 Jul 2022 10:45:27 +0800</pubDate>
      <guid>https://yangleisx.github.io/post/paper-cdgc/</guid>
      <description>&lt;p&gt;论文题目：Class-wise Dynamic Graph Convolution for Semantic Segmentation&lt;/p&gt;
&lt;p&gt;作者：Hanzhe Hu, Deyi Ji, Weihao Gan, Shuai Bai, Wei Wu, and Junjie Yan&lt;/p&gt;
&lt;p&gt;会议/时间：ECCV2020&lt;/p&gt;
&lt;p&gt;链接: &lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-030-58520-4_1?noAccess=true&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Springer&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;论文目标&#34;&gt;论文目标&lt;/h2&gt;
&lt;p&gt;使用GCN的方式来增强图像分割模型的感受野，提取丰富的上下文信息。使用由粗到细的方式，在GR中只引入同类型像素的特征进行增强。&lt;/p&gt;
&lt;p&gt;使用膨胀卷积等方式增大感受野不适用于像素级预测的密集任务，会出现信息丢失。因此可以使用GCN或者基于Attention 注意力机制的方法。但是使用注意力的算法往往使用全部像素特征聚合进行分类，很难得到具有判别力的像素特征。因此采用了只关注同类别像素特征和关注难正例、难反例的方式加强特征提取。同时也进一步减小全连接图带来的计算成本。&lt;/p&gt;
&lt;h2 id=&#34;相关工作&#34;&gt;相关工作&lt;/h2&gt;
&lt;p&gt;语义分割相关的工作包括UNet、SegNet、RefineNet、PSPNet、Deeplab等。都是采用了增加层数、膨胀卷积的方式增加感受野。&lt;/p&gt;
&lt;p&gt;在聚合上下文信息方面的工作包括DeepLab 系列、DANet、PSPNet、Non-Local Block等，包括使用Multi-Grid或者Attention 注意力机制的方式增强感受野。&lt;/p&gt;
&lt;p&gt;在Graph Reasoning 图推理相关的工作包括DeepLab引入的CRF 条件随机场、GloRe等。基本都是建立全连接图进行处理和分析。综合考虑了所有像素的特征。&lt;/p&gt;
&lt;h2 id=&#34;本文方法&#34;&gt;本文方法&lt;/h2&gt;
&lt;p&gt;整体结构如下，首先进行粗检测结果，得到$M$个类别的Mask图，接着将原本的图像特征重复$M$次分别用对应的Mask进行掩码处理，得到类别有关的特征。接着进行图卷积。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;CDGC-arch.png&#34; srcset=&#34;
               /post/paper-cdgc/CDGC-arch_hu005c453fbbd4ab03cec5faa1546b9681_153140_e60758a8b07c427bb4ec4ab161078aa9.webp 400w,
               /post/paper-cdgc/CDGC-arch_hu005c453fbbd4ab03cec5faa1546b9681_153140_3d48d0b38d268d70163fec80825fe6b8.webp 760w,
               /post/paper-cdgc/CDGC-arch_hu005c453fbbd4ab03cec5faa1546b9681_153140_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-cdgc/CDGC-arch_hu005c453fbbd4ab03cec5faa1546b9681_153140_e60758a8b07c427bb4ec4ab161078aa9.webp&#34;
               width=&#34;760&#34;
               height=&#34;355&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;图卷积的关键是建立邻接矩阵，这里关键的两点设计为相似度邻接矩阵和难例筛选。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;CDGC-CDGC.png&#34; srcset=&#34;
               /post/paper-cdgc/CDGC-CDGC_hu1b8e78203dcd82461934794b31912a46_86459_ea87bcb7ec25f3eb250f6a6d04d344e0.webp 400w,
               /post/paper-cdgc/CDGC-CDGC_hu1b8e78203dcd82461934794b31912a46_86459_911dd36e9995276b4e8bd8bde1456c7b.webp 760w,
               /post/paper-cdgc/CDGC-CDGC_hu1b8e78203dcd82461934794b31912a46_86459_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-cdgc/CDGC-CDGC_hu1b8e78203dcd82461934794b31912a46_86459_ea87bcb7ec25f3eb250f6a6d04d344e0.webp&#34;
               width=&#34;760&#34;
               height=&#34;281&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;在构建图的时候，一种方法（Basic-GCN）使用相似度和Softmax操作得到行归一化之后的邻接矩阵。



$$\begin{aligned}
F(x_i, x_j) &amp;= \phi(x_i)^T \phi&#39;(x_j) \\
A_{ij} &amp;= \frac{exp(F(x_i, x_j))}{\sum_j exp(F(x_i, x_j))}
\end{aligned}$$
&lt;/p&gt;
&lt;p&gt;另一种方法（Dynamic Sampling GCN）是使用难例筛选，令 $C$ 为预测得到的Mask，$G$ 为Ground Truth。可以得到 $Easy Positive = C \cap G$，$HardNegative = C - C\cap G$，$Hard Positive = G - C \cap G$。&lt;/p&gt;
&lt;p&gt;于是采样点为



$$\begin{aligned}
Sampled &amp;= C-C\cap G + G - C\cap G + ratio \cdot C\cap G\\
&amp;= C\cup G - (1-ratio)\cdot C \cap G
\end{aligned}$$

这里在训练的时候使用Ground Truth进行难例筛选，在推断的时候使用全部预测mask进行推断。&lt;/p&gt;
&lt;p&gt;在GCN的部分使用M组参数分别进行卷积。最后通过1x1的卷积得到与原本特征形式相同的特征。&lt;/p&gt;
&lt;p&gt;训练的损失函数包括粗检测结果和最终检测结果的监督。同时在Backbone中间也加入了辅助损失加快收敛。&lt;/p&gt;
&lt;h2 id=&#34;结果分析&#34;&gt;结果分析&lt;/h2&gt;
&lt;p&gt;ablation study做的比较多。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;CDGC-ablation1.png&#34; srcset=&#34;
               /post/paper-cdgc/CDGC-ablation1_hud4420f06e394ccad63dbe50950eab379_103598_3b650bbeeea10a16cb60061671dd0a5d.webp 400w,
               /post/paper-cdgc/CDGC-ablation1_hud4420f06e394ccad63dbe50950eab379_103598_d77c0d941efe84aed90e3201314b7b3c.webp 760w,
               /post/paper-cdgc/CDGC-ablation1_hud4420f06e394ccad63dbe50950eab379_103598_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-cdgc/CDGC-ablation1_hud4420f06e394ccad63dbe50950eab379_103598_3b650bbeeea10a16cb60061671dd0a5d.webp&#34;
               width=&#34;760&#34;
               height=&#34;232&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;CDGC-ablation2.png&#34; srcset=&#34;
               /post/paper-cdgc/CDGC-ablation2_hu2e825f9924a40310977bb2ac253e526d_124341_5ba079682f5b229052107e0e0448a64e.webp 400w,
               /post/paper-cdgc/CDGC-ablation2_hu2e825f9924a40310977bb2ac253e526d_124341_7c04d70b1f9c789f5934d3785a125e0c.webp 760w,
               /post/paper-cdgc/CDGC-ablation2_hu2e825f9924a40310977bb2ac253e526d_124341_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-cdgc/CDGC-ablation2_hu2e825f9924a40310977bb2ac253e526d_124341_5ba079682f5b229052107e0e0448a64e.webp&#34;
               width=&#34;760&#34;
               height=&#34;240&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;CDGC-ablation3.png&#34; srcset=&#34;
               /post/paper-cdgc/CDGC-ablation3_huf28f1cc6bc7e734ac9e271af98d2adb1_52662_b97b600a551bf56a4c77b7475adf69d8.webp 400w,
               /post/paper-cdgc/CDGC-ablation3_huf28f1cc6bc7e734ac9e271af98d2adb1_52662_fa54d802281e4ec201d2c279bddcb95d.webp 760w,
               /post/paper-cdgc/CDGC-ablation3_huf28f1cc6bc7e734ac9e271af98d2adb1_52662_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-cdgc/CDGC-ablation3_huf28f1cc6bc7e734ac9e271af98d2adb1_52662_b97b600a551bf56a4c77b7475adf69d8.webp&#34;
               width=&#34;760&#34;
               height=&#34;199&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;最后比了一下SOTA。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;CDGC-sota.png&#34; srcset=&#34;
               /post/paper-cdgc/CDGC-sota_hu44a91e207708b09c17c1c9d65e937690_250473_1ccb4551d1e77fc54d9b10417cd6e8ad.webp 400w,
               /post/paper-cdgc/CDGC-sota_hu44a91e207708b09c17c1c9d65e937690_250473_bb1139909d4e31436fd352bfb70f8bf9.webp 760w,
               /post/paper-cdgc/CDGC-sota_hu44a91e207708b09c17c1c9d65e937690_250473_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-cdgc/CDGC-sota_hu44a91e207708b09c17c1c9d65e937690_250473_1ccb4551d1e77fc54d9b10417cd6e8ad.webp&#34;
               width=&#34;760&#34;
               height=&#34;635&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;在图卷积的部分就设计了难例筛选，而不是像OHEM那样在最后的分割图上做mining。&lt;/p&gt;
&lt;p&gt;从coarse-to-fine的思路出发。&lt;/p&gt;
&lt;p&gt;个人觉得有点类似OCRNet。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>【论文】Improving Semantic Segmentation in Aerial Imagery via Graph Reasoning and Disentangled Learning</title>
      <link>https://yangleisx.github.io/post/paper-pgr/</link>
      <pubDate>Wed, 06 Jul 2022 10:17:43 +0800</pubDate>
      <guid>https://yangleisx.github.io/post/paper-pgr/</guid>
      <description>&lt;p&gt;论文题目： Improving Semantic Segmentation in Aerial Imagery via Graph Reasoning and Disentangled Learning&lt;/p&gt;
&lt;p&gt;作者：Ruigang Niu, Xian Sun, Yu Tian, Wenhui Diao, Yingchao Feng, Kun Fu&lt;/p&gt;
&lt;p&gt;会议/时间：TGRS2021&lt;/p&gt;
&lt;p&gt;链接: &lt;a href=&#34;https://www.researchgate.net/publication/355465205_Improving_Semantic_Segmentation_in_Aerial_Imagery_via_Graph_Reasoning_and_Disentangled_Learning&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ResearchGate&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;论文目标&#34;&gt;论文目标&lt;/h2&gt;
&lt;p&gt;在航空影像分割问题中，由于存在前景背景不平衡，类内数据差异较大以及密集/小目标的存在，性能受到限制。文章通过引入Graph Reasoning 图推理和Disentangled Representation Learning 解耦表示学习的思路，提升在航空影像上分割的效果。&lt;/p&gt;
&lt;p&gt;为了提取丰富的上下文信息，之前的工作使用了FCN、ASPP等方式，也可以使用基于Attention 注意力机制的方式。为了进一步增加上下文信息，可以使用Graph Reasoning 图推理的方式。&lt;/p&gt;
&lt;p&gt;对于密集目标、小目标等容易出现特征含糊不清的问题，引入了解耦的多分支的结构。使用多任务学习的方式来解决分割和边缘检测的问题。&lt;/p&gt;
&lt;h2 id=&#34;相关工作&#34;&gt;相关工作&lt;/h2&gt;
&lt;p&gt;使用GR的算法中，GloRe使用1D卷积在全连接图上实现图卷积，SPyGR直接在像素空间上做图卷积，忽略了像素空间和语义空间之间的语义差异。CDGC引入了从粗检测到精细化的方式（有点类似OCRNet？）DisenGCN将GCN和Disentangled Learning两者相结合。&lt;/p&gt;
&lt;h2 id=&#34;本文方法&#34;&gt;本文方法&lt;/h2&gt;
&lt;p&gt;整体结构图下。首先使用FPN得到层次特征，使用GR模块处理特征。将特征送入双分支解耦学习模块，分别进行前景估计和边缘对齐，最后将所有的特征融合到一起进行预测。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;pgr-disen-structure.png&#34; srcset=&#34;
               /post/paper-pgr/pgr-disen-structure_hu42f8ef22bf07443464a7487f6234e4fa_749556_1b6c4f43f1efdd36f12839363fd2fc3d.webp 400w,
               /post/paper-pgr/pgr-disen-structure_hu42f8ef22bf07443464a7487f6234e4fa_749556_4f1285a31cde5c716561f54ffaa91409.webp 760w,
               /post/paper-pgr/pgr-disen-structure_hu42f8ef22bf07443464a7487f6234e4fa_749556_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-pgr/pgr-disen-structure_hu42f8ef22bf07443464a7487f6234e4fa_749556_1b6c4f43f1efdd36f12839363fd2fc3d.webp&#34;
               width=&#34;760&#34;
               height=&#34;353&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;这里的图卷积模块，吸收了HBP 层次双线性池化的思想，将相邻的三个不同分辨率的特征图进行缩放之后计算Hadamard积然后映射到若干个点，然后进行图卷积，最后使用卷积得到的结果对原本的特征相乘在映射回到原本的像素空间中。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;pgr-disen-gr.png&#34; srcset=&#34;
               /post/paper-pgr/pgr-disen-gr_hu7af98c730b489b2d0f9d0d6f35621975_41429_a08ed3f5d2c397be56ce578b7b000673.webp 400w,
               /post/paper-pgr/pgr-disen-gr_hu7af98c730b489b2d0f9d0d6f35621975_41429_fd1cebc680eb0e5bb3e8913573be0ce0.webp 760w,
               /post/paper-pgr/pgr-disen-gr_hu7af98c730b489b2d0f9d0d6f35621975_41429_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-pgr/pgr-disen-gr_hu7af98c730b489b2d0f9d0d6f35621975_41429_a08ed3f5d2c397be56ce578b7b000673.webp&#34;
               width=&#34;708&#34;
               height=&#34;250&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;使用HBP 层次双线性池化进行映射的时候有如下公式进行池化。
$$G_{proj}(F^2, \mathcal{U}(F^1), \mathcal{U}(F^3)) = \frac{1}{H_2W_2}\sum\limits_{i=1}^{H_2W_2} f^2_i\circ f&amp;rsquo;^1_i\circ f&amp;rsquo;^3_i$$&lt;/p&gt;
&lt;p&gt;最后按通道分成g组，即 $C = g\times d$ ，可以认为分成了g个节点，每一个节点的特征维度为d（可以认为是d个像素空间的点构成了一个图空间的点）。在进行图卷积的时候，令 $H = \sigma(A_g X W_g)$ ，其中 $A_g\in R^{N\times N},X\in R^{N \times C},W_g\in R^{C\times F}$ ，这里的$N$就是上面的$g$，$C$就是上面的$d$。&lt;/p&gt;
&lt;p&gt;在邻接矩阵的设计上，使用了四种不同的策略，分别是固定为一跳邻居、单位阵初始化的可学习参数、正态分布初始化的可学习参数、均匀分布初始化的可学习参数。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;pgr-disen-adj.png&#34; srcset=&#34;
               /post/paper-pgr/pgr-disen-adj_hubce0fc37d4f14e646f179ebf00ee670e_78264_a79abd52f3160dacce050992d11ece86.webp 400w,
               /post/paper-pgr/pgr-disen-adj_hubce0fc37d4f14e646f179ebf00ee670e_78264_17222b45add156dc27f84114a6c75a1a.webp 760w,
               /post/paper-pgr/pgr-disen-adj_hubce0fc37d4f14e646f179ebf00ee670e_78264_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-pgr/pgr-disen-adj_hubce0fc37d4f14e646f179ebf00ee670e_78264_a79abd52f3160dacce050992d11ece86.webp&#34;
               width=&#34;760&#34;
               height=&#34;252&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;最后在反向映射的过程中，采用了类似SE-Net的方式，将图卷积得到的结果作为通道注意力与原始数据相乘。&lt;/p&gt;
&lt;p&gt;在前景估计分支，作者使用了贝叶斯理论, 实际结构是学习得到一个分割图然后concat起来。使用了 $B = \delta(M_{fg} \cdot I\parallel (1-M_{fg}) \cdot I \parallel I)$ 的拼接方式。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;pgr-disen-entimation.png&#34; srcset=&#34;
               /post/paper-pgr/pgr-disen-fg-entimation_hu39e31c122b11baf80fda755c39575483_96734_d7c214f8f9c1a65893de4f0ed376a1b6.webp 400w,
               /post/paper-pgr/pgr-disen-fg-entimation_hu39e31c122b11baf80fda755c39575483_96734_553d67e70d91f4335104636b7bbba46e.webp 760w,
               /post/paper-pgr/pgr-disen-fg-entimation_hu39e31c122b11baf80fda755c39575483_96734_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-pgr/pgr-disen-fg-entimation_hu39e31c122b11baf80fda755c39575483_96734_d7c214f8f9c1a65893de4f0ed376a1b6.webp&#34;
               width=&#34;760&#34;
               height=&#34;288&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;在边界对齐模块，作者号称使用了类似Optical Flow 光流的思想，学习得到一个类似光流的边界检测图然后与原本的特征相结合。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;pgr-disen-bam.png&#34; srcset=&#34;
               /post/paper-pgr/pgr-disen-bam_hu9308b3195789f8350ee447770743ed12_69347_d17ddba4976fef5bd87de8a377c734e1.webp 400w,
               /post/paper-pgr/pgr-disen-bam_hu9308b3195789f8350ee447770743ed12_69347_1002431e9a39250de6d30aa15a451515.webp 760w,
               /post/paper-pgr/pgr-disen-bam_hu9308b3195789f8350ee447770743ed12_69347_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-pgr/pgr-disen-bam_hu9308b3195789f8350ee447770743ed12_69347_d17ddba4976fef5bd87de8a377c734e1.webp&#34;
               width=&#34;760&#34;
               height=&#34;435&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;在最后损失函数设计上，对于最后的分割使用Cross Emtropy 交叉熵，前景使用BCE_Loss和Dice Loss，添加了类似PSPNet中的辅助Loss。&lt;/p&gt;
&lt;h2 id=&#34;结果分析&#34;&gt;结果分析&lt;/h2&gt;
&lt;p&gt;在iSAID数据集和Vaihingen、Cityscapes数据集上测试了性能。
基本模型结构使用预训练的ResNet-50/101。&lt;/p&gt;
&lt;p&gt;简单看一下Ablation Study的效果。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;pgr-disen-ablation.png&#34; srcset=&#34;
               /post/paper-pgr/pgr-disen-ablation_hu24765524787a58c68c7eace6997cea4b_123171_914aee42f9e522e84faf29050ee657f2.webp 400w,
               /post/paper-pgr/pgr-disen-ablation_hu24765524787a58c68c7eace6997cea4b_123171_dccf4094bbbb4bae7b9225a66c84ebd3.webp 760w,
               /post/paper-pgr/pgr-disen-ablation_hu24765524787a58c68c7eace6997cea4b_123171_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-pgr/pgr-disen-ablation_hu24765524787a58c68c7eace6997cea4b_123171_914aee42f9e522e84faf29050ee657f2.webp&#34;
               width=&#34;659&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;使用了图推理的方式提取上下文信息。这一部分的论文还蛮多的，这里用了一种分组的方式来进行处理。比直接使用像素点的节约时间和空间，用通道注意力的方式进行反向映射的方式也成本比较低。&lt;/p&gt;
&lt;p&gt;使用了多分支的模型，分别学习分割图和边缘检测。利用边缘检测增强分割效果的想法蛮常见的。例如【论文】Boundary-aware Graph Reasoning for Semantic Segmentation或者【论文】Real-time Scene Text Detection with Differentiable Binarization|。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>【论文】Tensor Low-Rank Reconstruction for Semantic Segmentation</title>
      <link>https://yangleisx.github.io/post/paper-low-rank-sep/</link>
      <pubDate>Wed, 06 Jul 2022 10:12:10 +0800</pubDate>
      <guid>https://yangleisx.github.io/post/paper-low-rank-sep/</guid>
      <description>&lt;p&gt;论文题目：Tensor Low-Rank Reconstruction for Semantic Segmentation&lt;/p&gt;
&lt;p&gt;作者：Wanli Chen, Xinge Zhu, Ruoqi Sun, Junjun He, Ruiyu Li, Xiaoyong Shen, Bei Yu&lt;/p&gt;
&lt;p&gt;会议/时间：ECCV 2020&lt;/p&gt;
&lt;p&gt;链接: &lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-030-58520-4_4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;springer&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;本文方法&#34;&gt;本文方法&lt;/h2&gt;
&lt;p&gt;整体结构如下。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;recos-arch1.png&#34; srcset=&#34;
               /post/paper-low-rank-sep/recos-arch1_hu069fd8b011d1e96f783ea6641c2f5cfb_112593_2fd73f95fbbdf333ea3c080704b89497.webp 400w,
               /post/paper-low-rank-sep/recos-arch1_hu069fd8b011d1e96f783ea6641c2f5cfb_112593_b9d3c72cb80ba9153904f90d75b193e2.webp 760w,
               /post/paper-low-rank-sep/recos-arch1_hu069fd8b011d1e96f783ea6641c2f5cfb_112593_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-low-rank-sep/recos-arch1_hu069fd8b011d1e96f783ea6641c2f5cfb_112593_2fd73f95fbbdf333ea3c080704b89497.webp&#34;
               width=&#34;760&#34;
               height=&#34;251&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;低秩分解可以形式化为 $$A = \sum\limits_{i=1}^{r} \lambda_i v_{ci}\otimes v_{hi}\otimes v_{wi}$$&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;recos-arch.png&#34; srcset=&#34;
               /post/paper-low-rank-sep/recos-arch_hu35440024ba1845106099448d5976593f_42553_6cfb8bc206a938223841e52c387b40ff.webp 400w,
               /post/paper-low-rank-sep/recos-arch_hu35440024ba1845106099448d5976593f_42553_7b830f0d3adf610b83166ef4a1c77192.webp 760w,
               /post/paper-low-rank-sep/recos-arch_hu35440024ba1845106099448d5976593f_42553_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-low-rank-sep/recos-arch_hu35440024ba1845106099448d5976593f_42553_6cfb8bc206a938223841e52c387b40ff.webp&#34;
               width=&#34;760&#34;
               height=&#34;170&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;特征分解模块的定义如下。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;recos-de.png&#34; srcset=&#34;
               /post/paper-low-rank-sep/recos-de_hu2b8c5659f1d0de09116353714db60356_71676_192d64e12951fc2b423af58c9b91b813.webp 400w,
               /post/paper-low-rank-sep/recos-de_hu2b8c5659f1d0de09116353714db60356_71676_0614b8b5deb912c11361099533df9c4e.webp 760w,
               /post/paper-low-rank-sep/recos-de_hu2b8c5659f1d0de09116353714db60356_71676_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-low-rank-sep/recos-de_hu2b8c5659f1d0de09116353714db60356_71676_192d64e12951fc2b423af58c9b91b813.webp&#34;
               width=&#34;760&#34;
               height=&#34;452&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;特征重构模块的定义如下。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;recons-recons.png&#34; srcset=&#34;
               /post/paper-low-rank-sep/recons-recons_hucbf10b5c5ca3e958955f72c7c975fe4b_69399_383c9b7ba37645e044df0e65d5d8a1b9.webp 400w,
               /post/paper-low-rank-sep/recons-recons_hucbf10b5c5ca3e958955f72c7c975fe4b_69399_fea66ce5f47eb1edb56ee9d6fc9b4037.webp 760w,
               /post/paper-low-rank-sep/recons-recons_hucbf10b5c5ca3e958955f72c7c975fe4b_69399_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-low-rank-sep/recons-recons_hucbf10b5c5ca3e958955f72c7c975fe4b_69399_383c9b7ba37645e044df0e65d5d8a1b9.webp&#34;
               width=&#34;760&#34;
               height=&#34;330&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;论文里面给了矩阵分解和一些传统方法之间的差异。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;recons-se.png&#34; srcset=&#34;
               /post/paper-low-rank-sep/recons-se_hu1e2b20a682aa392830359daad6619bbc_18909_1f0342d011e1ac232c9687c3c866b077.webp 400w,
               /post/paper-low-rank-sep/recons-se_hu1e2b20a682aa392830359daad6619bbc_18909_7bb3a36972c048a74c40c817264b94c5.webp 760w,
               /post/paper-low-rank-sep/recons-se_hu1e2b20a682aa392830359daad6619bbc_18909_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-low-rank-sep/recons-se_hu1e2b20a682aa392830359daad6619bbc_18909_1f0342d011e1ac232c9687c3c866b077.webp&#34;
               width=&#34;454&#34;
               height=&#34;258&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;recons-cbam.png&#34; srcset=&#34;
               /post/paper-low-rank-sep/recons-cbam_hu14fb0848504c3af276c469b5cb966298_15043_8c05f2f5aea3a3b23f606a371e78cb31.webp 400w,
               /post/paper-low-rank-sep/recons-cbam_hu14fb0848504c3af276c469b5cb966298_15043_a97f3752f8a53fe60fd41bea092cd6bc.webp 760w,
               /post/paper-low-rank-sep/recons-cbam_hu14fb0848504c3af276c469b5cb966298_15043_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-low-rank-sep/recons-cbam_hu14fb0848504c3af276c469b5cb966298_15043_8c05f2f5aea3a3b23f606a371e78cb31.webp&#34;
               width=&#34;708&#34;
               height=&#34;110&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>【论文】Spatial Pyramid Based Graph Reasoning for Semantic Segmentation</title>
      <link>https://yangleisx.github.io/post/paper-spygr/</link>
      <pubDate>Wed, 06 Jul 2022 10:01:45 +0800</pubDate>
      <guid>https://yangleisx.github.io/post/paper-spygr/</guid>
      <description>&lt;p&gt;论文题目：Spatial Pyramid Based Graph Reasoning for Semantic Segmentation&lt;/p&gt;
&lt;p&gt;作者：Xia Li,Yibo Yang, Qijie Zhao, Tiancheng Shen, Zhouchen Lin, Hong Liu&lt;/p&gt;
&lt;p&gt;会议/时间：CVPR2020&lt;/p&gt;
&lt;p&gt;链接: &lt;a href=&#34;https://arxiv.org/abs/2003.10211&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;论文目标&#34;&gt;论文目标&lt;/h2&gt;
&lt;p&gt;通过在基于GCN/UNet的结构中添加Graph Reasoning 图推理来引入长距离的上下文信息依赖。使用Non-Local Block等Attention 注意力机制的解决方案计算复杂度比较高。使用图卷积网络的解决方案通常需要首先将网格状的图像数据转换/映射到图网络数据，这个映射过程的成本比较高，而且可学习的映射可能会损失数据中在空间上的关系。&lt;/p&gt;
&lt;p&gt;通常的图卷积网络定义在非欧式空间中，不能直接添加到现有的CNN结构中，因此论文作者设计了一个数据有关的相似度矩阵作为图卷积中的Laplacian矩阵进行图卷积的计算。&lt;/p&gt;
&lt;h2 id=&#34;本文方法&#34;&gt;本文方法&lt;/h2&gt;
&lt;p&gt;整体结构如下，在FCN特征融合的部分添加GR模块，进行长距离依赖的学习。



$$\begin{align}
Y^{(s+1)} &amp;= GR(X^{(s+1)}) + \Pi_{up}(Y^{(s)}) \\
Y^{(0)} &amp;= GR(X^{(0)}) \\
X^{(s)} &amp;= \Pi_{down}(X^{(s+1)})
\end{align}$$
&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;pyramid-gr.png&#34; srcset=&#34;
               /post/paper-spygr/pyramid-gr_hua88d50e9be7b3cee7c60c017afaca377_674928_d848671f6a2e5737c3b04fd0ec4a585e.webp 400w,
               /post/paper-spygr/pyramid-gr_hua88d50e9be7b3cee7c60c017afaca377_674928_029a92ebfe656e3e1bd4ad0704f38c8c.webp 760w,
               /post/paper-spygr/pyramid-gr_hua88d50e9be7b3cee7c60c017afaca377_674928_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-spygr/pyramid-gr_hua88d50e9be7b3cee7c60c017afaca377_674928_d848671f6a2e5737c3b04fd0ec4a585e.webp&#34;
               width=&#34;760&#34;
               height=&#34;318&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;考虑到 $H^{l+1} = \sigma(\tilde L H^lW^l)$ 的图卷积网络通用格式，其中 $\tilde L = I - \tilde D^{-\frac{1}{2}} \tilde A \tilde D^{-\frac{1}{2}}$,$\tilde A = A + I$ ，有 $\tilde D_{ii} = \sum_j \tilde A_{ij}$ 。如果不使用欧式空间到图网络空间的映射，直接在特征图上进行图卷积。需要对模型做相应的修改。&lt;/p&gt;
&lt;p&gt;上式中的 $\tilde A$ 表示正规化之后的邻接矩阵，或者称为相似度矩阵，在这个论文中使用 $\tilde A_{ij} = \phi(X)_i \tilde\Lambda(X) \phi(X)_j^T$ 计算，即使用位置无关但是数据有关的点乘注意力实现。而不是使用训练得到的固定的邻接矩阵。这里的 $\tilde \Lambda$ 的计算方式采用类似通道注意力的方式实现，即先进行GAP然后卷积，最后得到对角矩阵。&lt;/p&gt;
&lt;p&gt;使用 $\tilde D$ 提供了正则化，不需要再进行Softmax操作。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;pyramid-gr-module.png&#34; srcset=&#34;
               /post/paper-spygr/pyramid-gr-module_hu1238d42526964fea1e9636e9ca1e81a1_49527_9cf1ec54a0f6f230764943dafc315287.webp 400w,
               /post/paper-spygr/pyramid-gr-module_hu1238d42526964fea1e9636e9ca1e81a1_49527_4ee2c7c3de0562ceece74e02b3c76a91.webp 760w,
               /post/paper-spygr/pyramid-gr-module_hu1238d42526964fea1e9636e9ca1e81a1_49527_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-spygr/pyramid-gr-module_hu1238d42526964fea1e9636e9ca1e81a1_49527_9cf1ec54a0f6f230764943dafc315287.webp&#34;
               width=&#34;760&#34;
               height=&#34;462&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;在之前的图推理方法中，将像素数据映射到Interspace中，得到图结构的节点数量远少于原本的像素数量，本文中直接实现的在像素域上的计算方法计算量比较大，因此引入了简化方法。即在计算 $\tilde D$ 的时候并不是直接计算 $\tilde A \in R^{HW \times HW}$ ，而是引入一个全1的向量，得到 $\tilde D = diag(\tilde A \cdot \vec 1) = diag(\phi(\tilde\Lambda(\phi^T \cdot \vec 1)))$ ，将所有的矩阵计算变为和一个向量的运算。计算左乘 $\tilde L X$ 的时候使用 $\tilde LX = X - \tilde D^{-\frac{1}{2}} \phi\tilde\Lambda\phi^T\tilde D^{-\frac{1}{2}}X = X - P(\tilde\Lambda(P^TX))$ ，其中 $P = \tilde D^{-\frac{1}{2}}\phi$ 。&lt;/p&gt;
&lt;h2 id=&#34;结果分析&#34;&gt;结果分析&lt;/h2&gt;
&lt;p&gt;首先进行Ablation Study。对于GR模块提出的Laplacian各个部分进行对比。可以看到效果提升。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;spygr-ablation.png&#34; srcset=&#34;
               /post/paper-spygr/spygr-ablation_huec8741a7a4086f59ec669dfad179ae4f_33872_b32cd73615180a77476d4e5e81aa7fb0.webp 400w,
               /post/paper-spygr/spygr-ablation_huec8741a7a4086f59ec669dfad179ae4f_33872_b3a1229d56c980564da0b7fe56f55955.webp 760w,
               /post/paper-spygr/spygr-ablation_huec8741a7a4086f59ec669dfad179ae4f_33872_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-spygr/spygr-ablation_huec8741a7a4086f59ec669dfad179ae4f_33872_b32cd73615180a77476d4e5e81aa7fb0.webp&#34;
               width=&#34;693&#34;
               height=&#34;335&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;在Cityscapes、Pascal VOC和MS COCO数据集上做了实验。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;spygr-cityscape.png&#34; srcset=&#34;
               /post/paper-spygr/spygr-cityscape_huf0e89c5e640b42fe0b9bae93bf830710_168459_39a18939874adc1b5367312f767da526.webp 400w,
               /post/paper-spygr/spygr-cityscape_huf0e89c5e640b42fe0b9bae93bf830710_168459_07064a8f2d92102c24f893bd817cbf17.webp 760w,
               /post/paper-spygr/spygr-cityscape_huf0e89c5e640b42fe0b9bae93bf830710_168459_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-spygr/spygr-cityscape_huf0e89c5e640b42fe0b9bae93bf830710_168459_39a18939874adc1b5367312f767da526.webp&#34;
               width=&#34;760&#34;
               height=&#34;305&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;spygr-coco.png&#34; srcset=&#34;
               /post/paper-spygr/spygr-coco_hu77a24da1243c1607444b679260beedcc_61234_50ee974f8a73cead750622a0b03c72e8.webp 400w,
               /post/paper-spygr/spygr-coco_hu77a24da1243c1607444b679260beedcc_61234_c42dbfbf8a8f9c98fcdff277a5966e5c.webp 760w,
               /post/paper-spygr/spygr-coco_hu77a24da1243c1607444b679260beedcc_61234_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-spygr/spygr-coco_hu77a24da1243c1607444b679260beedcc_61234_50ee974f8a73cead750622a0b03c72e8.webp&#34;
               width=&#34;683&#34;
               height=&#34;349&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>【论文】A Hierarchical Fused Fuzzy Deep Neural Network for Data Classification</title>
      <link>https://yangleisx.github.io/post/paper-fdnn/</link>
      <pubDate>Wed, 06 Jul 2022 09:50:55 +0800</pubDate>
      <guid>https://yangleisx.github.io/post/paper-fdnn/</guid>
      <description>&lt;p&gt;论文题目：A Hierarchical Fused Fuzzy Deep Neural Network for Data Classification&lt;/p&gt;
&lt;p&gt;作者：Yue Deng, Zhiquan Ren, Youyong Kong, Feng Bao, and Qionghai Dai&lt;/p&gt;
&lt;p&gt;会议/时间：IEEE TRANSACTIONS ON FUZZY SYSTEMS, 2017&lt;/p&gt;
&lt;p&gt;链接: &lt;a href=&#34;https://ieeexplore.ieee.org/document/7482843&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ieeexplore&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;论文目标&#34;&gt;论文目标&lt;/h2&gt;
&lt;p&gt;在深度神经网络中引入模糊逻辑从而应对数据中的噪声和不确定度。&lt;/p&gt;
&lt;p&gt;之前的工作大多是使用模糊逻辑处理模型的输入（Fuzzier）或者输出（Defuzzier），论文中将模糊逻辑和神经网络并行处理得到的特征融合在一起。&lt;/p&gt;
&lt;h2 id=&#34;本文方法&#34;&gt;本文方法&lt;/h2&gt;
&lt;p&gt;整体结构如图。模糊逻辑部分使用高斯隶属度，神经网络部分使用MLP，最后使用线性组合的方式得到融合后的模型，再通过全链接得到任务所需要的输出结果。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;FDNN-arch.png&#34; srcset=&#34;
               /post/paper-fdnn/FDNN-arch_hua1d7a6de72dc49afc9a35271f97ceb10_1283191_21551d170db447f7b976b7b371c27684.webp 400w,
               /post/paper-fdnn/FDNN-arch_hua1d7a6de72dc49afc9a35271f97ceb10_1283191_b84550e267eab29bdad053300b35020e.webp 760w,
               /post/paper-fdnn/FDNN-arch_hua1d7a6de72dc49afc9a35271f97ceb10_1283191_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-fdnn/FDNN-arch_hua1d7a6de72dc49afc9a35271f97ceb10_1283191_21551d170db447f7b976b7b371c27684.webp&#34;
               width=&#34;760&#34;
               height=&#34;577&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;在训练的时候，考虑到模型对初始化比较敏感，使用了$w \sim U[-\frac{1}{\sqrt{n^{l-1}}}, \frac{1}{\sqrt{n^{l-1}}}]$初始化神经网络部分的参数，使用[[K-means 算法]]得到的聚类结果作为fuzzy部分的初始化。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We follow the method in [16] to initialize the fuzzy member-ship nodes according to k-means results on the input data, where k is suggested the same as the class number to be classified.
[16]  N.KasabovandQ.Song,“Denfis:Dynamic evolving neural-fuzzy inference system and its application for time-series prediction,” IEEE Trans. Fuzzy Syst., vol. 10, no. 2, pp. 144–154, Apr. 2002.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;结果分析&#34;&gt;结果分析&lt;/h2&gt;
&lt;p&gt;在三个任务上进行了实验：1）data classification tasks of image categorization, 2）brain magnetic reso- nance imaging (MRI) segmentation and 3）high-frequency financial tick prediction。取得了不错的效果。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>【论文】Towards Deep Learning Models Resistant to Adversarial Attacks</title>
      <link>https://yangleisx.github.io/post/paper-adver-attack/</link>
      <pubDate>Sun, 05 Dec 2021 15:26:53 +0800</pubDate>
      <guid>https://yangleisx.github.io/post/paper-adver-attack/</guid>
      <description>&lt;p&gt;论文题目：【论文】Towards Deep Learning Models Resistant to Adversarial Attacks&lt;/p&gt;
&lt;p&gt;作者：Aleksander Madry，Aleksandar Makelov，Ludwig Schmidt，Dimitris Tsipras，Adrian Vladu&lt;/p&gt;
&lt;p&gt;会议/时间：ICLR2018&lt;/p&gt;
&lt;p&gt;链接: &lt;a href=&#34;https://arXiv.org/pdf/1706.06083.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;论文目标&#34;&gt;论文目标&lt;/h2&gt;
&lt;p&gt;很多模型对 Adversarial Examples 对抗样本 比较敏感，特别是计算机视觉任务中，输入图片添加微小扰动就可能使模型输出错误的结果。文章中通过对抗训练的方式得到一个对对抗样本不敏感的模型，从而使得模型具有较高的鲁棒性。&lt;/p&gt;
&lt;h2 id=&#34;相关工作&#34;&gt;相关工作&lt;/h2&gt;
&lt;p&gt;现有的对抗样本算法，主要专注解决两个问题，一个是如何用更小的扰动对结果造成更大的影响，另一个是如何训练一个鲁棒性较强的模型，或者是对抗样本较难获得的模型。&lt;/p&gt;
&lt;p&gt;对抗样本的生成方面，主要是 FGSM 快速梯度下降法 算法及其各种变种，以及 PGD 算法。&lt;/p&gt;
&lt;p&gt;在防御端，主要是采用 Adversarial Training 对抗训练 的思路，利用FGSM生成的对抗样本参与训练。&lt;/p&gt;
&lt;h2 id=&#34;本文方法&#34;&gt;本文方法&lt;/h2&gt;
&lt;p&gt;考虑到对抗样本攻击通常在输入数据上添加一个扰动，因此针对对抗样本提高模型鲁棒性的方式可以表示为一个 Min-Max优化问题 ，或者称为 对抗优化 问题，表示如下。其中$L(\theta, x, y)$为经验损失。&lt;/p&gt;
&lt;p&gt;$$
\min_{\theta} \rho(\theta), where\quad \rho(\theta) = \mathbb{E}_{(x,y)\sim \mathcal{D}}\left[\max_{\delta \in S} L(\theta,x+\delta,y) \right]
$$&lt;/p&gt;
&lt;p&gt;解决这样的对抗优化问题需要解决非凸的优化问题，包括内部的最大化问题和外部的最小化问题。&lt;/p&gt;
&lt;p&gt;文中通过实验发现，在选定数据的 $l_{\infty}-Ball$内部（参考 无穷范数 ）存在非常多的局部最优点，并且具有很小的损失值。在实验中通过使用 PGD 算法 生成对抗数据并测试可以得到，PGD可以作为一种通用的攻击方式，在选定数据附近得到一个局部最优。因此能对抗PGD的算法也能比较好的对抗一阶攻击（First-Order Adversarial，只利用一阶梯度值生成对抗样本的攻击）。因此使用PGD生成的对抗样本作为内部最大化问题的解。&lt;/p&gt;
&lt;p&gt;对于外部的最小化问题，可以直接使用SGD优化器进行优化，即计算 $\nabla_{\theta}\ \rho(\theta)$。假设Danskin&amp;rsquo;s定理在当前问题上成立，可推出可以使用SGD优化器优化外部最小化问题求解，实验结果也证明这一点。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;adversarial-training-capacity.png&#34; srcset=&#34;
               /post/paper-adver-attack/adversarial-training-capacity_hua331c90d6e8d4c52090921fa3f0357fe_281144_094bb953067b6ffb35d33ae102323146.webp 400w,
               /post/paper-adver-attack/adversarial-training-capacity_hua331c90d6e8d4c52090921fa3f0357fe_281144_4b8a893f322346e70f8e3d0631b9ee5d.webp 760w,
               /post/paper-adver-attack/adversarial-training-capacity_hua331c90d6e8d4c52090921fa3f0357fe_281144_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-adver-attack/adversarial-training-capacity_hua331c90d6e8d4c52090921fa3f0357fe_281144_094bb953067b6ffb35d33ae102323146.webp&#34;
               width=&#34;760&#34;
               height=&#34;366&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;上图说明，对于使用对抗训练的模型，由于模型对于数据的微小扰动不敏感，模型就需要学习到更加复杂的决策边界，这也需要模型具有更大的 模型容量|容量 。因此需要换用容量更大更复杂的模型。&lt;/p&gt;
&lt;h2 id=&#34;结果分析&#34;&gt;结果分析&lt;/h2&gt;
&lt;p&gt;使用不同的对抗样本进行对抗训练并测试的结果如下，一方面增加模型容量能提升对抗攻击下的模型性能，使得最终学到的损失函数鞍点更小，另一方面使用PGD 算法作为对抗目标的效果比使用FGSM 快速梯度下降法更好。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;adversarial-training-exp.png&#34; srcset=&#34;
               /post/paper-adver-attack/adversarial-training-exp_hu60869119e0e0780cd7f691f57bdd0375_246143_373e56c95b7692f3aec7a7aab3fd0b61.webp 400w,
               /post/paper-adver-attack/adversarial-training-exp_hu60869119e0e0780cd7f691f57bdd0375_246143_af7ef3b82450cde535a1cf74acc196b2.webp 760w,
               /post/paper-adver-attack/adversarial-training-exp_hu60869119e0e0780cd7f691f57bdd0375_246143_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-adver-attack/adversarial-training-exp_hu60869119e0e0780cd7f691f57bdd0375_246143_373e56c95b7692f3aec7a7aab3fd0b61.webp&#34;
               width=&#34;760&#34;
               height=&#34;333&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;通过对抗样本参与训练可以增强模型的鲁棒性。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>【论文】Learning to Segment Every Thing</title>
      <link>https://yangleisx.github.io/post/paper-seg-every/</link>
      <pubDate>Sun, 07 Nov 2021 21:00:31 +0800</pubDate>
      <guid>https://yangleisx.github.io/post/paper-seg-every/</guid>
      <description>&lt;p&gt;论文题目：Learning to Segment Every Thing&lt;/p&gt;
&lt;p&gt;作者：Ronghang Hu，Piotr Dollar， Kaiming He，Trevor Darrell， Ross Girshick&lt;/p&gt;
&lt;p&gt;会议/时间：CVPR2018&lt;/p&gt;
&lt;p&gt;链接: &lt;a href=&#34;https://arXiv.org/pdf/1711.10370.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;论文目标&#34;&gt;论文目标&lt;/h2&gt;
&lt;p&gt;现有的Instance Segmentation 实例分割任务受到了数据集的限制，只能解决很少的几类目标的检测。由于像素级的分割标注成本非常高，因此文章提出了一种部分监督的训练方式，使用给定边界框的目标检测数据集训练实例分割模型。因此将实例分割扩展到几千类不同的目标，即题目中的“Segment Everything”。&lt;/p&gt;
&lt;p&gt;这里的部分监督表示，在训练过程中，一小部分训练数据给定了分割Mask作为强监督，其他的数据只给定了目标的Bounding Box作为弱监督。&lt;/p&gt;
&lt;h2 id=&#34;相关工作&#34;&gt;相关工作&lt;/h2&gt;
&lt;p&gt;Instance Segmentation 实例分割任务近年来受到关注，RCNN 系列#Mask R-CNN在实例分割任务中取得了很好的效果。缺点是需要大量的强监督，即“Fully Supervised Learning”。&lt;/p&gt;
&lt;p&gt;目前有一些通过预测得到模型参数的方式，例如Model Regression Network等方式通过模型预测的方式得到分类模型权重从而实现Zero-Shot Learning，LSDA模型通过一个图像分类模型的权重预测得到一个目标检测模型的权重。本论文中就使用一个预测模型得到分割头部的权重。&lt;/p&gt;
&lt;p&gt;目前也有一些弱监督学习实现Image Segmentation 图像分割的方法，例如使用边框标注、图像分类信息、目标大小等信息进行监督，但是大多只能解决Semantic Segmentation 语义分割任务。&lt;/p&gt;
&lt;p&gt;视觉嵌入类似Word Embedding 词嵌入的方式，将图像映射到空间中，其中语义信息相类似或者图像相似度比较高的被映射到相近的位置。在本文中认为Mask R-CNN的目标检测头部的参数可以认为是一种视觉嵌入的形式，蕴含了目标类别和位置的信息。&lt;/p&gt;
&lt;h2 id=&#34;本文方法&#34;&gt;本文方法&lt;/h2&gt;
&lt;p&gt;模型建立在RCNN 系列#Mask R-CNN的基础上，考虑到Mask R-CNN在边界框回归的目标检测模型（即RCNN 系列#Faster R-CNN）上添加了一个分割头部，因此可以直接应用在本文中。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;segeverything-model.png&#34; srcset=&#34;
               /post/paper-seg-every/segeverything-model_hu34ef8b38447309fdddc3cfb5f9849a4d_236779_869180cd97a9e7e07583befa30724fb8.webp 400w,
               /post/paper-seg-every/segeverything-model_hu34ef8b38447309fdddc3cfb5f9849a4d_236779_97321f7ced8463c5c21b9b7036327f5f.webp 760w,
               /post/paper-seg-every/segeverything-model_hu34ef8b38447309fdddc3cfb5f9849a4d_236779_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-seg-every/segeverything-model_hu34ef8b38447309fdddc3cfb5f9849a4d_236779_869180cd97a9e7e07583befa30724fb8.webp&#34;
               width=&#34;760&#34;
               height=&#34;305&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;模型中设计了一个转移函数$\mathcal{T}(\cdot)$实现了从检测模型参数到分割模型参数的转移，即$\omega_{seg}^c = \mathcal{T}(\omega_{det}^c; \theta)$。其中$c$表示对于指定的类型。这里的函数$\mathcal{T}$可以使用一个简单的全连接网络实现。&lt;/p&gt;
&lt;p&gt;在训练时，对于给定了Bounding Box和分割Mask的类别数据，同时使用Mask Loss和Box Loss监督。对于没有分割Mask监督的数据只使用Box Loss监督。&lt;/p&gt;
&lt;p&gt;具体训练包含两种不同的策略，一种是分级处理的训练方式，先训练一个目标检测模型，再固定Backbone和检测模型的参数，训练转移函数进行分割。另一种是端到端的训练，考虑到通常情况使用端到端的训练可以是模型综合不同的监督信号学习的更好，建议使用端到端的联合训练的方式。&lt;/p&gt;
&lt;p&gt;由于数据中一部分包含Mask监督，另一部分不包含，在训练的时候，对于检测参数$\omega_{det}$阻断梯度传播，即Mask Loss只参与了$\mathcal{T}(\cdot)$的参数更新。从而防止上述数据的不一致性影响到Backbone的训练。&lt;/p&gt;
&lt;p&gt;在Mask R-CNN中，分割头部使用FCN取得了更好的效果，考虑到在DeepMask中，使用MLP进行分割的效果也很好，因此在上述转移函数$\mathcal{T}(\cdot)$的基础上添加一个新的类型无关（Class-Agnostic）的MLP分支。&lt;/p&gt;
&lt;h2 id=&#34;结果分析&#34;&gt;结果分析&lt;/h2&gt;
&lt;p&gt;在MS COCO数据集上进行了比较详细的Ablation Study。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;segeverything-ablation.png&#34; srcset=&#34;
               /post/paper-seg-every/segeverything-ablation_hu3ae7b1eac63a5abfaebc507b95c6795c_316278_b2d6658470f6e1b928b620025202878d.webp 400w,
               /post/paper-seg-every/segeverything-ablation_hu3ae7b1eac63a5abfaebc507b95c6795c_316278_e8c4e788599d8a3bc3abb4b9351f8846.webp 760w,
               /post/paper-seg-every/segeverything-ablation_hu3ae7b1eac63a5abfaebc507b95c6795c_316278_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-seg-every/segeverything-ablation_hu3ae7b1eac63a5abfaebc507b95c6795c_316278_b2d6658470f6e1b928b620025202878d.webp&#34;
               width=&#34;760&#34;
               height=&#34;370&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

在Visual Genome数据集上使用3000种不同类型的数据进行训练，由于没有Mask标注，只能给出直观的结果。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;segeverything-quality.png&#34; srcset=&#34;
               /post/paper-seg-every/segeverything-quality_hu12811f4e4bca454720028377488a3d0a_2379987_58fa4c94bf1c12bc37067540b7ede91c.webp 400w,
               /post/paper-seg-every/segeverything-quality_hu12811f4e4bca454720028377488a3d0a_2379987_57e7ef7ef13a16210a79feb24e5d0df4.webp 760w,
               /post/paper-seg-every/segeverything-quality_hu12811f4e4bca454720028377488a3d0a_2379987_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-seg-every/segeverything-quality_hu12811f4e4bca454720028377488a3d0a_2379987_58fa4c94bf1c12bc37067540b7ede91c.webp&#34;
               width=&#34;760&#34;
               height=&#34;273&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;segeverything-quality2.pn&#34; alt=&#34;segeverything-quality2.pn&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

相比之前的结果，模型可以检测到更加抽象的概念，而且可以检测到组成一个目标的各个部分。&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;提出了一种部分监督的方式，可以根据部分给定分割图和部分只给定边界框的数据训练实例分割模型。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>【论文】Embedding Watermarks into Deep Neural Networks</title>
      <link>https://yangleisx.github.io/post/paper-uchida/</link>
      <pubDate>Tue, 19 Oct 2021 12:36:21 +0800</pubDate>
      <guid>https://yangleisx.github.io/post/paper-uchida/</guid>
      <description>&lt;p&gt;论文题目： Embedding Watermarks into Deep Neural Networks&lt;/p&gt;
&lt;p&gt;作者： Yusuke Uchida, Yuki Nagai,  Shigeyuki Sakazawa&lt;/p&gt;
&lt;p&gt;会议/时间：ICMR 2017&lt;/p&gt;
&lt;p&gt;链接: &lt;a href=&#34;https://arxiv.org/pdf/1701.04082.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;论文目标&#34;&gt;论文目标&lt;/h2&gt;
&lt;p&gt;目前预训练深度神经网络的应用越来越广，需要一种数字水印技术来保护预训练深度神经网络的知识产权，避免被滥用。论文首先定义了水印技术的场景，并提出了一种水印嵌入技术，可以在模型训练、精调或者蒸馏过程中嵌入到目标模型中且不影响模型性能。并且面对攻击者的精调和剪枝等行为时仍能保留在模型中。&lt;/p&gt;
&lt;h2 id=&#34;问题定义&#34;&gt;问题定义&lt;/h2&gt;
&lt;h3 id=&#34;模型水印要求&#34;&gt;模型水印要求&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;要求&lt;/th&gt;
&lt;th&gt;解释&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Fidelity&lt;/td&gt;
&lt;td&gt;添加水印之后模型的性能没有明显下降&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Robustness&lt;/td&gt;
&lt;td&gt;模型经过修改后水印仍存在&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Capacity&lt;/td&gt;
&lt;td&gt;水印技术应该能够嵌入大量信息&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Security&lt;/td&gt;
&lt;td&gt;水印应该难以被他人读写和修改&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Efficiency&lt;/td&gt;
&lt;td&gt;嵌入和提取水印的过程应该足够快&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;模型水印嵌入方法&#34;&gt;模型水印嵌入方法&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;在训练过程中嵌入（Train From Scratch）&lt;/li&gt;
&lt;li&gt;在精调过程中嵌入（Fine-Tuning）&lt;/li&gt;
&lt;li&gt;在蒸馏过程中嵌入（Model Distillation）&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;攻击方法&#34;&gt;攻击方法&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;精调（Fine Tuning）&lt;/li&gt;
&lt;li&gt;模型压缩（Model Compression）&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;本文方法&#34;&gt;本文方法&lt;/h2&gt;
&lt;p&gt;主要考虑卷积神经网络中卷积层的参数。对于一个$Kernel-Size=S$，输入通道数为$D$，输出通道数（卷积核数量）为$L$的卷积层，不考虑偏置，其参数为$W \in \mathbb{R}^{S\times S\times D\times L}$。计算多个卷积核的均值
$ {\bar W}_{i,j,k}=\frac{1}{L}\sum_l W_{i,j,k,l} $，并展平得到$w \in \mathbb{R}^M(M = S\times S\times D)$作为嵌入的目标。将$T$-bit的信息$b\in {0,1}^T$嵌入其中。&lt;/p&gt;
&lt;p&gt;在提取时，使用$b_j = s(\sum_i X_{ji}\omega_i)$计算嵌入的信息。其中$\omega$表示卷积核均值，$X \in \mathbb{R}^{T \times M}$表示嵌入密钥，$s(\cdot)$为阶跃函数。&lt;/p&gt;
&lt;p&gt;在训练时，为了将信息嵌入模型中，在原本的损失函数上添加了一个权重参数正则项$E(\omega)=E_0(\omega)+\lambda E_R(\omega)$。&lt;/p&gt;
&lt;p&gt;考虑到上述的模型提取方式类似二分类方法，因此添加的权重参数正则项使用BCE损失，使用训练过程中的参数提取结果作为监督。&lt;/p&gt;



$$\begin{aligned}
E_R(\omega)=&amp; -\sum\limits_{j=1}^{T}(b_j \log(y_j)+(1-b_j)\log(1-y_j)) \\
y_j =&amp; \sigma(\sum_i X_{ji}\omega_i)\\
\sigma(x) =&amp; \frac{1}{1+\exp(-x)}
\end{aligned}$$

&lt;p&gt;关于密钥$X$的设计，考虑到水印的性能，有三种设计形式：$X^{direct}$的每一行为独热码，直接将信息映射到参数中。$X^{diff}$的每一行包含一个1和一个-1，其他的为0，将信息映射到参数的差值中。$X^{random}$的数字采样于标准正态分布。&lt;/p&gt;
&lt;h2 id=&#34;结果分析&#34;&gt;结果分析&lt;/h2&gt;
&lt;h3 id=&#34;ablation-study&#34;&gt;Ablation Study&lt;/h3&gt;
&lt;p&gt;关于密钥$W$的设计，实验证明使用$X^{direct}$和$X^{diff}$嵌入都会造成较大的性能下降。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;uchilda-ablation1.png&#34; srcset=&#34;
               /post/paper-uchida/uchilda-ablation1_hu020f76b2fac645ca040c9c2b8808f584_142443_1bd43ab42c65fc7fdb45609cc364b9c1.webp 400w,
               /post/paper-uchida/uchilda-ablation1_hu020f76b2fac645ca040c9c2b8808f584_142443_ccbcb82027d7180cd0974a8bdfd23099.webp 760w,
               /post/paper-uchida/uchilda-ablation1_hu020f76b2fac645ca040c9c2b8808f584_142443_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-uchida/uchilda-ablation1_hu020f76b2fac645ca040c9c2b8808f584_142443_1bd43ab42c65fc7fdb45609cc364b9c1.webp&#34;
               width=&#34;760&#34;
               height=&#34;543&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;uchilda-ablation2.png&#34; srcset=&#34;
               /post/paper-uchida/uchilda-ablation2_huaef4544ec3ceb6f08c6e3feb7087e296_46728_1e07467dd2f6378ce66c6b35ed63ff23.webp 400w,
               /post/paper-uchida/uchilda-ablation2_huaef4544ec3ceb6f08c6e3feb7087e296_46728_1ab0144af431bf0f6e6e89f66de398d8.webp 760w,
               /post/paper-uchida/uchilda-ablation2_huaef4544ec3ceb6f08c6e3feb7087e296_46728_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-uchida/uchilda-ablation2_huaef4544ec3ceb6f08c6e3feb7087e296_46728_1e07467dd2f6378ce66c6b35ed63ff23.webp&#34;
               width=&#34;760&#34;
               height=&#34;275&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;而且可以看到$X^{random}$不仅不造成性能下降，而且对原始的参数分布影响不大。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;uchilda-abliation3.png&#34; srcset=&#34;
               /post/paper-uchida/uchilda-abliation3_hu8a490fff758d59cf3b93fae55dab670d_249995_a6ed559bf7cbc000374f8c1d8b49bcee.webp 400w,
               /post/paper-uchida/uchilda-abliation3_hu8a490fff758d59cf3b93fae55dab670d_249995_e82fa27997d65097fbd4801db80fb355.webp 760w,
               /post/paper-uchida/uchilda-abliation3_hu8a490fff758d59cf3b93fae55dab670d_249995_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-uchida/uchilda-abliation3_hu8a490fff758d59cf3b93fae55dab670d_249995_a6ed559bf7cbc000374f8c1d8b49bcee.webp&#34;
               width=&#34;760&#34;
               height=&#34;437&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;经过实验发现，当嵌入的信息量超过卷积层参数数量的时候，嵌入损失和性能下降会变得明显。而且采用直接嵌入的方式会难以在性能下降和嵌入损失之间达到均衡。&lt;/p&gt;
&lt;h3 id=&#34;robustness&#34;&gt;Robustness&lt;/h3&gt;
&lt;p&gt;实验证实这一方法可以应对Fine-Tuning和迁移学习。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;uchilda-robust1.png&#34; srcset=&#34;
               /post/paper-uchida/uchilda-robust1_huccaa388cb5e18fe485032a5cd4aac933_80855_1e815e01e225c4374d4915f17889c389.webp 400w,
               /post/paper-uchida/uchilda-robust1_huccaa388cb5e18fe485032a5cd4aac933_80855_ec7316e2cd02db623a28f1750a8e7ecd.webp 760w,
               /post/paper-uchida/uchilda-robust1_huccaa388cb5e18fe485032a5cd4aac933_80855_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-uchida/uchilda-robust1_huccaa388cb5e18fe485032a5cd4aac933_80855_1e815e01e225c4374d4915f17889c389.webp&#34;
               width=&#34;760&#34;
               height=&#34;177&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;面对剪枝操作时，特别是按照权重升序的剪枝方法时仍然能保留水印。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;uchilda-pruning1.png&#34; srcset=&#34;
               /post/paper-uchida/uchilda-pruning1_hue91a7d15437d84b47e8ef38ad2aed188_91203_27b08dd1792170c989c0f5c347a2b6a4.webp 400w,
               /post/paper-uchida/uchilda-pruning1_hue91a7d15437d84b47e8ef38ad2aed188_91203_c46dae44bca9c8d43f6139b063b93ec3.webp 760w,
               /post/paper-uchida/uchilda-pruning1_hue91a7d15437d84b47e8ef38ad2aed188_91203_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-uchida/uchilda-pruning1_hue91a7d15437d84b47e8ef38ad2aed188_91203_27b08dd1792170c989c0f5c347a2b6a4.webp&#34;
               width=&#34;760&#34;
               height=&#34;601&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;uchilda-pruning2.png&#34; srcset=&#34;
               /post/paper-uchida/uchilda-pruning2_hu3941a9e9d4bde31df4b05641e9000461_73404_c9f333fa2af0b1a553759469bced3d65.webp 400w,
               /post/paper-uchida/uchilda-pruning2_hu3941a9e9d4bde31df4b05641e9000461_73404_370e6c1705370d35e683fb378e996f50.webp 760w,
               /post/paper-uchida/uchilda-pruning2_hu3941a9e9d4bde31df4b05641e9000461_73404_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-uchida/uchilda-pruning2_hu3941a9e9d4bde31df4b05641e9000461_73404_c9f333fa2af0b1a553759469bced3d65.webp&#34;
               width=&#34;760&#34;
               height=&#34;629&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;提出了一种，为权重参数添加正则项的水印嵌入方法，其水印提取是通过矩阵映射的方式实现的。具有一定的鲁棒性。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>【论文】Look More Than Once: An Accurate Detector for Text of Arbitrary Shapes</title>
      <link>https://yangleisx.github.io/post/paper-lomo/</link>
      <pubDate>Fri, 15 Oct 2021 16:19:26 +0800</pubDate>
      <guid>https://yangleisx.github.io/post/paper-lomo/</guid>
      <description>&lt;p&gt;论文题目：Look More Than Once: An Accurate Detector for Text of Arbitrary Shapes&lt;/p&gt;
&lt;p&gt;作者：Chengquan Zhang， Borong Liang， Zuming Huang， Mengyi En，Junyu Han，Errui Ding， Xinghao Ding&lt;/p&gt;
&lt;p&gt;会议/时间：CVPR2019&lt;/p&gt;
&lt;p&gt;链接: &lt;a href=&#34;https://ieeexplore.ieee.org/document/8953775&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ieeexplore&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;论文目标&#34;&gt;论文目标&lt;/h2&gt;
&lt;p&gt;现有模型受到感受野的限制，很难实现对于长文本和弯曲文本的准确识别。论文中设计实现了一个LOMO网络，首先通过DR获得粗检测结果，再使用IRM迭代优化结果，最后使用SEM得到任意多边形的文本检测结果。&lt;/p&gt;
&lt;h2 id=&#34;相关工作&#34;&gt;相关工作&lt;/h2&gt;
&lt;p&gt;目前的文本检测有三种思路：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;基于Component，检测到文本部分然后通过后处理合并。例如CTPN、SegLink、WordSup等。&lt;/li&gt;
&lt;li&gt;基于Detection，类似通用的目标检测。例如TextBoxes、RRD、RRPN、EAST等。&lt;/li&gt;
&lt;li&gt;基于Segmentation，类似语义分割。例如TextSnake、PSENet等。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;本文方法&#34;&gt;本文方法&lt;/h2&gt;
&lt;p&gt;整体的模型结构如下。基本的特征提取部分使用ResNet50和FPN实现。最终得到1/4大小的128通道的特征图。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;lomo_structure&#34; srcset=&#34;
               /post/paper-lomo/lomo_structure_hue5e8f1c01a26b4b030fc1d74546bd49e_76805_90fa04cb99324adf5342fc0c2d8bb44f.webp 400w,
               /post/paper-lomo/lomo_structure_hue5e8f1c01a26b4b030fc1d74546bd49e_76805_8154e8a1b9b69e022ffc9331f28fdd46.webp 760w,
               /post/paper-lomo/lomo_structure_hue5e8f1c01a26b4b030fc1d74546bd49e_76805_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-lomo/lomo_structure_hue5e8f1c01a26b4b030fc1d74546bd49e_76805_90fa04cb99324adf5342fc0c2d8bb44f.webp&#34;
               width=&#34;760&#34;
               height=&#34;222&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;提取到的图像特征首先经过Direct Regressor得到粗检测结果。这里DR的设计与EAST基本相同，包括test/non-text分类结果和四个边界点的偏移值。在训练分类结果时使用了一种改进的Dice-Loss。其中的权重$w$被设置为与文本框的短边长度称反比。
$$L_{cls} = 1 - \frac{2 * sum(y * \hat{y} * w)}{sum(y*w) + sum(\hat{y} * w)}$$&lt;/p&gt;
&lt;p&gt;IRM的结构如下。首先根据DR的结果，从FPN的特征中使用ROI Transform提取特征，并使用卷积和Sigmoid激活得到四个通道的注意力图（分别为四个边角）。通过Reduce-Sum和卷积得到四个边角的偏移量。对DR的结果进行修正。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;lomo_irm&#34; srcset=&#34;
               /post/paper-lomo/lomo_irm_hua6a8cc7fafa855021711f1ab194f25b8_224730_030a8c2816d9869d78572a4437806822.webp 400w,
               /post/paper-lomo/lomo_irm_hua6a8cc7fafa855021711f1ab194f25b8_224730_1de9dc4e39bd20eab806ddac74fa802e.webp 760w,
               /post/paper-lomo/lomo_irm_hua6a8cc7fafa855021711f1ab194f25b8_224730_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-lomo/lomo_irm_hua6a8cc7fafa855021711f1ab194f25b8_224730_030a8c2816d9869d78572a4437806822.webp&#34;
               width=&#34;760&#34;
               height=&#34;408&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;IRM部分的损失函数为使用Smoothed-L1来监督。
$$L_{irm} = \frac{1}{K*8}\sum\limits_{k=1}^{K}\sum\limits_{j=1}^{8}smooth_{L1}(c_k^j - \hat{c}_k^j)$$&lt;/p&gt;
&lt;p&gt;SEM的结构如下。根据IRM修正之后的结果，进一步得到多边形框的结果。除去上述检测框中的背景部分。首先同样是使用ROI Transform提取特征，然后经过两次上采样之后，再生成预测结果，包括文本区域、文本中心线（收缩后的文本区域）、边界偏置。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;lomo_sem&#34; srcset=&#34;
               /post/paper-lomo/lomo_sem_hu197b4a28fc238235005fe279f206a35c_463791_dcab21e3b7cb6bbd41effc2d60e9e701.webp 400w,
               /post/paper-lomo/lomo_sem_hu197b4a28fc238235005fe279f206a35c_463791_c500b747ebe86e80595781b39993d294.webp 760w,
               /post/paper-lomo/lomo_sem_hu197b4a28fc238235005fe279f206a35c_463791_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-lomo/lomo_sem_hu197b4a28fc238235005fe279f206a35c_463791_dcab21e3b7cb6bbd41effc2d60e9e701.webp&#34;
               width=&#34;760&#34;
               height=&#34;334&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;要想得到文本检测结果，需要首先在文本中心线上采样N个点，然后根据边界偏置得到文本行的上下边界上的控制点，相连接得到多边形框。&lt;/p&gt;
&lt;p&gt;在训练时，首先使用生成数据对DR部分进行训练，然后才使用真实数据同时训练三个分支。为了防止训练时DR产生的错误结果影响另外两分支的训练，将DR结果中的50%随机替换为GT。在预测的时候DR的结果经过NMS之后再经过多次IRM，最后通过SEM得到结果。&lt;/p&gt;
&lt;h2 id=&#34;结果分析&#34;&gt;结果分析&lt;/h2&gt;
&lt;p&gt;ablation study显示，IRM迭代次数增加可以提升模型性能，但是相应的处理时间增加，因此权衡之后设置为2。在IRM中，添加四个角点的注意力图也可以提升模型性能。&lt;/p&gt;
&lt;p&gt;ablation study显示，添加SEM可以显著提升模型性能（7.17%）。对于SEM结果的处理中，采样点数量增加效果提升并逐渐收敛，因此选择为7。&lt;/p&gt;
&lt;p&gt;在长文本数据集ICDAR2017-RCTW上的效果如下。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;lomo_exp&#34; srcset=&#34;
               /post/paper-lomo/lomo_exp_hue6cf532ef9bb217ee46ab7eb14485d58_87274_c25b22cd1421810eef90d6091023cd11.webp 400w,
               /post/paper-lomo/lomo_exp_hue6cf532ef9bb217ee46ab7eb14485d58_87274_013795f36ed5a8900db90377dbb84780.webp 760w,
               /post/paper-lomo/lomo_exp_hue6cf532ef9bb217ee46ab7eb14485d58_87274_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-lomo/lomo_exp_hue6cf532ef9bb217ee46ab7eb14485d58_87274_c25b22cd1421810eef90d6091023cd11.webp&#34;
               width=&#34;760&#34;
               height=&#34;369&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;在弯曲文本数据集ICDAR2015上的效果如下。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;lomo_exp_ic15&#34; srcset=&#34;
               /post/paper-lomo/lomo_exp_ic15_hu6ba3ca2b944b614b37850df321849ae3_197529_368999009f1fcda3df853bd004c83b20.webp 400w,
               /post/paper-lomo/lomo_exp_ic15_hu6ba3ca2b944b614b37850df321849ae3_197529_550e6de229affa5bc7a23a582e8b28b9.webp 760w,
               /post/paper-lomo/lomo_exp_ic15_hu6ba3ca2b944b614b37850df321849ae3_197529_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-lomo/lomo_exp_ic15_hu6ba3ca2b944b614b37850df321849ae3_197529_368999009f1fcda3df853bd004c83b20.webp&#34;
               width=&#34;760&#34;
               height=&#34;735&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;在多语言数据集ICDAR2017-MLT上的效果如下。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;lomo_exp_ic17&#34; srcset=&#34;
               /post/paper-lomo/lomo_exp_ic17_hu48cfa2842547b2094ad750e232ad9020_114588_31408eaa332c57853f96034bb042dbfa.webp 400w,
               /post/paper-lomo/lomo_exp_ic17_hu48cfa2842547b2094ad750e232ad9020_114588_12907b4145ded085e0c5feb464cdef92.webp 760w,
               /post/paper-lomo/lomo_exp_ic17_hu48cfa2842547b2094ad750e232ad9020_114588_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-lomo/lomo_exp_ic17_hu48cfa2842547b2094ad750e232ad9020_114588_31408eaa332c57853f96034bb042dbfa.webp&#34;
               width=&#34;760&#34;
               height=&#34;550&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;设计了IRM在粗检测结果上精调。
设计了SEM实现文本框形状的优化，从四边形变换为任意形状。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>【论文】DE-GAN: A Conditional Generative Adversarial Network for Document Enhancement</title>
      <link>https://yangleisx.github.io/post/paper-de-gan/</link>
      <pubDate>Sun, 22 Aug 2021 14:20:44 +0800</pubDate>
      <guid>https://yangleisx.github.io/post/paper-de-gan/</guid>
      <description>&lt;p&gt;论文题目：DE-GAN： A Conditional Generative Adversarial Network for Document Enhancement&lt;/p&gt;
&lt;p&gt;作者：Mohamed Ali Souibgui and Yousri Kessentini&lt;/p&gt;
&lt;p&gt;会议/时间：arXiv&lt;/p&gt;
&lt;p&gt;链接: &lt;a href=&#34;https://arxiv.org/abs/2010.08764v1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;论文目标&#34;&gt;论文目标&lt;/h2&gt;
&lt;p&gt;文档图像中通常会拥有比较多的退化（Degradation），因此在使用OCR系统处理文档图像的时候效果比较差，论文通过生成对抗网络（GAN）实现了一个文本增强模型，在例如去除模糊、去除污渍、去除水印、二值化等多种文档增强人物上具有较好的效果。&lt;/p&gt;
&lt;p&gt;在通常的文档去除污渍、去除水印等任务中，对于这些水印/退化条件缺少先验知识，特别是污渍将字符完全覆盖的情况。因此很难用传统的方法去除。近年来生成模型在应对缺失数据、多模态任务中取得非常好的结果。因此在论文中，首次在文档增强任务中使用了生成对抗网络来解决二值化和去除水印的任务。而且论文中还提出了密集水印/印章移除任务。&lt;/p&gt;
&lt;h2 id=&#34;相关工作&#34;&gt;相关工作&lt;/h2&gt;
&lt;p&gt;文档图像修复和二值化任务旨在将文本和背景/污渍相区分，传统方法中大多使用设置阈值的方式，包括全局阈值、局部阈值搜索等方式，但是传统方法对于文档状态非常敏感，不够通用。在使用深度神经网络的方法中，整体思路类似图像分割，输出像素级的预测结果，判断当前像素为文本前景还是污渍背景。在论文中所使用的文档增强任务将污渍背景预测为正确的颜色/灰度，而不是设置为0。&lt;/p&gt;
&lt;p&gt;在水印移除任务中，目标是将水印前景与文本背景相分离，多出现在自然图像处理（Natural Image Processing）任务中，此前常见的做法首先使用通用的目标检测算法检测水印/印章的位置然后再移除。&lt;/p&gt;
&lt;p&gt;目前生成对抗网络已经广泛应用于图像到图像的转换任务中，例如语义分割、图像超分辨率等。其中conditional GANs常被应用在一些与文档增强相似的任务中。例如CycleGAN、pix2pix-HD等模型。&lt;/p&gt;
&lt;h2 id=&#34;本文方法&#34;&gt;本文方法&lt;/h2&gt;
&lt;p&gt;首先将文档增强任务定义为图像到图像的转换任务。即给定原始图像生成对应的干净的图像。在最简单的GAN网络中，给定随机抽样$z$，生成图像$y$。在conditional GAN网络中，给定了一个另外的输入参数$x$，生成器依据输入$x$和随机向量$z$生成$y$，即 $G_{\varphi_G}:{x,z}\to y$，判别器依据输入$x$判别$y$的真伪，即 $D_{\varphi_D}: {x, y}\to P(real)$。&lt;/p&gt;
&lt;p&gt;在最简单的conditional GAN网络中，给定输入为$I^W$，Ground Truth为 $I^{GT}$，模型的对抗损失如下。即最小化判别器分类的损失。



$$\begin{aligned}
L_{GAN}(\varphi_G, \varphi_D) &amp;= \mathbb{E}_{I^W, I^{GT}}\log[D_{\varphi_D}(I^W, I^{GT})] \\
&amp;+ \mathbb{E}_{I_W} \log[1 - D_{\varphi_D}(I^W, G_{\varphi_G}(I^W))]
\end{aligned}$$
&lt;/p&gt;
&lt;p&gt;在论文中，为了加速模型的训练，添加了额外的辅助损失指导模型的训练。公式如下。（添加了生成器输出结果和Ground Truth之间的交叉熵损失？）&lt;/p&gt;



$$\begin{aligned}
L_{log}(\varphi_G) &amp;= \mathbb{E}_{I^{GT}, I^W}[-(I^{GT}\log(G_{\varphi_G}(I^W)) \\
&amp;+ ((1-I^{GT})\log(1-G_{\varphi_G}(I^W))))] \\
L_{net}(\varphi_G, \varphi_D) &amp;= \min_{\varphi_G}\max_{\varphi_D}L_{GAN}(\varphi_G, \varphi_D) + \lambda L_{log}(\varphi_G)
\end{aligned}$$

&lt;p&gt;模型整体结构如下。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;degan-framework.png&#34; srcset=&#34;
               /post/paper-de-gan/degan-framework_hu4285d5e44b7de519051d72788fb69a9c_33383_7bf16cb7f8da5cd349efe315b6fa95d2.webp 400w,
               /post/paper-de-gan/degan-framework_hu4285d5e44b7de519051d72788fb69a9c_33383_f65069218b402cabdbde4510434d802f.webp 760w,
               /post/paper-de-gan/degan-framework_hu4285d5e44b7de519051d72788fb69a9c_33383_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-de-gan/degan-framework_hu4285d5e44b7de519051d72788fb69a9c_33383_7bf16cb7f8da5cd349efe315b6fa95d2.webp&#34;
               width=&#34;708&#34;
               height=&#34;354&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;对于生成器，使用了基于U-Net的结构，添加旁路链接可以减少信息的丢失，也可以使模型学习更加高级的特征。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;degan-g.png&#34; srcset=&#34;
               /post/paper-de-gan/degan-g_hue75605222a4990094c388307a3a660bb_98344_55317096c391148a0604af9501f36ec2.webp 400w,
               /post/paper-de-gan/degan-g_hue75605222a4990094c388307a3a660bb_98344_689f545cf7b69f66aeed511863227af8.webp 760w,
               /post/paper-de-gan/degan-g_hue75605222a4990094c388307a3a660bb_98344_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-de-gan/degan-g_hue75605222a4990094c388307a3a660bb_98344_55317096c391148a0604af9501f36ec2.webp&#34;
               width=&#34;760&#34;
               height=&#34;314&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;对于判别器，使用简单的多层卷积的结构。在输入层将生成器输出的图像与Ground Truth通过Concatenate相连接。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;degan-d.png&#34; srcset=&#34;
               /post/paper-de-gan/degan-d_hu5ea99f2437ccf78ca339eb691113dbfa_34118_76bfb52ab4cb6adbf17279799c514a84.webp 400w,
               /post/paper-de-gan/degan-d_hu5ea99f2437ccf78ca339eb691113dbfa_34118_e959223c55dbf347ac9a74d81e1e1401.webp 760w,
               /post/paper-de-gan/degan-d_hu5ea99f2437ccf78ca339eb691113dbfa_34118_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-de-gan/degan-d_hu5ea99f2437ccf78ca339eb691113dbfa_34118_76bfb52ab4cb6adbf17279799c514a84.webp&#34;
               width=&#34;604&#34;
               height=&#34;642&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;结果分析&#34;&gt;结果分析&lt;/h2&gt;
&lt;p&gt;在文档清洁和二值化任务中，损失函数中的$\lambda$设置为100。与最简单的U-Net相比，模型性能有所提升。说明使用对抗训练可以增强模型的性能。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;degan-exp10.png&#34; srcset=&#34;
               /post/paper-de-gan/degan-exp10_hu74ec59108980cdd3448894a323d146e9_19184_3e0cc3c0031f4926121b9d91fbd2a20d.webp 400w,
               /post/paper-de-gan/degan-exp10_hu74ec59108980cdd3448894a323d146e9_19184_da96817c2db5c1cc5fac6fa41ec02cc6.webp 760w,
               /post/paper-de-gan/degan-exp10_hu74ec59108980cdd3448894a323d146e9_19184_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-de-gan/degan-exp10_hu74ec59108980cdd3448894a323d146e9_19184_3e0cc3c0031f4926121b9d91fbd2a20d.webp&#34;
               width=&#34;436&#34;
               height=&#34;130&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;在DIBCO2013上与现有模型相比达到了SOTA的效果。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;degan-exp11.png&#34; srcset=&#34;
               /post/paper-de-gan/degan-exp11_hub1fa137e22eed7acec6253b636f4e6e4_85070_dc68b2360788bab53597333ac5cf8f80.webp 400w,
               /post/paper-de-gan/degan-exp11_hub1fa137e22eed7acec6253b636f4e6e4_85070_3a2def9166f6326d42384bc56ca97267.webp 760w,
               /post/paper-de-gan/degan-exp11_hub1fa137e22eed7acec6253b636f4e6e4_85070_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-de-gan/degan-exp11_hub1fa137e22eed7acec6253b636f4e6e4_85070_dc68b2360788bab53597333ac5cf8f80.webp&#34;
               width=&#34;692&#34;
               height=&#34;392&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;在水印和印章移除任务中，损失函数中的$\lambda$设置为500。与现有模型相比达到了SOTA的效果。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;degan-exp20.png&#34; srcset=&#34;
               /post/paper-de-gan/degan-exp20_hu71168d6e8ecfcb161672ce4d23aeaea5_29187_161015f113cc97c2eea4aad62a30273a.webp 400w,
               /post/paper-de-gan/degan-exp20_hu71168d6e8ecfcb161672ce4d23aeaea5_29187_40bc5be3a895784023fe40741d3f9166.webp 760w,
               /post/paper-de-gan/degan-exp20_hu71168d6e8ecfcb161672ce4d23aeaea5_29187_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-de-gan/degan-exp20_hu71168d6e8ecfcb161672ce4d23aeaea5_29187_161015f113cc97c2eea4aad62a30273a.webp&#34;
               width=&#34;432&#34;
               height=&#34;194&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;除此之外，还在文档去模糊等任务上和下游的OCR任务上做了测试，均取得了非常好的效果。&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;论文中首次提出了使用GAN解决文本增强任务。文中提出的DE-GAN可以认为是pix2pix模型的升级版，通过修改生成器和辅助损失函数提升了模型性能，在很多文档增强任务中都取得了非常好的效果。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>【论文】ABCNet: Real-time Scene Text Spotting with Adaptive Bezier-Curve Network</title>
      <link>https://yangleisx.github.io/post/paper-abcnet/</link>
      <pubDate>Thu, 19 Aug 2021 16:26:18 +0800</pubDate>
      <guid>https://yangleisx.github.io/post/paper-abcnet/</guid>
      <description>&lt;p&gt;论文题目：【论文】ABCNet： Real-time Scene Text Spotting with Adaptive Bezier-Curve Network&lt;/p&gt;
&lt;p&gt;作者：Yuliang Liu, Hao Chen, Chunhua Shen, Tong He, Lianwen Jin, Liangwei Wang&lt;/p&gt;
&lt;p&gt;会议/时间：CVPR2020&lt;/p&gt;
&lt;p&gt;链接: &lt;a href=&#34;https://arxiv.org/pdf/2002.10200v2.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;论文目标&#34;&gt;论文目标&lt;/h2&gt;
&lt;p&gt;目前的文本检测和识别任务，包括基于字符或者基于分割的方式，通常需要负责的后处理流程提取得到文本边界框或者是负责的模型结构和处理流程。文章通过提出自适应的贝塞尔曲线网络，实现了端到端的检测和识别任务，同时避免引入计算开支，在保证较高的准确率的时候具有较高的效率。使用贝塞尔曲线作为边界框也可以减少任意形状文本的表示成本。&lt;/p&gt;
&lt;h2 id=&#34;相关工作&#34;&gt;相关工作&lt;/h2&gt;
&lt;p&gt;端到端的文本识别任务通常包括文本检测和文本识别两个阶段。在通用的端到端文本识别任务中，可以使用RoI Pooling等方式将检测阶段提取到的特征应用在识别任务中。类似的思路还包括RoI Masking、RoI Transform、Text Align Sampling、RoI Slide等方式。在面向任意形状文本的端到端识别任务中，关键在于在检测阶段使用什么方式来表示任意形状的文本，例如使用多边形边框或者使用基于单字符的方式。&lt;/p&gt;
&lt;h2 id=&#34;本文方法&#34;&gt;本文方法&lt;/h2&gt;
&lt;p&gt;模型整体结构如下，主要包括贝塞尔曲线表示的文本检测，贝塞尔对齐和轻量级的文本识别网络。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;abc-framework.png&#34; srcset=&#34;
               /post/paper-abcnet/abc-framework_hu13fcdaf3d6c375b986558f1742fdd23c_1135898_4dc9c4365cc5f761eed146ea8d235b55.webp 400w,
               /post/paper-abcnet/abc-framework_hu13fcdaf3d6c375b986558f1742fdd23c_1135898_50c04ebc8384578ae851260e8a25d942.webp 760w,
               /post/paper-abcnet/abc-framework_hu13fcdaf3d6c375b986558f1742fdd23c_1135898_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-abcnet/abc-framework_hu13fcdaf3d6c375b986558f1742fdd23c_1135898_4dc9c4365cc5f761eed146ea8d235b55.webp&#34;
               width=&#34;760&#34;
               height=&#34;258&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;在模型中，将文本边框使用贝塞尔曲线表示。其中文本行的长边使用三阶贝塞尔曲线表示，短边则直接相连。因此每个文本行对象使用8个控制点表示。模型的预测输出阶段输出16个通道的数据，表示这8个控制点到外接正矩形左上角的偏移量（其实是计算 $\Delta x = b_{ix}-x_{min}, \Delta y = b_{iy}-y_{min}$）。这样可以将控制点位于图像边界之外的情况也考虑进来。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;abc-curve.png&#34; srcset=&#34;
               /post/paper-abcnet/abc-curve_hu6de74e0599c6f15c9e37504ac664e03c_556476_f7b179bb25a0c57241f199ca6cfbafd4.webp 400w,
               /post/paper-abcnet/abc-curve_hu6de74e0599c6f15c9e37504ac664e03c_556476_ab9fa1d54bbac73ae93a11804d8acfbc.webp 760w,
               /post/paper-abcnet/abc-curve_hu6de74e0599c6f15c9e37504ac664e03c_556476_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-abcnet/abc-curve_hu6de74e0599c6f15c9e37504ac664e03c_556476_f7b179bb25a0c57241f199ca6cfbafd4.webp&#34;
               width=&#34;760&#34;
               height=&#34;263&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;由于使用贝塞尔曲线表示任意形状的文本位置，在训练时需要将数据集中使用矩形/多边形的标注转换为贝塞尔曲线的形式。这里使用最小二乘的方式优化求解如下方程，可以得到多边形标注对应的贝塞尔曲线控制点坐标。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;abc-bezier.png&#34; srcset=&#34;
               /post/paper-abcnet/abc-bezier_hu636676d3db2b19535a3d87cd9127eda8_54443_c9137640f0d05155e293f27df8e4a58e.webp 400w,
               /post/paper-abcnet/abc-bezier_hu636676d3db2b19535a3d87cd9127eda8_54443_4f6d8677921c03dba0028e53be35d9fb.webp 760w,
               /post/paper-abcnet/abc-bezier_hu636676d3db2b19535a3d87cd9127eda8_54443_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-abcnet/abc-bezier_hu636676d3db2b19535a3d87cd9127eda8_54443_c9137640f0d05155e293f27df8e4a58e.webp&#34;
               width=&#34;760&#34;
               height=&#34;201&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;在提取特征用于识别模型时，文中提出了BezierAlign方式，取代RoI Pooling等其他的方式，可以提取到更加稳定的特征。具体的方式为在平行于贝塞尔曲线的位置进行等距离采样，采用双线性差值的方式得到输出的特征值。公式如下。其中，输出特征值为 $h_{out} \times w_{out}$。对于输出特征图上的点 $g_i$，坐标为 $(g_{iw}, g_{ih})$，根据 $t$计算上下贝塞尔曲线上的采样点 $tp$和 $bp$，从而得到在原图中的采样点 $op$，再使用双线性差值的方式提取特征。



$$\begin{aligned}
t &amp;= \frac{g_{iw}}{w_{out}} \\
op &amp;= bp \cdot \frac{g_{ih}}{h_{out}} + tp\cdot (1 - \frac{g_{ih}}{h_{out}})
\end{aligned}$$

对齐结果如下。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;abc-align.png&#34; srcset=&#34;
               /post/paper-abcnet/abc-align_hu51b62bfd7b53bfac21732b5f157f673d_947342_321953791497cfc7d8508ba6ecb4aaf9.webp 400w,
               /post/paper-abcnet/abc-align_hu51b62bfd7b53bfac21732b5f157f673d_947342_ba899f44160fde32e1f73e84b13bfbf9.webp 760w,
               /post/paper-abcnet/abc-align_hu51b62bfd7b53bfac21732b5f157f673d_947342_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-abcnet/abc-align_hu51b62bfd7b53bfac21732b5f157f673d_947342_321953791497cfc7d8508ba6ecb4aaf9.webp&#34;
               width=&#34;760&#34;
               height=&#34;397&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;在训练的时候，使用Ground Truth中的边界框从检测阶段中提取特征用于识别模块的训练，避免检测阶段尚未收敛时产生的影响。&lt;/p&gt;
&lt;h2 id=&#34;结果分析&#34;&gt;结果分析&lt;/h2&gt;
&lt;p&gt;在Total-Text数据集上进行Ablation Study。可以证明文中提出的BezierAlign相比其他的特征提取和对齐方式能提高性能，同时BezierAlign受到采样数量的影响，考虑到性能和速度的折衷，经过实验之后选择了（7，32）。&lt;/p&gt;
&lt;p&gt;在Total-Text和CTW1500上的测试表示模型性能达到了SOTA。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;abc-sota1.png&#34; srcset=&#34;
               /post/paper-abcnet/abc-sota1_hu20818bef5b5e4fcc8784dc9f678bbd54_263383_943ee9c7a16930b417b4586bf8eca43d.webp 400w,
               /post/paper-abcnet/abc-sota1_hu20818bef5b5e4fcc8784dc9f678bbd54_263383_c453214c3ed4ce214be34c44b847c1cb.webp 760w,
               /post/paper-abcnet/abc-sota1_hu20818bef5b5e4fcc8784dc9f678bbd54_263383_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-abcnet/abc-sota1_hu20818bef5b5e4fcc8784dc9f678bbd54_263383_943ee9c7a16930b417b4586bf8eca43d.webp&#34;
               width=&#34;760&#34;
               height=&#34;275&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;abc-sota2.png&#34; srcset=&#34;
               /post/paper-abcnet/abc-sota2_hu028a8156b3c75188e982fc12b0fba2d6_100681_a86f0b36079159962b39c764c27ed9f7.webp 400w,
               /post/paper-abcnet/abc-sota2_hu028a8156b3c75188e982fc12b0fba2d6_100681_3da6875338cfa0872395a6a6c1c02754.webp 760w,
               /post/paper-abcnet/abc-sota2_hu028a8156b3c75188e982fc12b0fba2d6_100681_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-abcnet/abc-sota2_hu028a8156b3c75188e982fc12b0fba2d6_100681_a86f0b36079159962b39c764c27ed9f7.webp&#34;
               width=&#34;760&#34;
               height=&#34;291&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;提出了使用Bezier表示任意形状文本边界的方式，结合BezierAlign可以实现性能的提升。同时不引入多余的计算量，具有较高的效率。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>【论文】Adaptive Boundary Proposal Network for Arbitrary Shape Text Detection</title>
      <link>https://yangleisx.github.io/post/paper-bpn/</link>
      <pubDate>Wed, 18 Aug 2021 10:05:23 +0800</pubDate>
      <guid>https://yangleisx.github.io/post/paper-bpn/</guid>
      <description>&lt;p&gt;论文题目：Adaptive Boundary Proposal Network for Arbitrary Shape Text Detection&lt;/p&gt;
&lt;p&gt;作者：Shi-Xue Zhang, Xiaobin Zhu, Chun Yang, Hongfa Wang, Xu-Cheng Yin&lt;/p&gt;
&lt;p&gt;会议/时间：ICCV2021&lt;/p&gt;
&lt;p&gt;链接: &lt;a href=&#34;http://arxiv.org/abs/2107.12664&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;论文目标&#34;&gt;论文目标&lt;/h2&gt;
&lt;p&gt;目前的任意形状文本检测工作尽管取得了不错的结果，但是仍然存在两个问题，一个是基于分割的文本检测方案需要复杂的后处理流程，从分割图中提取文本边框坐标，而且很难区分间距比较小的文本目标，。另一个问题是，基于分割的方法很容易受到图像中的噪声等影响，特别是由于文本没有比较完整的轮廓，基于轮廓的检测方式效果比较差。&lt;/p&gt;
&lt;p&gt;在文中提出了一个端到端的文本检测模型，包括BPN(Boundary Proposal Network)和一个迭代的变形模型，可以不经过任何后处理就直接生成任意形状文本的边界位置。&lt;/p&gt;
&lt;h2 id=&#34;相关工作&#34;&gt;相关工作&lt;/h2&gt;
&lt;p&gt;基于回归的方法通常计算生成的anchor或者当前像素到图文边界框的距离，很难识别任意形状的文本或者是宽高比较大的文本。基于联通成分的方法通常找到文本中的单独成分然后连接起来，需要复杂的后处理流程。基于分割的方法通常通过文本边界来进行像素级预测，但是难以区分比较近的多个对象。基于轮廓点的方法学习寻找文本对象的轮廓点，相比分割的方法具有更高的效率和准确度。&lt;/p&gt;
&lt;h2 id=&#34;本文方法&#34;&gt;本文方法&lt;/h2&gt;
&lt;p&gt;模型主要结构如图，包含三个部分，最前方的特征提取（Shared Convolution）、边界建议网络（Boundary Proposal Network，BPN）和自适应边界修正模型（Adaptive Deformation Model）。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;bpm-framwork.png&#34; srcset=&#34;
               /post/paper-bpn/bpm-framwork_hu024d5da65794d10a935b8d9aed4dc0af_597612_ec3c20169353fa2cfb0fafb0bcf25aea.webp 400w,
               /post/paper-bpn/bpm-framwork_hu024d5da65794d10a935b8d9aed4dc0af_597612_271575792534b44b1f18a0c47c7e4a8c.webp 760w,
               /post/paper-bpn/bpm-framwork_hu024d5da65794d10a935b8d9aed4dc0af_597612_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-bpn/bpm-framwork_hu024d5da65794d10a935b8d9aed4dc0af_597612_ec3c20169353fa2cfb0fafb0bcf25aea.webp&#34;
               width=&#34;760&#34;
               height=&#34;271&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;特征提取模块如图，是基于ResNet-50的类似FPN/U-Net的结构。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;bpm-extract.png&#34; srcset=&#34;
               /post/paper-bpn/bpm-extract_hu7ab58675c64ef03ef3ed367439d390a8_147485_85c7b3e71ac65c21249fb8a867016c11.webp 400w,
               /post/paper-bpn/bpm-extract_hu7ab58675c64ef03ef3ed367439d390a8_147485_0189e9b45fc98475698c2d5277531626.webp 760w,
               /post/paper-bpn/bpm-extract_hu7ab58675c64ef03ef3ed367439d390a8_147485_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-bpn/bpm-extract_hu7ab58675c64ef03ef3ed367439d390a8_147485_85c7b3e71ac65c21249fb8a867016c11.webp&#34;
               width=&#34;680&#34;
               height=&#34;652&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;在BPN中，输出为四个通道：text/non-text classification、distance field和direction field。其中距离图和方向图表示当前点距离边界框上最近的点的距离和方向。通过距离和方向可以从当前点的坐标导出边界框的坐标。相关的监督数据如下图所示。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;bpm-datagen.png&#34; srcset=&#34;
               /post/paper-bpn/bpm-datagen_hu98c8eda15567f7848de14f86b231474d_512199_259c1d883c43e9ce665042474de98e50.webp 400w,
               /post/paper-bpn/bpm-datagen_hu98c8eda15567f7848de14f86b231474d_512199_80c12eb0ebb2e0383ccd0691be4fc7da.webp 760w,
               /post/paper-bpn/bpm-datagen_hu98c8eda15567f7848de14f86b231474d_512199_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-bpn/bpm-datagen_hu98c8eda15567f7848de14f86b231474d_512199_259c1d883c43e9ce665042474de98e50.webp&#34;
               width=&#34;760&#34;
               height=&#34;660&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;BPN输出的四个通道数据与特征提取模块输出的特征相拼接。从中选择N个点，每个点特征长度为C，得到了NxC的矩阵输入修正模型。&lt;/p&gt;
&lt;p&gt;在自适应修正模型中，分为编码器和解码器两部分。编码器通过三个并行的模块：RNN（使用Bi-LSTM），GCN（将每个点与相近的四个点相连接），CNN（1x1的卷积）之后再连接起来进入解码器。解码器使用全连接层输出正确结果距离当前结果的偏置值用于更新。&lt;/p&gt;
&lt;p&gt;在训练修正模型时采用迭代的方式。整体的损失函数如下。



$$\begin{aligned}
L =&amp; L_{BP} + \lambda\frac{L_{BD}}{1 + e^{(i-eps) / eps}} \\
L_{BP} =&amp; L_{cls} + \alpha \times L_D + L_V \\
L_V =&amp; \sum w(p)||V_p - \hat{V}_p||_2 + \frac{1}{T}\sum(1 - cos(V_p \hat{V}_p)) \\
w(p) =&amp; \frac{1}{\sqrt{GT_p}} \\
L_{BD} =&amp; \frac{1}{T}\sum L_{(p,p&#39;)} \\
L_{(p,p&#39;)} =&amp; \min\limits_{j \in [0,1,2,...N-1]}\sum\limits_{i=0}^{N-1} smooth_{L1} (p_i, p&#39;_{(i+j)\% N}) \\
\end{aligned}$$
&lt;/p&gt;
&lt;h2 id=&#34;结果分析&#34;&gt;结果分析&lt;/h2&gt;
&lt;p&gt;实验在Total-Text、CTW-1500、MSRA-TD500、SynthText、ICDAR2017-MLT数据集上训练。&lt;/p&gt;
&lt;p&gt;经过Ablation Study可以证明提出的Adaptive Deformation Model可以提高模型的性能。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;bpm-ablation.png&#34; srcset=&#34;
               /post/paper-bpn/bpm-ablation_hu519d8687a55a664363c5ff216d1b53ce_197512_5e62beb57334a9bf6cd82d3b626100d8.webp 400w,
               /post/paper-bpn/bpm-ablation_hu519d8687a55a664363c5ff216d1b53ce_197512_255bc3bdab81a072b8b6da0a128e3732.webp 760w,
               /post/paper-bpn/bpm-ablation_hu519d8687a55a664363c5ff216d1b53ce_197512_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-bpn/bpm-ablation_hu519d8687a55a664363c5ff216d1b53ce_197512_5e62beb57334a9bf6cd82d3b626100d8.webp&#34;
               width=&#34;760&#34;
               height=&#34;169&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;经过实验，每个目标的控制点（轮廓点）数量为20、迭代修正次数为3时效果较好，综合距离图、方向图的效果比只使用分类图要好，FPN分辨率选择1/2或者1/4均能达到比较好的效果。&lt;/p&gt;
&lt;p&gt;在常见数据集上实验证明达到了SOTA。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;bpm-sota1.png&#34; srcset=&#34;
               /post/paper-bpn/bpm-sota1_hu0d30f42887772276413914ea654c37f2_312383_cb6e61dbaf476acf8a46b23e57c76651.webp 400w,
               /post/paper-bpn/bpm-sota1_hu0d30f42887772276413914ea654c37f2_312383_d1ccdc18e95a07f7cfc929dfe1d40a0d.webp 760w,
               /post/paper-bpn/bpm-sota1_hu0d30f42887772276413914ea654c37f2_312383_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-bpn/bpm-sota1_hu0d30f42887772276413914ea654c37f2_312383_cb6e61dbaf476acf8a46b23e57c76651.webp&#34;
               width=&#34;689&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;bpm-sota2.png&#34; srcset=&#34;
               /post/paper-bpn/bpm-sota2_hu416ce7169ecd6c5e19eda3a5af8d8288_245684_426240cfb96d10aca8cc80f8cf06c8ac.webp 400w,
               /post/paper-bpn/bpm-sota2_hu416ce7169ecd6c5e19eda3a5af8d8288_245684_2355bfcdf49d717229ceef84bf9a0572.webp 760w,
               /post/paper-bpn/bpm-sota2_hu416ce7169ecd6c5e19eda3a5af8d8288_245684_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-bpn/bpm-sota2_hu416ce7169ecd6c5e19eda3a5af8d8288_245684_426240cfb96d10aca8cc80f8cf06c8ac.webp&#34;
               width=&#34;760&#34;
               height=&#34;721&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;bpm-sota3.png&#34; srcset=&#34;
               /post/paper-bpn/bpm-sota3_hu1816c979a4d2fa4170b88ce41fba6d80_185800_50610521230f5852b10d491d0f0372e9.webp 400w,
               /post/paper-bpn/bpm-sota3_hu1816c979a4d2fa4170b88ce41fba6d80_185800_e1db90bb768635f4b1779ba424d18409.webp 760w,
               /post/paper-bpn/bpm-sota3_hu1816c979a4d2fa4170b88ce41fba6d80_185800_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-bpn/bpm-sota3_hu1816c979a4d2fa4170b88ce41fba6d80_185800_50610521230f5852b10d491d0f0372e9.webp&#34;
               width=&#34;760&#34;
               height=&#34;677&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;使用图网络和RNN作为修正模型对粗检测结果进行修正。&lt;/li&gt;
&lt;li&gt;提出BPN，根据模型提取的特征输出边界框的粗检测结果。&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
    
    <item>
      <title>【论文】MOST: A Multi-Oriented Scene Text Detector with Localization Refinement</title>
      <link>https://yangleisx.github.io/post/paper-most/</link>
      <pubDate>Fri, 09 Jul 2021 09:35:24 +0800</pubDate>
      <guid>https://yangleisx.github.io/post/paper-most/</guid>
      <description>&lt;p&gt;论文题目：MOST: A Multi-Oriented Scene Text Detector with Localization Refinement&lt;/p&gt;
&lt;p&gt;作者：Minghang He, Minghui Liao, Zhibo Yang, Humen Zhong, Jun Tang, Wenqing Cheng, Cong Yao, Yongpan Wang, Xiang Bai&lt;/p&gt;
&lt;p&gt;会议/时间：CVPR 2021&lt;/p&gt;
&lt;p&gt;链接: &lt;a href=&#34;https://arxiv.org/abs/2104.01070&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;论文目标&#34;&gt;论文目标&lt;/h2&gt;
&lt;p&gt;传统的文本检测算法对于长文本和小文本的检测效果比较差。为了解决长文本和小文本的检测问题，引入了新的NMS、IoU Loss和特征对齐模块。&lt;/p&gt;
&lt;p&gt;例如在EAST中，由于使用分类输出作为NMS的权重，而且模型感受野比较小，很难获得足够的信息检测到长文本。&lt;/p&gt;
&lt;h2 id=&#34;相关工作&#34;&gt;相关工作&lt;/h2&gt;
&lt;p&gt;现有的文本检测算法主要包括两种思路，自下而上和自上而下。其中前者主要是检测到文本中的部分元素，然后合并得到完整的检测结果，包括SegLink、TextSnake、CRAFT、PSENet等，由于需要复杂的后处理算法处理模型的输出结果，整体的处理效率受限制，而且后处理算法的效果也会影响到整体的结果。后者直接使用模型输出文本边框的检测结果，包括EAST、TextBoxes等。自上而下的又可以分为一阶段的和两阶段的，前者直接输出模型结果，后者包括Mask TextSpotter系列，使用类似Mask R-CNN的思路，首先使用RPN输出Proposal。&lt;/p&gt;
&lt;p&gt;在LOMO模型中，使用了迭代优化模型，使用RoI Transform迭代优化模型结果，从而解决长文本的检测问题。&lt;/p&gt;
&lt;h2 id=&#34;本文方法&#34;&gt;本文方法&lt;/h2&gt;
&lt;p&gt;模型整体结构如下，特征提取模块使用了基于ResNet50的FPN网络，上下两个分支来自EAST中的分类和位置预测图。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;most-pipeline.png&#34; srcset=&#34;
               /post/paper-most/most-pipeline_hub31138c7ee6e9798cb996bcbff1b8f7d_346123_90d84ada2f6cfb838eedfe8a52152213.webp 400w,
               /post/paper-most/most-pipeline_hub31138c7ee6e9798cb996bcbff1b8f7d_346123_cf9492e490e7456149c388e67ec49618.webp 760w,
               /post/paper-most/most-pipeline_hub31138c7ee6e9798cb996bcbff1b8f7d_346123_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-most/most-pipeline_hub31138c7ee6e9798cb996bcbff1b8f7d_346123_90d84ada2f6cfb838eedfe8a52152213.webp&#34;
               width=&#34;760&#34;
               height=&#34;301&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;来自EAST的两个分支的监督与EAST相同。分类分支使用向内收缩后的文本框表示，位置预测图包括四个通道，表示当前位置距离文本框上下左右的距离。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;most-gt.png&#34; srcset=&#34;
               /post/paper-most/most-gt_hu7103210d8ba9e0b562c6310c4fffebfc_451183_8653220c4fee0ef897227ab56f9ddf65.webp 400w,
               /post/paper-most/most-gt_hu7103210d8ba9e0b562c6310c4fffebfc_451183_b2f8e574cb9fb0d08a8db18c8a90d11b.webp 760w,
               /post/paper-most/most-gt_hu7103210d8ba9e0b562c6310c4fffebfc_451183_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-most/most-gt_hu7103210d8ba9e0b562c6310c4fffebfc_451183_8653220c4fee0ef897227ab56f9ddf65.webp&#34;
               width=&#34;760&#34;
               height=&#34;240&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;TFAM的结构如下。首先生成粗检测结果，然后将粗检测结果和特征传入变形卷积层中。其中变形卷积选择采样点有两种方式，分别是Feature-Based Sampling和Localization-Based Sampling。其中，前者是使用额外的卷积来计算采样点的偏移量，后者是直接使用当前点预测的粗检测框的位置作为偏移后的采样点。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;most-tfam.png&#34; srcset=&#34;
               /post/paper-most/most-tfam_hu6803c23a0ad86228c678a3104e3dbd5b_432206_9ed97911bd2b65e0d3fd48fd5842fcc2.webp 400w,
               /post/paper-most/most-tfam_hu6803c23a0ad86228c678a3104e3dbd5b_432206_1a6bfc22dff6cae982f62af9d8290c2b.webp 760w,
               /post/paper-most/most-tfam_hu6803c23a0ad86228c678a3104e3dbd5b_432206_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-most/most-tfam_hu6803c23a0ad86228c678a3104e3dbd5b_432206_9ed97911bd2b65e0d3fd48fd5842fcc2.webp&#34;
               width=&#34;760&#34;
               height=&#34;512&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;本文中提出的Position-awareness NMS与EAST中的locality-awareness NMS不同。在EAST中进行的NMS是，是利用加权平均的方式，使用Text/Non-Text分类的结果作为权值进行加权和然后进行正常的NMS。&lt;/p&gt;
&lt;p&gt;本文中提出的PA-NMS基于一个假设，距离边界越近的点，得到的距离边界的距离越准确，因此在聚合的时候使用的权重为当前点距离边界的位置。当使用边框p和q聚合得到m时，计算过程如下。其中TBLR分别为EAST输出的Geo-Map的四个通道，即Position-Awareness Map。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;most-panms.png&#34; srcset=&#34;
               /post/paper-most/most-panms_hua80365d90b236e6f2e7f6ba060013aca_83329_0e302c004c8515de1a9b0172c204ab27.webp 400w,
               /post/paper-most/most-panms_hua80365d90b236e6f2e7f6ba060013aca_83329_a8cd634d2d0323fdcbdd20baf445786c.webp 760w,
               /post/paper-most/most-panms_hua80365d90b236e6f2e7f6ba060013aca_83329_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-most/most-panms_hua80365d90b236e6f2e7f6ba060013aca_83329_0e302c004c8515de1a9b0172c204ab27.webp&#34;
               width=&#34;760&#34;
               height=&#34;277&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;其中Position-Awareness Map（相当于EAST中的Geo-Map归一化到0-1）的生成方式如下。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;most-pamap.png&#34; srcset=&#34;
               /post/paper-most/most-pamap_hua804e58035a47ad1df6d5d88dfa673dd_52251_039a270f42ddd0c3ed534b8bf53f1904.webp 400w,
               /post/paper-most/most-pamap_hua804e58035a47ad1df6d5d88dfa673dd_52251_f1386455a4a7e6d0ec986a8c805c3c1c.webp 760w,
               /post/paper-most/most-pamap_hua804e58035a47ad1df6d5d88dfa673dd_52251_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-most/most-pamap_hua804e58035a47ad1df6d5d88dfa673dd_52251_039a270f42ddd0c3ed534b8bf53f1904.webp&#34;
               width=&#34;760&#34;
               height=&#34;252&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;在损失函数的选择上，原本EAST中使用IoU-Loss。但是文中提到如下观点。因此引入了Instance-wise IoU Loss。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;the large text region contains far more positive samples than the small text region, which makes the regression loss bias towards large and long text instances.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;其中$N_t$为Text Instance的数量，$S_j$为每个Text Instance中Positive Sample的数量。（这里我没有很清楚具体Text Instance和Positive Sample的定义，为理解为对于每一个Text/Non-Text分类为真的点都要计算IoU，对于面积比较大的文本计算IoU的次数比较多，导致模型偏重大文本和长文本。）
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;most-insiou.png&#34; srcset=&#34;
               /post/paper-most/most-insiou_huc1f2e63174bd72e887a50fc6f8bf9a73_28498_116a830ef35c364f0388ca9fc05ef442.webp 400w,
               /post/paper-most/most-insiou_huc1f2e63174bd72e887a50fc6f8bf9a73_28498_c1bdb124d511971e01531898f032fbee.webp 760w,
               /post/paper-most/most-insiou_huc1f2e63174bd72e887a50fc6f8bf9a73_28498_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-most/most-insiou_huc1f2e63174bd72e887a50fc6f8bf9a73_28498_116a830ef35c364f0388ca9fc05ef442.webp&#34;
               width=&#34;760&#34;
               height=&#34;176&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;最终的损失函数定义如下，完整的损失包括分类损失、检测损失、位置损失。分类损失使用BCE-Loss计算，位置损失使用Smoothed L1-Loss计算。在检测损失中包括了检测结果的IoU-Loss和边界框角度的Cosine-Loss。



$$\begin{align}
L &amp;= L_s + \lambda_{gc} L_{gc} + \lambda_{gr} L_{gr}+ \lambda_{p} L_{p} \\
L_s &amp;= \operatorname{BCE-Loss}() \\
L_g &amp;= L_{iou} + \lambda_{i} L_{ins-iou} + \lambda_{\theta} L_{\theta} \\
L_{\theta} &amp;= \frac{1}{|\Omega|}\sum\limits_{i \in \Omega}1-\cos (\hat{\theta_i} - \theta_i^*) \\
L_p &amp;= \frac{1}{4|\Omega|}\sum\limits_{i \in \Omega}\sum\limits_{\Psi \in \{L,R,T,B\}}\operatorname{SmoothedL1}(\hat{\Psi_i}-\Psi_i^*)
\end{align}$$
&lt;/p&gt;
&lt;h2 id=&#34;结果分析&#34;&gt;结果分析&lt;/h2&gt;
&lt;p&gt;在数据集SynthText上预训练，在数据集MLT17、MTWI、IC15、MSRA-TD500(with HUST-TR400)上训练和测试。&lt;/p&gt;
&lt;p&gt;在Ablation Study中，证明Localization-Based Sampling相比Feature-Based Sampling效果更好，而同时结合这两种的效果最好。同时也证明了本文中提出的TFAM、PA-NMS、Instance-wise IoU Loss都能提升模型的性能。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;ablation&#34; srcset=&#34;
               /post/paper-most/most-ablation_hue0c9503d6af440e8a3f646eaf775485f_152871_5af1d345682da5bb47713e36183257c6.webp 400w,
               /post/paper-most/most-ablation_hue0c9503d6af440e8a3f646eaf775485f_152871_df100e4cfc453b26b2436f2f70e01c17.webp 760w,
               /post/paper-most/most-ablation_hue0c9503d6af440e8a3f646eaf775485f_152871_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-most/most-ablation_hue0c9503d6af440e8a3f646eaf775485f_152871_5af1d345682da5bb47713e36183257c6.webp&#34;
               width=&#34;760&#34;
               height=&#34;213&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;在与SOTA模型的比较中，MOST也都能获得不错的性能提升。&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;为了解决长文本的检测问题，引入了TFAM扩展感受野修正粗检测结果。
为了解决不同大小的文本的检测问题，引入了Instance-wise IoU Loss，防止损失函数过度关注大文本和长文本目标。
在NMS阶段引入了Position-Aware NMS，可以更好的合并检测框。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>【论文】Feature Pyramid Networks for Object Detection</title>
      <link>https://yangleisx.github.io/post/paper-fpn/</link>
      <pubDate>Tue, 11 May 2021 17:34:24 +0800</pubDate>
      <guid>https://yangleisx.github.io/post/paper-fpn/</guid>
      <description>&lt;p&gt;论文题目：Feature Pyramid Networks for Object Detection&lt;/p&gt;
&lt;p&gt;作者：Tsung-Yi Lin, Piotr Doll´ar, Ross Girshick, Kaiming He, Bharath Hariharan, Serge Belongie&lt;/p&gt;
&lt;p&gt;会议/时间：CVPR2017&lt;/p&gt;
&lt;p&gt;链接: &lt;a href=&#34;https://arxiv.org/pdf/1612.03144.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arxiv&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;论文目标&#34;&gt;论文目标&lt;/h2&gt;
&lt;p&gt;在传统目标检测任务中，通常会使用图像金字塔的方式来检测到不同尺度大小的目标。但是在深度神经网络中，直接使用图像金字塔的计算复杂度比较高，因此限制了图像金字塔的使用。&lt;/p&gt;
&lt;p&gt;在论文中提出了一种使用旁路链接的特征金字塔网络，可以充分利用到不同尺度的特征信息，从而提高目标检测等相关任务的性能，同时不会引入太高的时空复杂度。&lt;/p&gt;
&lt;h2 id=&#34;相关工作&#34;&gt;相关工作&lt;/h2&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;fpn-background.png&#34; srcset=&#34;
               /post/paper-fpn/fpn-background_huf531b35a9d44cb5b2a188982dfe69d26_243279_e0d193ab1b3be548b6ed763bbd9e4b99.webp 400w,
               /post/paper-fpn/fpn-background_huf531b35a9d44cb5b2a188982dfe69d26_243279_2e1bd25a20b7afea829303a536ff9697.webp 760w,
               /post/paper-fpn/fpn-background_huf531b35a9d44cb5b2a188982dfe69d26_243279_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-fpn/fpn-background_huf531b35a9d44cb5b2a188982dfe69d26_243279_e0d193ab1b3be548b6ed763bbd9e4b99.webp&#34;
               width=&#34;760&#34;
               height=&#34;450&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;最早的图像金字塔网络中，将原始图像经过不同程度的缩放之后，分别提取特征信息用于预测，时空复杂度都很高。在使用卷积神经网络的模型中，使用多层卷积层方式逐渐提取高级图像特征，最终用于预测的方式具有较高的鲁棒性，相比传统方法取得了不错的效果，但是没有充分利用到不同尺度的信息，中间层提取的特征解释性比较差。相比之下在[[SSD 系列]]中使用卷积神经网络的中间层的特征进行预测，不仅利用到了不同尺度的图像特征，相比只使用最高层特征的方式也没有引入太多的计算成本。&lt;/p&gt;
&lt;h2 id=&#34;本文方法&#34;&gt;本文方法&lt;/h2&gt;
&lt;p&gt;通过旁路连接，构建了一个自上而下的特征金字塔网络，在编码器一端使用现有的分类网络结构，逐层提取特征。在特征融合部分，将上层特征经过上采样放大后，与使用卷积修改通道数之后的下层特征相加，得到了融合后的特征。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;fpn-model.png&#34; srcset=&#34;
               /post/paper-fpn/fpn-model_hu9c78d5d8bce24d834d8d37e1b932031e_78864_a1058fdd997555176c12b757cc365d56.webp 400w,
               /post/paper-fpn/fpn-model_hu9c78d5d8bce24d834d8d37e1b932031e_78864_e56effc5bfcc848986360b4b512397b3.webp 760w,
               /post/paper-fpn/fpn-model_hu9c78d5d8bce24d834d8d37e1b932031e_78864_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-fpn/fpn-model_hu9c78d5d8bce24d834d8d37e1b932031e_78864_a1058fdd997555176c12b757cc365d56.webp&#34;
               width=&#34;760&#34;
               height=&#34;553&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;在预测的时候，在每一层融合后的特征都使用固定大小的卷积得到特征图用于预测，从而可以检测到不同大小的目标。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;fpn-predict.png&#34; srcset=&#34;
               /post/paper-fpn/fpn-predict_huab057986c0c2ea0858aff032601037a3_72530_7ea7da4dabe5ad3ae9289e3f41d1061b.webp 400w,
               /post/paper-fpn/fpn-predict_huab057986c0c2ea0858aff032601037a3_72530_076a055153e4236c9193603d781dba7f.webp 760w,
               /post/paper-fpn/fpn-predict_huab057986c0c2ea0858aff032601037a3_72530_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-fpn/fpn-predict_huab057986c0c2ea0858aff032601037a3_72530_7ea7da4dabe5ad3ae9289e3f41d1061b.webp&#34;
               width=&#34;760&#34;
               height=&#34;293&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;论文中指出在特征融合的时候可以设计多种融合形式，不局限于文中的1*1卷积，也可以使用残差模块等，论文中只讨论整体的特征融合结构。&lt;/p&gt;
&lt;h2 id=&#34;结果分析&#34;&gt;结果分析&lt;/h2&gt;
&lt;p&gt;文中将提出的FPN结构加入RPN 区域建议网络和Fast R-CNN网络中进行了测试。同时也另在图像分割任务中进行了实验。
经实验可以看到所提出的特征金字塔结构均能取得性能的提升。&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;提出了一种自上而下的特征融合结构，使用逐元素相加的方式实现上下层特征相结合，并且在每一层融合后的特征之上都生成特征图用于预测。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>【论文】ContourNet: Taking a Further Step toward Accurate Arbitrary-shaped Scene Text Detection</title>
      <link>https://yangleisx.github.io/post/paper-contournet/</link>
      <pubDate>Sat, 08 May 2021 11:58:37 +0800</pubDate>
      <guid>https://yangleisx.github.io/post/paper-contournet/</guid>
      <description>&lt;p&gt;论文题目：ContourNet: Taking a Further Step toward Accurate Arbitrary-shaped Scene Text Detection&lt;/p&gt;
&lt;p&gt;作者：Yuxin Wang, Hongtao Xie, Zhengjun Zha, Mengting Xing, Zilong Fu, Yongdong Zhang&lt;/p&gt;
&lt;p&gt;会议/时间：CVPR2020&lt;/p&gt;
&lt;p&gt;链接: &lt;a href=&#34;https://arxiv.org/pdf/2004.04940.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;论文目标&#34;&gt;论文目标&lt;/h2&gt;
&lt;p&gt;目前的图像文本检测算法常常会将一些纹理信息识别为文本，得到假正例，而且图像文本常常具有多种尺度大小和形状，难以识别。
在论文中提出了一种模型，通过引入自适应区域建议网络和正交的纹理敏感模块来解决上述问题。前者是一种区域大小无关的RPN网络，使用IoU监督，后者可以从正交的两个方向上检测纹理信息，有效避免假正例。模型检测结果为目标的若干个轮廓点，可以通过进一步后处理得到多边形边界框。&lt;/p&gt;
&lt;h2 id=&#34;相关工作&#34;&gt;相关工作&lt;/h2&gt;
&lt;p&gt;近年来图像文本检测算法对于假正例的问题有多种解决思路：SPCNET中使用了语义特征来修正边界框，或者也可以使用置信度等修正边界框。&lt;/p&gt;
&lt;p&gt;对于图像尺度多变的问题，MSR使用了多尺度网络结构、DSRN提出了双向多尺度关系网络等。&lt;/p&gt;
&lt;p&gt;在传统文本检测中常用的算法包括连通域分析和滑窗处理。在深度模型中常用的思路包括基于边界框回归的思路和基于语义的思路。前者包括EAST、DDR、LOMO等首先需要使用Anchor-Based或者Anchor-Free的思路生成边界框。后者需要对分割图进行后处理。&lt;/p&gt;
&lt;h2 id=&#34;本文方法&#34;&gt;本文方法&lt;/h2&gt;
&lt;p&gt;模型包括两个部分，前半部分Adaptive-RPN生成文本所在区域，然后使用LOTM进行两个方向的处理得到文本轮廓点。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;contournet-model.png&#34; srcset=&#34;
               /post/paper-contournet/contournet-model_huf34869e6413c14dae070f21368b19187_469432_568e3e41d62db16e7b86d7cc8ab2a9da.webp 400w,
               /post/paper-contournet/contournet-model_huf34869e6413c14dae070f21368b19187_469432_ebb278e967f467850bc1c3e4d2419d76.webp 760w,
               /post/paper-contournet/contournet-model_huf34869e6413c14dae070f21368b19187_469432_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-contournet/contournet-model_huf34869e6413c14dae070f21368b19187_469432_568e3e41d62db16e7b86d7cc8ab2a9da.webp&#34;
               width=&#34;760&#34;
               height=&#34;365&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;在Adaptive-RPN中，传统方法是使用生成四个值 ${\Delta x, \Delta y, \Delta w, \Delta h}$去优化得到矩形区域，使用$l_1\ Loss$监督。但是这样的方式对于区域的大小比较敏感。&lt;/p&gt;
&lt;p&gt;本文中一方面使用$N$个点来表示$RoI$，其中一个点表示区域中心，剩下$N-1$个点表示区域的边框，根据这$N$个点的最边界点得到$RoI$。另一方面使用$IoU\ Loss$来监督这个$N$个点的回归，对尺度不敏感。计算最边界点的方式如下：



$$\begin{aligned}
Proposal = &amp; \{x_{tl}, y_{tl}, x_{rb}, y_{rb}\} \\
= &amp; \{\min\{x_r\}_{r=1}^n, \min\{y_r\}_{r=1}^n, \\
\ &amp; \ \max\{x_r\}_{r=1}^n, \max\{y_r\}_{r=1}^n\}
\end{aligned}$$
&lt;/p&gt;
&lt;p&gt;在获得文本边界点的时候，使用了相互正交的两个方向上的特征分别得到轮廓点热力图。文中假设对于一个文字在两个方向上都具有明显的特征，但是其他无意义的纹理通常只在一个方向上具有比较明显的特征，因此可以通过两个方向上的分别处理区分开。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;contournet-lotm.png&#34; srcset=&#34;
               /post/paper-contournet/contournet-lotm_hu1f8367564ae0da1e7b76bd4a25fab0af_129610_982f4e4dc55b17a563257aedd4f8819a.webp 400w,
               /post/paper-contournet/contournet-lotm_hu1f8367564ae0da1e7b76bd4a25fab0af_129610_b8828b1b111024586611394e644982c3.webp 760w,
               /post/paper-contournet/contournet-lotm_hu1f8367564ae0da1e7b76bd4a25fab0af_129610_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-contournet/contournet-lotm_hu1f8367564ae0da1e7b76bd4a25fab0af_129610_982f4e4dc55b17a563257aedd4f8819a.webp&#34;
               width=&#34;760&#34;
               height=&#34;411&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;接着在后处理算法中将两个轮廓点热力图合并起来得到文本轮廓点。其算法如下，首先分别进行NMS处理减少多余的点，然后将两个方向上都都具有较高置信度的点作为输出得到轮廓点。最终通过Alpha-Shape Algorithm得到多边形边界框。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;contournet-algo.png &#34; srcset=&#34;
               /post/paper-contournet/contournet-algo_huc05daf44840c88ea26ce7c8b2f5bfe41_166220_937f68c3422437f851074aea82d983dc.webp 400w,
               /post/paper-contournet/contournet-algo_huc05daf44840c88ea26ce7c8b2f5bfe41_166220_d55bb833e9e761d6594d0ba924502714.webp 760w,
               /post/paper-contournet/contournet-algo_huc05daf44840c88ea26ce7c8b2f5bfe41_166220_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-contournet/contournet-algo_huc05daf44840c88ea26ce7c8b2f5bfe41_166220_937f68c3422437f851074aea82d983dc.webp&#34;
               width=&#34;760&#34;
               height=&#34;590&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;需要注意在生成监督的时候，对于两个方向上的轮廓点都使用相同的监督，即通过原始多边形边界框生成的宽度不低于2个像素的边框（这里是我的理解，原文如下）。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;we use &lt;code&gt;distance_transform_edt&lt;/code&gt; in &lt;em&gt;Scipy&lt;/em&gt; to obtain the two-points wide edge.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;结果分析&#34;&gt;结果分析&lt;/h2&gt;
&lt;p&gt;对于Adap-RPN中表示RoI的点的数量的实验。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;contournet-exp-rpn.png&#34; srcset=&#34;
               /post/paper-contournet/contournet-exp-rpn_hu2f3d3389ed3fc8465a3eded79117fd1d_89131_c099c95da86b809c0346e962797fcd15.webp 400w,
               /post/paper-contournet/contournet-exp-rpn_hu2f3d3389ed3fc8465a3eded79117fd1d_89131_175bd17a2a8146907992faebfc5b4d61.webp 760w,
               /post/paper-contournet/contournet-exp-rpn_hu2f3d3389ed3fc8465a3eded79117fd1d_89131_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-contournet/contournet-exp-rpn_hu2f3d3389ed3fc8465a3eded79117fd1d_89131_c099c95da86b809c0346e962797fcd15.webp&#34;
               width=&#34;760&#34;
               height=&#34;292&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;对于Adap_RPN的有效性的实验。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;contournet-exp-rpn2.png&#34; srcset=&#34;
               /post/paper-contournet/contournet-exp-rpn2_hufe01bff1d556de78dca664fbcaa82af3_148154_5621283773d11c4a6f50740b04f3124c.webp 400w,
               /post/paper-contournet/contournet-exp-rpn2_hufe01bff1d556de78dca664fbcaa82af3_148154_ee2a20c7d4dc59ca924e44b07627fdbf.webp 760w,
               /post/paper-contournet/contournet-exp-rpn2_hufe01bff1d556de78dca664fbcaa82af3_148154_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-contournet/contournet-exp-rpn2_hufe01bff1d556de78dca664fbcaa82af3_148154_5621283773d11c4a6f50740b04f3124c.webp&#34;
               width=&#34;760&#34;
               height=&#34;469&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;对于LOTM中$1*N$和$N*1$卷积长度的实验。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;contournet-exp-lotm.png&#34; srcset=&#34;
               /post/paper-contournet/contournet-exp-lotm_hu5e80dd2567d46141f37f6e443e26b431_77680_e6f5656fbf21a123cff20e5755a7282e.webp 400w,
               /post/paper-contournet/contournet-exp-lotm_hu5e80dd2567d46141f37f6e443e26b431_77680_2ab972dff72a756353c96350681c50f6.webp 760w,
               /post/paper-contournet/contournet-exp-lotm_hu5e80dd2567d46141f37f6e443e26b431_77680_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-contournet/contournet-exp-lotm_hu5e80dd2567d46141f37f6e443e26b431_77680_e6f5656fbf21a123cff20e5755a7282e.webp&#34;
               width=&#34;760&#34;
               height=&#34;276&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;对于LOTM的两方向特征融合效果的实验。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;contournet-exp-lotm2.png&#34; srcset=&#34;
               /post/paper-contournet/contournet-exp-lotm2_hud85ddd2639cada510ac8118e7c39bd05_123574_ebca31ea7d0357a1136ebd0644e67599.webp 400w,
               /post/paper-contournet/contournet-exp-lotm2_hud85ddd2639cada510ac8118e7c39bd05_123574_9079e76a397907498af58f5f45263508.webp 760w,
               /post/paper-contournet/contournet-exp-lotm2_hud85ddd2639cada510ac8118e7c39bd05_123574_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-contournet/contournet-exp-lotm2_hud85ddd2639cada510ac8118e7c39bd05_123574_ebca31ea7d0357a1136ebd0644e67599.webp&#34;
               width=&#34;760&#34;
               height=&#34;385&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;在三个任务上（弯曲文本：Total-Text，长弯曲文本：CTW1500，多方向文本：ICDAR2015）均相对SOTA有一定的提升。可以看到本文方法有效减少了假正例的出现。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>【论文】Densely Connected Convolutional Networks</title>
      <link>https://yangleisx.github.io/post/paper-densenet/</link>
      <pubDate>Tue, 20 Apr 2021 14:23:12 +0800</pubDate>
      <guid>https://yangleisx.github.io/post/paper-densenet/</guid>
      <description>&lt;p&gt;论文题目： Densely Connected Convolutional Networks&lt;/p&gt;
&lt;p&gt;作者： Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger&lt;/p&gt;
&lt;p&gt;会议/时间：CVPR2017&lt;/p&gt;
&lt;p&gt;链接: &lt;a href=&#34;https://arxiv.org/pdf/1608.06993.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;论文目标&#34;&gt;论文目标&lt;/h2&gt;
&lt;p&gt;在CNN中，增加模型深度可以增加卷积层的感受野，从而提升CNN的性能，但是深度太大的CNN在训练的时候会出现梯度消失的问题。根据ResNet 残差网络的设计，通过添加短路链接可以使得比较深的网络更容易训练和收敛。&lt;/p&gt;
&lt;p&gt;为了进一步融合不同尺度特征，本文提出的DenseNet模型通过添加稠密的短路链接可以使特征图得到充分的利用，增大梯度信息流，从而减少参数数量，提升模型的性能。通过稠密的短路连接，每一个卷积层都可以看作与输入的数据和最后的全连接层直接相连，从而得到充分的监督。&lt;/p&gt;
&lt;h2 id=&#34;相关工作&#34;&gt;相关工作&lt;/h2&gt;
&lt;p&gt;在Highway Network中，首次引入了短路连接的思想，从而使得训练上百层的神经网络成为可能。在ResNet 残差网络中，通过短路连接，将每一个Residual Block的输入通过恒等映射直接连接到输出，实现了突破性的提升。在Stochastic Depth网络中训练的时候随机跳过了模型中的某些层，实际上也能取得非常不错的效果，这就证明在非常深的模型中有一些卷积层的存在其实是冗余的。&lt;/p&gt;
&lt;p&gt;与上述模型不同的是，在GoogLeNet和FractalNet中使用了Inception结构增加模型宽度的方式来提升模型的性能。&lt;/p&gt;
&lt;p&gt;其他的提升性能的思路包括：NIN（在卷积层中添加多层感知机提取丰富特征），DSN（使用一些分类器监督卷积的中间结果），DFN（将不同网络的中间层融合在一起）等。&lt;/p&gt;
&lt;h2 id=&#34;本文方法&#34;&gt;本文方法&lt;/h2&gt;
&lt;p&gt;将多个卷积层组合得到Dense Block，其中每一个卷积层的输入都是之前所有层的输出按通道连接得到。即 $x_l = H_l([x_0, x_1, &amp;hellip;, x_{l-1}])$。同时需要注意，每一个卷积层都是由带有bottleneck的混合卷积操作组成的，其中的混合卷积操作指BatchNorm + ReLU + Conv3x3组成，通道数为 $k$，bottleneck是指BatchNorm + ReLU + Conv1x1组成，通道数为 $4k$。这里的 $k$称为Growth Rate，即每通过一个卷积层，通道数都会增加 $k$个。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;densenet-block.png&#34; srcset=&#34;
               /post/paper-densenet/densenet-block_hu7dcd82c9194017b16ba2c3edbb5da1ac_310954_f6ff3f3b2f9c80fcc2b71e164d48697c.webp 400w,
               /post/paper-densenet/densenet-block_hu7dcd82c9194017b16ba2c3edbb5da1ac_310954_07dae10bbcf78dd0a2b9920ac9a58f0e.webp 760w,
               /post/paper-densenet/densenet-block_hu7dcd82c9194017b16ba2c3edbb5da1ac_310954_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-densenet/densenet-block_hu7dcd82c9194017b16ba2c3edbb5da1ac_310954_f6ff3f3b2f9c80fcc2b71e164d48697c.webp&#34;
               width=&#34;760&#34;
               height=&#34;556&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;在每一个Dense Block之中的特征图大小是相同的，在相邻的DenseBlock之间引入Transition层，包括Conv1x1和Pooling2x2，其中的卷积操作用于压缩通道数量，防止通道数量无限制的增大，而池化操作用于压缩特征图数量。最终得到完整的DenseNet结构如下。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;densenet-model2.png&#34; srcset=&#34;
               /post/paper-densenet/densenet-model2_hu8726d8431475ce320741688cef110cb4_97794_1374042b9d8668d59956d845ec1ec0c7.webp 400w,
               /post/paper-densenet/densenet-model2_hu8726d8431475ce320741688cef110cb4_97794_4f7aa769d69df463278c2e420f2f8474.webp 760w,
               /post/paper-densenet/densenet-model2_hu8726d8431475ce320741688cef110cb4_97794_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-densenet/densenet-model2_hu8726d8431475ce320741688cef110cb4_97794_1374042b9d8668d59956d845ec1ec0c7.webp&#34;
               width=&#34;760&#34;
               height=&#34;126&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;在实验中提出的不同大小的模型结构如下：
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;densenet-structure.png&#34; srcset=&#34;
               /post/paper-densenet/densenet-structure_hu7523153a526e2b2b74084e7721c3e0f3_156361_658617b12c29ebd8afe8cebfe6c96344.webp 400w,
               /post/paper-densenet/densenet-structure_hu7523153a526e2b2b74084e7721c3e0f3_156361_43c78deb1b9236c7575373e26f749c3a.webp 760w,
               /post/paper-densenet/densenet-structure_hu7523153a526e2b2b74084e7721c3e0f3_156361_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-densenet/densenet-structure_hu7523153a526e2b2b74084e7721c3e0f3_156361_658617b12c29ebd8afe8cebfe6c96344.webp&#34;
               width=&#34;760&#34;
               height=&#34;392&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;结果分析&#34;&gt;结果分析&lt;/h2&gt;
&lt;p&gt;模型在CIFAR、SVHN、ImageNet等任务上测试，结果与现有的SOTA模型相比，取得了比较大的提升。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;densenet-vs-sota.png&#34; srcset=&#34;
               /post/paper-densenet/densenet-vs-sota_hu3b47d34787c751ac5dd679e2fc581d8f_371053_59f6dc542272968df87365df748f0526.webp 400w,
               /post/paper-densenet/densenet-vs-sota_hu3b47d34787c751ac5dd679e2fc581d8f_371053_248310b7c83627fba61eafd2a158f8ae.webp 760w,
               /post/paper-densenet/densenet-vs-sota_hu3b47d34787c751ac5dd679e2fc581d8f_371053_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-densenet/densenet-vs-sota_hu3b47d34787c751ac5dd679e2fc581d8f_371053_59f6dc542272968df87365df748f0526.webp&#34;
               width=&#34;760&#34;
               height=&#34;457&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;重点比较了ResNet，证明了DenseNet使用更小的模型参数也能取得与ResNet相近甚至更好的效果，证明了之前提到了使用DenseNet可以减少模型参数的说法。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;densenet-vs-resnet.png&#34; srcset=&#34;
               /post/paper-densenet/densenet-vs-resnet_hu78cb94f206f47ab713e6c70212245b31_200683_b84754fc828d146b12772ce9a2accb2b.webp 400w,
               /post/paper-densenet/densenet-vs-resnet_hu78cb94f206f47ab713e6c70212245b31_200683_566a5f189088fd6eb55f2996592f8443.webp 760w,
               /post/paper-densenet/densenet-vs-resnet_hu78cb94f206f47ab713e6c70212245b31_200683_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-densenet/densenet-vs-resnet_hu78cb94f206f47ab713e6c70212245b31_200683_b84754fc828d146b12772ce9a2accb2b.webp&#34;
               width=&#34;760&#34;
               height=&#34;393&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;需要注意的是，尽管DenseNet模型参数数量很少，但是由于包含稠密连接，最直接的实现在训练和优化的时候中间变量需要占用大量的显存，空间效率很低。因此本文的作者在另一篇文章&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;中提出了DenseNet的优化实现。&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;使用稠密的短路连接，使得模型学到丰富的不同尺度的信息，使梯度信息的传播最大化，从而在减少参数数量的基础上提升模型的性能。&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;G. Pleiss, D. Chen, G. Huang, T. Li, L. van der Maaten, and K. Q. Weinberger. Memory-efficient implementation of densenets. arXiv preprint arXiv:1707.06990, 2017. 5&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>【论文】Densely connected multidilated convolutional networks for dense prediction tasks</title>
      <link>https://yangleisx.github.io/post/paper-d3net/</link>
      <pubDate>Sat, 17 Apr 2021 17:28:42 +0800</pubDate>
      <guid>https://yangleisx.github.io/post/paper-d3net/</guid>
      <description>&lt;p&gt;论文题目：Densely connected multidilated convolutional networks for dense prediction tasks&lt;/p&gt;
&lt;p&gt;作者：Naoya Takahashi, Yuki Mitsufuji&lt;/p&gt;
&lt;p&gt;会议/时间：CVPR2021&lt;/p&gt;
&lt;p&gt;链接: &lt;a href=&#34;https://arxiv.org/abs/2011.11844&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:2011.1184v1&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;论文目标&#34;&gt;论文目标&lt;/h2&gt;
&lt;p&gt;在密集预测任务中，模型通常需要处理和学习非常大范围的上下文信息，为了满足这一需求，常用的方法包括增加模型的深度（例如Resnet和DenseNet）、增加模型的宽度（例如Inception）。一种效果比较好的方式是增加短路链接使得低层特征能够传递到比较高的卷积层中，因此在ResNet的基础上有了DenseNet。更深的网络具有的优点是高层的卷积模块具有更大的感受野，从而可以学习到更大范围的上下文信息。但是更深的网络通常比较难以训练。为了增大感受野而不增加深度，常用的方法包括空洞卷积，使用注意力机制，使用特征金字塔网络等。&lt;/p&gt;
&lt;p&gt;为了进一步提升在密集预测任务上的效果，作者将空洞卷积引入了DenseNet中，并且设计了D2-Block模块和D3Net网络，消除简单使用空洞卷积可能会引入的混淆问题，并在图像分割和语音讲话人分离两个任务上测试，取得了不错的效果。&lt;/p&gt;
&lt;h2 id=&#34;相关工作&#34;&gt;相关工作&lt;/h2&gt;
&lt;p&gt;在DenseNet中，使用了稠密的短路链接，每一层卷积层的输入都是之前的所有卷积层的输出，充分利用了不同尺度的特征图的信息，从而学习到输入数据在不同尺度上的信息。同时也增加了丰富的梯度流，使得模型能学到更丰富的信息。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;densenet-block.png&#34; srcset=&#34;
               /post/paper-d3net/densenet-block_hu7dcd82c9194017b16ba2c3edbb5da1ac_310954_f6ff3f3b2f9c80fcc2b71e164d48697c.webp 400w,
               /post/paper-d3net/densenet-block_hu7dcd82c9194017b16ba2c3edbb5da1ac_310954_07dae10bbcf78dd0a2b9920ac9a58f0e.webp 760w,
               /post/paper-d3net/densenet-block_hu7dcd82c9194017b16ba2c3edbb5da1ac_310954_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-d3net/densenet-block_hu7dcd82c9194017b16ba2c3edbb5da1ac_310954_f6ff3f3b2f9c80fcc2b71e164d48697c.webp&#34;
               width=&#34;760&#34;
               height=&#34;556&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;在空洞卷积或者说膨胀卷积中，在计算卷积操作的时候为卷积核之间填充了0，从而能显著的增加感受野。考虑到使用相同的膨胀因子导致的网格效应，可以采用HDC的思路，选择若干层为一组，组内使用逐渐增大的膨胀因子，使用多组连接得到完整的网络。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;dilated-conv.png&#34; srcset=&#34;
               /post/paper-d3net/dilated-conv_hu0171633157af6b6146c68933c0aa844a_69059_743c6cffa52beb89b05da5f82069c4e0.webp 400w,
               /post/paper-d3net/dilated-conv_hu0171633157af6b6146c68933c0aa844a_69059_e3205e362674dea843f6970aba94f543.webp 760w,
               /post/paper-d3net/dilated-conv_hu0171633157af6b6146c68933c0aa844a_69059_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-d3net/dilated-conv_hu0171633157af6b6146c68933c0aa844a_69059_743c6cffa52beb89b05da5f82069c4e0.webp&#34;
               width=&#34;760&#34;
               height=&#34;296&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;本文方法&#34;&gt;本文方法&lt;/h2&gt;
&lt;p&gt;设计了一个将空洞卷积引入稠密卷积模块的方法，从而设计了D2-Block和D3Net。&lt;/p&gt;
&lt;p&gt;在设计D2-Block的时候，直接将空洞卷积引入了DenseNet的Dense-Block中，此时由于增加了稠密连接，最后一层使用膨胀因子为4的空洞卷积时，其感受野之间出现了盲区，可能会丢失一些局部的信息。&lt;/p&gt;
&lt;p&gt;因此文章中提出了混合膨胀因子的空洞卷积，也就是较高层的卷积层在计算空洞卷积的时候，对于不同层输入的特征图使用不同的膨胀因子，从而避免了上述的盲区，捕捉到更多的信息。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;d3net-block-viz.png&#34; srcset=&#34;
               /post/paper-d3net/d3net-block-viz_hue17dbdc1f4ce90adb05df81a2ca8fa3b_42452_d8cfc385957bc5a254755fb536f7e7a5.webp 400w,
               /post/paper-d3net/d3net-block-viz_hue17dbdc1f4ce90adb05df81a2ca8fa3b_42452_f7ca699cee3e9e0361230d2018208eff.webp 760w,
               /post/paper-d3net/d3net-block-viz_hue17dbdc1f4ce90adb05df81a2ca8fa3b_42452_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-d3net/d3net-block-viz_hue17dbdc1f4ce90adb05df81a2ca8fa3b_42452_d8cfc385957bc5a254755fb536f7e7a5.webp&#34;
               width=&#34;722&#34;
               height=&#34;392&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;文章中将上述的D2-Block通过稠密连接相组合得到D3-Block。同时使用HDC的思路来设置参数，每一组内的膨胀系数都是互素而且逐渐增加的，同时多次重复来实现锯齿形的膨胀因子。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;d3net-d3block.png&#34; srcset=&#34;
               /post/paper-d3net/d3net-d3block_hu009dbfbaa3a6392bc06a4615dd67e67d_51999_a8ec835bb31cd96d6d99d18fa1b5a5dd.webp 400w,
               /post/paper-d3net/d3net-d3block_hu009dbfbaa3a6392bc06a4615dd67e67d_51999_c483673d3cdf8c688087b600119ee349.webp 760w,
               /post/paper-d3net/d3net-d3block_hu009dbfbaa3a6392bc06a4615dd67e67d_51999_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-d3net/d3net-d3block_hu009dbfbaa3a6392bc06a4615dd67e67d_51999_a8ec835bb31cd96d6d99d18fa1b5a5dd.webp&#34;
               width=&#34;754&#34;
               height=&#34;322&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;同时参考DenseNet的设计，在每一个D2-Block的前后都设计了Bottleneck和Compress模块，从而避免模型中的通道数量无限制的增加。&lt;/p&gt;
&lt;h2 id=&#34;实验结果分析&#34;&gt;实验结果分析&lt;/h2&gt;
&lt;p&gt;文章中进行了两组密集预测任务的实验，分别是语义分割和音频源分离。&lt;/p&gt;
&lt;p&gt;在第一个任务中使用CityScapes数据集。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;d3net-result.png&#34; srcset=&#34;
               /post/paper-d3net/d3net-result_hu90aed87a2dd68fca1652ba4d5042ebc0_111989_0987c37e661ef1616f7f2420e90de5cb.webp 400w,
               /post/paper-d3net/d3net-result_hu90aed87a2dd68fca1652ba4d5042ebc0_111989_7417f53fc77082c05ed0d01547749344.webp 760w,
               /post/paper-d3net/d3net-result_hu90aed87a2dd68fca1652ba4d5042ebc0_111989_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-d3net/d3net-result_hu90aed87a2dd68fca1652ba4d5042ebc0_111989_0987c37e661ef1616f7f2420e90de5cb.webp&#34;
               width=&#34;704&#34;
               height=&#34;504&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;第二个任务使用MUSDB18数据集。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;d3net-result2.png &#34; srcset=&#34;
               /post/paper-d3net/d3net-result2_hu78f3dcbd15dad4502553212ef603d9d4_87018_ed439a4931b9150abf0140e6180689f4.webp 400w,
               /post/paper-d3net/d3net-result2_hu78f3dcbd15dad4502553212ef603d9d4_87018_f0f8ef26c25c97a83252ebfb826afadd.webp 760w,
               /post/paper-d3net/d3net-result2_hu78f3dcbd15dad4502553212ef603d9d4_87018_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-d3net/d3net-result2_hu78f3dcbd15dad4502553212ef603d9d4_87018_ed439a4931b9150abf0140e6180689f4.webp&#34;
               width=&#34;726&#34;
               height=&#34;362&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;将空洞卷积引入了DenseNet中。提出了混合膨胀因子的空洞卷积。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>【论文】Real-time Scene Text Detection with Differentiable Binarization</title>
      <link>https://yangleisx.github.io/post/paper-db-mod/</link>
      <pubDate>Fri, 05 Mar 2021 14:39:17 +0800</pubDate>
      <guid>https://yangleisx.github.io/post/paper-db-mod/</guid>
      <description>&lt;p&gt;论文题目：Real-time Scene Text Detection with Differentiable Binarization&lt;/p&gt;
&lt;p&gt;作者：Minghui Liao, Zhaoyi Wan, Cong Yao, Kai Chen, Xiang Bai&lt;/p&gt;
&lt;p&gt;会议/时间：AAAI2020&lt;/p&gt;
&lt;p&gt;链接：&lt;a href=&#34;https://arxiv.org/pdf/1911.08947.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/pdf/1911.08947.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;论文目标&#34;&gt;论文目标&lt;/h2&gt;
&lt;p&gt;基于目标分割的文本检测系统通常能取得比较好的结果，尤其是在检测形状比较多变的文本目标的时候，但是基于分割的系统通常需要设计后处理算法，对模型输出的概率图加以处理得到文本区域的位置。为此本文设计了Differentiable Binarization模块，使得模型同时输出分割图和阈值，使用模型输出的阈值对分割图进行二值化可以得到比较好的结果。&lt;/p&gt;
&lt;h2 id=&#34;相关工作&#34;&gt;相关工作&lt;/h2&gt;
&lt;p&gt;近年来图像文本检测的算法主要分为两类，即基于回归的和基于分割的。前者包括TextBoxes、SSD、EAST、SegLink等，后者包括Mask TextSpotter、PSENet等。同时有一些快速文本检测算法，旨在在不损失精确度的情况下提高预测的速度。例如在SSD的基础上发展了TextBoxes++和RRD等，在PVANet基础上发展EAST等。&lt;/p&gt;
&lt;h2 id=&#34;本文思路解决方案&#34;&gt;本文思路/解决方案&lt;/h2&gt;
&lt;p&gt;整体使用特征金字塔结构，不同尺度的特征图融合得到特征F，在此基础上得到预测图和阈值图，利用阈值图T对预测图P二值化，得到二值化的图像B，最终得到检测结果。在训练过程中预测图和二值化图使用相同的目标监督，在预测的时候可以通过预测图或者二值化图中的任意一个得到目标检测结果。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;model.png&#34; srcset=&#34;
               /post/paper-db-mod/model_hu8b91cfa8a958a3ea05e65647422b5d1f_225050_ad1041e642619a7fa78042fff7304f10.webp 400w,
               /post/paper-db-mod/model_hu8b91cfa8a958a3ea05e65647422b5d1f_225050_7e7e08eb3023e5c6d400daa1d682dd4b.webp 760w,
               /post/paper-db-mod/model_hu8b91cfa8a958a3ea05e65647422b5d1f_225050_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-db-mod/model_hu8b91cfa8a958a3ea05e65647422b5d1f_225050_ad1041e642619a7fa78042fff7304f10.webp&#34;
               width=&#34;760&#34;
               height=&#34;244&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;为了便于梯度传播，这里使用了可微的二值化函数代替标准的二值化。



$$\begin{align}
\hat{B}_{i,j} = \frac{1}{1+e^{-k(P_{i,j}-T_{i,j})}}
\end{align}$$

同时在文章中使用的ResNet-18和ResNet-50中采用了可变形卷积取代原本的卷积操作，经实验有一定的性能提升。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;gt.png&#34; srcset=&#34;
               /post/paper-db-mod/gt_hu57c744a76dc6991a3a74ef49fddf2109_183791_aaa07ba932c4d5e94a77273d824652c2.webp 400w,
               /post/paper-db-mod/gt_hu57c744a76dc6991a3a74ef49fddf2109_183791_26f05d2db6f8c366547ab23074976da8.webp 760w,
               /post/paper-db-mod/gt_hu57c744a76dc6991a3a74ef49fddf2109_183791_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-db-mod/gt_hu57c744a76dc6991a3a74ef49fddf2109_183791_aaa07ba932c4d5e94a77273d824652c2.webp&#34;
               width=&#34;760&#34;
               height=&#34;371&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;在生成标签时，使用Vatti算法，将多边形框收缩得到预测图的标签。同时将多边形框收缩和膨胀，其中的区域作为阈值图的区域，阈值图的取值由距离原多边形框的距离确定。&lt;/p&gt;
&lt;p&gt;在计算loss的时候，使用 Binary Cross-Entropy(BCE)作为预测图和二值化图的损失函数，使用L1距离作为阈值图的损失函数。其中$R_d$是膨胀后的边界框内部的像素，$S_l$是用于计算的数据集合。



$$\begin{align}
L &amp;= L_s+\alpha L_b+\beta L_t \\
L_s = L_b &amp;= \sum\limits_{i \in S_l}y_i\log x_i + (1-y_i)\log(1-x_i) \\
L_t &amp;= \sum\limits_{i\in R_d}|y_i^* - x_i^*|
\end{align}$$
&lt;/p&gt;
&lt;h2 id=&#34;结果&#34;&gt;结果&lt;/h2&gt;
&lt;p&gt;使用了可变形卷积和DB模块之后，模型在CTW1500和MSRA-TD500上的性能均有所提升。为阈值图添加监督之后性能也有所提升。在卷曲文本、多语言文本、多朝向文本上的效果均相比之前的模型有所提升。&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;引入了DB模块，通过二值化阈值的预测提升检测的结果，同时使用了比较精简的模型，具有较快的处理速度。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>【论文】EAST: An Efficient and Accurate Scene Text Detector</title>
      <link>https://yangleisx.github.io/post/paper-east/</link>
      <pubDate>Sun, 06 Dec 2020 11:01:22 +0800</pubDate>
      <guid>https://yangleisx.github.io/post/paper-east/</guid>
      <description>&lt;p&gt;论文题目：EAST: An Efficient and Accurate Scene Text Detector&lt;/p&gt;
&lt;p&gt;作者：Xinyu Zhou, Cong Yao, He Wen, Yuzhi Wang, Shuchang Zhou, Weiran He, and Jiajun Liang&lt;/p&gt;
&lt;p&gt;会议/时间：CVPR2017&lt;/p&gt;
&lt;p&gt;链接：&lt;a href=&#34;https://arxiv.org/abs/1704.03155v2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/1704.03155v2&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;论文目标&#34;&gt;论文目标&lt;/h2&gt;
&lt;p&gt;传统的文字检测模型尽管可以取得不错的效果，但是往往由多个阶段和结构组成，这样复杂的结构往往会影响到整体的性能，因此本文的作者设计了一种简单快速的pipeline，可以从给定图像中直接检测到文本的位置。通过使用一个简单的神经网络而不是多个模块组合的方式加快了处理的速度，简化了模型的复杂度。&lt;/p&gt;
&lt;h2 id=&#34;相关工作&#34;&gt;相关工作&lt;/h2&gt;
&lt;p&gt;常规的处理方法依赖于人工设计的特征，例如SWT和MSER等模型通过边缘检测等方式。但是这些依赖人工设计特征的算法在处理一些有挑战性的场景时效果比较差，例如低分辨率或者出现失真的情况下。&lt;/p&gt;
&lt;p&gt;相比之下，基于深度神经网络的文本检测算法由于能取得更好的效果，逐渐成为主流，包括使用CNN对文本检测的结果进行筛选或者使用FCN生成热力图/分割图对原始图像处理得到检测结果。但是多数基于深度神经网络的模型由多个阶段组成，结构比较复杂性能也比较差。&lt;/p&gt;
&lt;h2 id=&#34;本文思路解决方案&#34;&gt;本文思路/解决方案&lt;/h2&gt;
&lt;p&gt;本文设计了一个基于FCN的神经网络，且完整的模型只有两个阶段，省去了冗余的中间处理阶段，即神经网络得到预测的文本框，再通过非极大值抑制得到最终的预测结果。&lt;/p&gt;
&lt;p&gt;基于FCN的网络结构设计如下，通过神经网络直接得到了三类型的输出：Score Map、Rotated Box、Quadrangle。其中的score map为置信度，置信度超过给定阈值的预测框通过NMS得到最终的输出。通过使用U-Net可以融合不同尺度的特征，有助于检测不同大小的文本目标。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;model.png&#34; srcset=&#34;
               /post/paper-east/model_hu7702e6f292f461fa99c7525a189fba9c_171712_4db68f420c1e788ffa18bcab762f9629.webp 400w,
               /post/paper-east/model_hu7702e6f292f461fa99c7525a189fba9c_171712_b71d3e980e5eb1c817dd05e8f9b7a166.webp 760w,
               /post/paper-east/model_hu7702e6f292f461fa99c7525a189fba9c_171712_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-east/model_hu7702e6f292f461fa99c7525a189fba9c_171712_4db68f420c1e788ffa18bcab762f9629.webp&#34;
               width=&#34;760&#34;
               height=&#34;711&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;在生成GT时，将原本的四边形缩小到0.7倍并二值化可以得到Score Map，对于没有RBOX标注的数据，首先根据QUAD创建一个最小矩形，再计算每个点到四个边界的距离得到RBOX标注。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;gt.png&#34; srcset=&#34;
               /post/paper-east/gt_hu1f193018c86b9eb08826848ab65faa45_342844_34e35a81e147deee62c57d84a63c1f07.webp 400w,
               /post/paper-east/gt_hu1f193018c86b9eb08826848ab65faa45_342844_ee9fdb7b1e1f88f6543120f35e2e41d3.webp 760w,
               /post/paper-east/gt_hu1f193018c86b9eb08826848ab65faa45_342844_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-east/gt_hu1f193018c86b9eb08826848ab65faa45_342844_34e35a81e147deee62c57d84a63c1f07.webp&#34;
               width=&#34;663&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;对于损失函数的计算，采用了Score Map损失和Geometry损失的加权平均值。由于在自然场景下的图像中，检测目标与背景占据的面积不均衡，在计算Loss的时候会受到影响，为了不引入额外的处理流程，计算Score Map损失时使用了class-balanced cross-entropy，可以比较好的解决正反例不均衡的情况。&lt;/p&gt;



$$\begin{aligned}
L_s =&amp;\; \operatorname{balances-xent}(\mathbf{\hat{Y}}, \mathbf{Y^*}) \\
=&amp; -\beta\mathbf{Y^*}\log\mathbf{\hat{Y}}-(1-\beta)(1-\mathbf{Y^*})\log(1-\mathbf{\hat{Y}}) \\
\beta =&amp;\; 1 - \frac{\sum_{y^*\in\mathbf{Y^*}}y^*}{|\mathbf{Y^*}|}
\end{aligned}$$

&lt;p&gt;对于Geometry输出的损失函数，在RBOX的情形中，对于AABB部分使用IOU损失，对于倾斜角的部分使用余弦函数计算损失。



$$\begin{aligned}
L_{AABB} =&amp;\; -\log\operatorname{IoU}(\mathbf{\hat{R}},\mathbf{R^*}) \\
=&amp;\;-\log\frac{|\mathbf{\hat{R}}\cap\mathbf{R^*}|}{|\mathbf{\hat{R}}\cup\mathbf{R^*}|} \\
L_\theta(\hat{\theta},\theta^*) =&amp;\;1 - \operatorname{cos}(\hat{\theta} - \theta^*) \\
L_g =&amp;\;L_{AABB} + \lambda_\theta L_\theta
\end{aligned}$$

在QUAD的情形中，使用增加了正则项的smoothed-L1损失计算。



$$\begin{aligned}
L_g =&amp;\; L_{QUAD}(\mathbf{\hat{Q}},\mathbf{Q^*}) \\
=&amp;\;\min\limits_{\mathbf{\tilde{Q}} \in P_{\mathbf{Q^*}}}\sum\limits_{c_i \in C_{\mathbf{Q}} \\\tilde{c}_i\in C_{\mathbf{\tilde{Q}}}} \frac{\operatorname{smoothed_{L1}}(c_i - \tilde{c}_i)}{8 \times N_{\mathbf{Q^*}}} \\
N_{\mathbf{Q^*}} =&amp;\;\min\limits_{i=1}^{4}D(p_i, p_{(i \operatorname{mod} 4)+1})
\end{aligned}$$
&lt;/p&gt;
&lt;p&gt;对于模型输出的结果，使用阈值筛选之后，再使用作者设计的一种基于合并候选框的NMS算法处理得到结果。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;nmns.png&#34; srcset=&#34;
               /post/paper-east/nmns_hu7a76d492d93e392f745dc73a46a3485f_127059_187dc3dc0db2c0ead92e487d7a46dbec.webp 400w,
               /post/paper-east/nmns_hu7a76d492d93e392f745dc73a46a3485f_127059_c83a7786e4151ad26f5453903adc6441.webp 760w,
               /post/paper-east/nmns_hu7a76d492d93e392f745dc73a46a3485f_127059_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-east/nmns_hu7a76d492d93e392f745dc73a46a3485f_127059_187dc3dc0db2c0ead92e487d7a46dbec.webp&#34;
               width=&#34;760&#34;
               height=&#34;664&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;结果&#34;&gt;结果&lt;/h2&gt;
&lt;p&gt;在ICDAR2015、COCO-Text和MSRA-TD500三个数据集上进行测试。同时对于模型的骨架也使用了PVANET、PVANET2x和VGG16三种不同的结构进行测试（网络框架都在InageNet上预训练）。经测试模型在上述数据集上能取得超过SOTA的F值。在处理速度上也能达到比较高的FPS。&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;本文提出的EAST模型，通过减少冗余的处理阶段，得到了简单快速的处理效果，通过FCN网络直接生成预测的结果（geometry map），结合NMS处理多余的候选框，可以得到比较好的效果。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>【论文】Shape Robust Text Detection with Progressive Scale Expansion Network</title>
      <link>https://yangleisx.github.io/post/paper-psenet/</link>
      <pubDate>Thu, 19 Nov 2020 10:55:55 +0800</pubDate>
      <guid>https://yangleisx.github.io/post/paper-psenet/</guid>
      <description>&lt;p&gt;论文题目：Shape Robust Text Detection with Progressive Scale Expansion Network&lt;/p&gt;
&lt;p&gt;作者：Wenhai Wang, Enze Xie, Xiang Li, Wenbo Hou, Tong Lu, Gang Yu, Shuai Shao&lt;/p&gt;
&lt;p&gt;会议/时间：CVPR2019&lt;/p&gt;
&lt;p&gt;链接：&lt;a href=&#34;https://arxiv.org/pdf/1903.12473.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arxiv&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;论文目标&#34;&gt;论文目标&lt;/h2&gt;
&lt;p&gt;在当前的文字检测算法在应用时有两个问题：当前的算法通常得到一个四边形边界框，难以检测任意形状的文本；如果两行文本距离比较近，有可能会被框选为同一个边界框。因此提出了PSENet，可以有效的解决上述的两个问题。&lt;/p&gt;
&lt;h2 id=&#34;相关工作&#34;&gt;相关工作&lt;/h2&gt;
&lt;p&gt;现有的基于CNN的文本检测模型可以分为两类：基于回归的方案和基于分割的方案。前者生成四边形的边界框，无法处理任意形状的文本，后者使用像素级的分类得到目标区域，但是很难区分开相聚比较近的目标。&lt;/p&gt;
&lt;p&gt;基于回归的文本检测方案大多是基于通用的目标检测模型，包括Faster R-CNN等。其他的文本检测模型还有TextBoxes、EAST等。大多数这一类的模型都需要设计Anchor而且由多个处理阶段组成，可能会导致性能比较差。基于分割的文本检测方案主要使用FCN，例如通过FCN获得热力图等，再进行后处理获得文本位置。&lt;/p&gt;
&lt;h2 id=&#34;本文思路解决方案&#34;&gt;本文思路/解决方案&lt;/h2&gt;
&lt;p&gt;本文提出的PSENet是基于分割的方案，每一个预测的分割称为kernel，形状相似但是大小不同，最后设计了一个基于BFS的渐进扩展算法，将原本的kernel扩展得到最终的预测分割。由于使用渐近扩展式的算法，对于最小尺寸的kernel，可以区分开距离较近的文本，同时也可以解决小尺寸分割难以覆盖完整文本的问题。&lt;/p&gt;
&lt;p&gt;模型结构是基于ResNet的FPN结构，从中选取不同大小的特征图连接得到混合特征图，最后通过卷积等操作得到不同尺寸的多个分割图，再经过尺寸扩展得到预测结果。&lt;/p&gt;
&lt;!-- ![pipeline](pipeline.png) --&gt;
&lt;p&gt;在这个尺寸扩展算法中，首先选择了尺寸最小的分割图进行连通域分析作为kernel，然后将其他的分割图作为输入，通过Scale Expansion算法计算新的扩展后的kernel，最终得到结果。作者提到对于位于多个文本之间的像素，使用先来先服务的方式合并到不同的标签中，同时由于使用了渐进式的方法，这些边界上的重合并不会影响最终的处理结果，这可能也是作者选择多个不同尺寸的分割图渐近处理的原因。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;algorithm&#34; srcset=&#34;
               /post/paper-psenet/algorithm_hud4cda9c211fdf528e67360bca4084d03_138031_d0c1d0e711bbff73edf17c946f5196ad.webp 400w,
               /post/paper-psenet/algorithm_hud4cda9c211fdf528e67360bca4084d03_138031_6a4e39df643c92d65b747a3533f377a0.webp 760w,
               /post/paper-psenet/algorithm_hud4cda9c211fdf528e67360bca4084d03_138031_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-psenet/algorithm_hud4cda9c211fdf528e67360bca4084d03_138031_d0c1d0e711bbff73edf17c946f5196ad.webp&#34;
               width=&#34;760&#34;
               height=&#34;564&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;在训练过程中，为了获得不同尺寸的分割图，需要提供对应的标签供学习，作者使用了Vatti clipping algorithm来将最初的文本框标签收缩一定的像素得到这些标签。算法中使用到的像素值通过下面的公式计算得到。其中m为最小的缩放比例，n为不同尺寸的分割图的个数。&lt;/p&gt;



$$\begin{aligned}
d_i = \frac{\mathrm{Area}(p_n)\times(1-r_i^2)}{\mathrm{Perimeter}(p_n)}\notag\\
r_i = 1 - \frac{(1-m)\times(n-i)}{n-1}
\end{aligned}$$

&lt;p&gt;在实验中作者使用了Dice Coefficient作为模型的评价指标，使用了完整尺寸的标签和缩放后的标签上的Dice系数作为损失函数来指导模型的学习。其中对于完整尺寸的标签，使用Online Hard Example Mining(OHEM)来获得一个mask协助训练。



$$\begin{aligned}
L &amp;= \lambda L_c + (1-\lambda)L_s\notag\\
L_c &amp;= 1 - D(S_n\cdot M, G_n \cdot M)\notag\\
L_s &amp;= 1 - \frac{\sum\limits^{n-1} D(S_i\cdot W, G_i \cdot W)}{n-1}\notag\\
W_{x,y} &amp;= \left\{
\begin{align}
1,&amp; \quad if\ S_{n,x,y} \geq0.5;\notag\\
0,&amp; \quad otherwise\notag\\
\end{align}
\right.
\end{aligned}$$
&lt;/p&gt;
&lt;h2 id=&#34;结果&#34;&gt;结果&lt;/h2&gt;
&lt;p&gt;实验包括了四个数据集：CTW1500、Total-Text、ICDAR2015和ICDAR2017MLT。模型使用预训练好的ResNet，在IC17-MLT上训练，而且没有使用另外的人造数据集。实验讨论的结果包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;最小尺寸的kernel并不能直接作为模型的输出，模型的F-measure结果比较差，而且文本框内容的识别结果也比较差。&lt;/li&gt;
&lt;li&gt;对于最小缩放比例m的选择，选择太大或者太小都会导致性能的下降。&lt;/li&gt;
&lt;li&gt;分割图的个数n增加时，性能会有一定的上升，但是不能无限制增大，在n大于5之后性能提升不大。&lt;/li&gt;
&lt;li&gt;修改模型骨架，例如增加ResNet的层数也会提升模型的性能。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;使用了基于FPN的结构，将不同尺度的特征图上采样并连接在一起。&lt;/p&gt;
&lt;p&gt;获得不同尺度下的分割图，再从小到大渐进式的合并，不仅可以检测到任意形状的文本，也可以避免将距离比较近的文本识别为同一对象。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>【论文】Character Region Awareness for Text Detection</title>
      <link>https://yangleisx.github.io/post/paper-craft/</link>
      <pubDate>Wed, 21 Oct 2020 15:10:45 +0800</pubDate>
      <guid>https://yangleisx.github.io/post/paper-craft/</guid>
      <description>&lt;p&gt;论文题目：Character Region Awareness for Text Detection&lt;/p&gt;
&lt;p&gt;作者：Youngmin Baek, Bado Lee, Dongyoon Han, Sangdoo Yun, and Hwalsuk Lee&lt;/p&gt;
&lt;p&gt;会议/时间：CVPR2019&lt;/p&gt;
&lt;p&gt;链接：&lt;a href=&#34;https://arxiv.org/pdf/1904.01941.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/pdf/1904.01941.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;论文目标&#34;&gt;论文目标&lt;/h2&gt;
&lt;p&gt;今年来使用深度神经网络实现的文本检测得到许多关注，但是过去的文本检测工作大多使用边界框框选每一个词，存在一定的缺陷，例如在单词非常长或者扭曲畸形的情况下效果不好，本文提出了一种基于检测单个字符的文本检测模型，通过检测连续的字符实现自下而上的单词检测。由于成本非常高，现有的文本检测数据库并没有提供字符级的标注，文中使用弱监督学习方法，可以在单词级标注的数据集上训练字符级模型。&lt;/p&gt;
&lt;h2 id=&#34;相关工作&#34;&gt;相关工作&lt;/h2&gt;
&lt;p&gt;很多的文本检测模型（即regression-based text detectors），使用了目标检测模型中常用的边框回归思路。尽管取得了比较好的效果，但是不能应对实际场景下的各种形状的文字。一些文本检测模型（segmentation-based text detectors），在像素级分割和检测文本区域。还有一些端到端的文本检测工具将文本检测和识别任务结合在一起，可以避免一些背景中的图形的影响，提高检测效果。大多数的文本检测是以单词为识别单位，但是在标注和划分时很难确定，造成效果较差。&lt;/p&gt;
&lt;h2 id=&#34;本文思路解决方案&#34;&gt;本文思路/解决方案&lt;/h2&gt;
&lt;p&gt;本文构建卷积神经网络，从图像中学习得到字符（region score）和字符之间的连接关系（affinity score），从而从数据中检测单词和句子。&lt;/p&gt;
&lt;p&gt;模型使用添加了BatchNormal的VGG-16作为基本结构，通过添加解码器和短路连接构建了类似U-Net的模型结构，最终输出2通道的特征图。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;model.png&#34; srcset=&#34;
               /post/paper-craft/model_hu1fb335c6275fa73c3302582f84ad14aa_169023_bf6beda1230d9fca60342c84beb180a8.webp 400w,
               /post/paper-craft/model_hu1fb335c6275fa73c3302582f84ad14aa_169023_f59ca2c5a7d8734e5c20732f4921ff95.webp 760w,
               /post/paper-craft/model_hu1fb335c6275fa73c3302582f84ad14aa_169023_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-craft/model_hu1fb335c6275fa73c3302582f84ad14aa_169023_bf6beda1230d9fca60342c84beb180a8.webp&#34;
               width=&#34;745&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;模型输出的特征图分别表示字符和字符之间的连接关系。region score表示当前像素为一个字符中心点的概率，affnity score表示当前像素为两个字符中间点的概率。本文使用了高斯热力图来表示字符的位置，相比使用几何形状框选，更容易表示各种形状的字符。&lt;/p&gt;
&lt;p&gt;数据标注的生成方式 如下，分别对每一个字符使用四边形框选，在每个框中选择上下两个三角的中心点生成新的四边形，将二维高斯热图变换填充进去得到。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;feature.png&#34; srcset=&#34;
               /post/paper-craft/feature_hu15c26d634247a3f161d99974d70781e1_447230_a761ea2b0fa2f3074412be13bd684d5f.webp 400w,
               /post/paper-craft/feature_hu15c26d634247a3f161d99974d70781e1_447230_977a16b7efaaa9b70dcfcdeffaa03c9f.webp 760w,
               /post/paper-craft/feature_hu15c26d634247a3f161d99974d70781e1_447230_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-craft/feature_hu15c26d634247a3f161d99974d70781e1_447230_a761ea2b0fa2f3074412be13bd684d5f.webp&#34;
               width=&#34;760&#34;
               height=&#34;246&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;在训练过程中，由于实际的数据集只有单词级或者句子级的标注，首先使用合成的样本训练得到一个临时的模型，然后将实际数据集中的单词或者句子裁剪出来通过模型得到单词边界框，与原数据预测的结果相比计算置信度。一种可行的方法是使用字符框的个数与单词长度相比较得到置信度，作为计算目标函数时的像素权重。即



$$L = \sum\limits_{p} S_c(p)\cdot(||S_r(p) - S_r^*(p)||^2_2+||S_a(p) - S_a^*(p)||^2_2)$$

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;training.png&#34; srcset=&#34;
               /post/paper-craft/training_huadad208ab8e79fcd0709c63ec18385c2_546833_9942b4b369218a32148bfb4cb853c996.webp 400w,
               /post/paper-craft/training_huadad208ab8e79fcd0709c63ec18385c2_546833_994ce79cd40bd3601e3851e8a7514a04.webp 760w,
               /post/paper-craft/training_huadad208ab8e79fcd0709c63ec18385c2_546833_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-craft/training_huadad208ab8e79fcd0709c63ec18385c2_546833_9942b4b369218a32148bfb4cb853c996.webp&#34;
               width=&#34;760&#34;
               height=&#34;297&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;最后在预测时需要进行相应的后处理，例如使用矩形框将检测到的文本从原数据裁剪出来包括如下操作：分别设置阈值将特征图转为二值，使用连通区域标记技术从二值图中框选单词，最后选择一个矩形框将上述连通区域框选出来&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;。或者使用多边形折线框框选：每个字符使用相同长度的竖线表示，竖线的中点连线得到单词中线，上述竖线转至与中线垂直，以端点为多边形的顶点绘制中线的平行线。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;postope.png&#34; srcset=&#34;
               /post/paper-craft/postope_hub9afa4812e8038ab856d53fcf75acc1b_226114_600e6bbdc74a9f46093b09cc947b74f3.webp 400w,
               /post/paper-craft/postope_hub9afa4812e8038ab856d53fcf75acc1b_226114_3314b2bbe73a1c098795fa27889323d0.webp 760w,
               /post/paper-craft/postope_hub9afa4812e8038ab856d53fcf75acc1b_226114_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-craft/postope_hub9afa4812e8038ab856d53fcf75acc1b_226114_600e6bbdc74a9f46093b09cc947b74f3.webp&#34;
               width=&#34;760&#34;
               height=&#34;587&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;结果&#34;&gt;结果&lt;/h2&gt;
&lt;p&gt;在选择的六个数据集上，经过训练和测试，都实现了超过SOTA的效果。可以证明使用字符级的检测效果比较好。&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;提出了一种基于检测单字符从而实现文本检测的方法，针对数据标注比较少的情况引入了弱监督的训练方式，取得了比较好的结果。&lt;/p&gt;
&lt;p&gt;模型通过检测字符而不是单词，在感受野比较小的情况下具有较好的鲁棒性，但是只能针对字符相分离的语言，不能处理孟加拉语、阿拉伯语等语言。模型中只有文本检测没有文本识别，与端到端的模型相比性能受限，但是在多个数据集上都取得了非常好的结果，证明泛化能力比较强。&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;可以使用opencv中提供的connectedComponents函数和minAreaRect函数等实现。&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>【论文】Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations</title>
      <link>https://yangleisx.github.io/post/paper-challenge-disentabgle/</link>
      <pubDate>Tue, 20 Oct 2020 22:06:26 +0800</pubDate>
      <guid>https://yangleisx.github.io/post/paper-challenge-disentabgle/</guid>
      <description>&lt;p&gt;论文题目：Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations&lt;/p&gt;
&lt;p&gt;作者：Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Rätsch, Sylvain Gelly, Bernhard Schölkopf,  Olivier Bachem&lt;/p&gt;
&lt;p&gt;会议/时间：ICML2019&lt;/p&gt;
&lt;p&gt;链接：&lt;a href=&#34;https://arxiv.org/pdf/1811.12359.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/pdf/1811.12359.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;论文目标&#34;&gt;论文目标&lt;/h2&gt;
&lt;p&gt;近年来关于disentangled representation 的unsupervised learning相关的研究非常多，其核心观点为现实世界中的数据是通过一些单独的可解释的元素生成的，可以通过无监督学习的方式学习到数据的一种表示。本文通过理论分析，证明在模型和数据不添加归纳偏置的情况下这种无监督学习是不可能实现的，同时文中在多个数据集上训练了近年来的相关模型。最终提出了进一步研究的方向。&lt;/p&gt;
&lt;h2 id=&#34;相关工作&#34;&gt;相关工作&lt;/h2&gt;
&lt;p&gt;目前效果比较好的模型大多是基于变分自编码器（Variational Autoencoder，VAE），即认为现实世界中的数据来源于一个高维隐变量$z \sim P(z)$，实际观察到的数据为$x$，通过建立一个自编码器，包括编码器和解码器学习得到$P(x|z)$和$Q(z|x)$，其中使用$Q(z|x)$近似实际的$P(z|x)$，从而从数据$x$中学到一个表示$r(x)$去寻找隐变量$z$。&lt;/p&gt;
&lt;p&gt;这一领域之前的研究包括独立成分分析（Independent Component Analysis，ICA）等。&lt;/p&gt;
&lt;h2 id=&#34;本文思路解决方案&#34;&gt;本文思路/解决方案&lt;/h2&gt;
&lt;p&gt;给定如下的理论&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For $d &amp;gt; 1$ , let $z \sim P$ denote any distribution which admits a density $p(z) = \prod_{i=1}^{d}p(z_i)$. Then, there exists an infinite family of bijective functions f : supp(z) → supp(z) such that $\frac {\partial f_i(u)}{\partial u_j}$ almost everywhere for all $i$ and $j$ (i.e., $z$ and $f(z)$ are completely entangled) and $P(z \leq u) = P(f(z) \leq u)$ for all $u \in supp(z)$ (i.e., they have the same marginal distribution).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;可以得到，对于给定的数据$x$，可以找到无限多的$p(z)$满足要求，且这些$p(z)$相互等价并满足disentangled的要求，一个无监督的模型难以区分这些$p(z)$。因此在实际的模型中，需要为模型结构和数据集引入合理的归纳偏置（inductive bias）。&lt;/p&gt;
&lt;p&gt;在实验设计中，使用到的模型为添加了正则项的VAE，包括betaVAE、AnnealedVAE、FactorVAE、beta-TCVAE、DIP-VAE等。在模型的度量标准中，使用了包括BetaVAE Metric、FactorVAE Metric、MIG(Mutual Information Gap)、Modularity、SAP Score等方法。使用到的数据集包括四个3D空间变化的数据集和三个随机的噪声数据集。&lt;/p&gt;
&lt;p&gt;在归纳偏置中，为了控制变量，对于所有的模型使用相同的卷积结构、优化算法、超参数，使用相同的Gaussian Encoder和Bernoulli Decoder，使用相同的隐变量维度（=10），构建模型进行测试。仅使用不同的正则项和不同的正则项权重。&lt;/p&gt;
&lt;h2 id=&#34;结果&#34;&gt;结果&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;当正则化系数增大时，使用拟合高斯的采样得到的相关性减小，使用均值表示的相关性增大。可以证明上述模型可以得到不同维度相关性比较弱的聚合先验，但是并不能表示均值表示也是不相关的。&lt;/li&gt;
&lt;li&gt;在不同的性能度量中，除了Modularity以外的几种都是相关的，只是在不同的数据集上的相关性有差异。&lt;/li&gt;
&lt;li&gt;模型的性能受到随机初始化和超参数的影响比较大，目标函数在其中影响比较小。&lt;/li&gt;
&lt;li&gt;如何选择无监督模型仍然等待得到解决。在不同数据集和度量指标之间的超参数迁移用处不大。&lt;/li&gt;
&lt;li&gt;实验并不能证明这些disentangled表示对于下游的任务有所帮助。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;在将来的研究中可以将重点更多放在如下三个方面：1）归纳偏置和显/隐性监督，2）disentangled representation的实际效益，3）实验设置和数据集的多样性。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>【论文】Non-local Neural Networks</title>
      <link>https://yangleisx.github.io/post/paper-non-local/</link>
      <pubDate>Sun, 11 Oct 2020 14:14:05 +0800</pubDate>
      <guid>https://yangleisx.github.io/post/paper-non-local/</guid>
      <description>&lt;p&gt;论文题目：Non-local Neural Networks&lt;/p&gt;
&lt;p&gt;作者：Xiaolong Wang, Ross Girshick, Abhinav Gupta, Kaiming He&lt;/p&gt;
&lt;p&gt;会议/时间：CVPR2018&lt;/p&gt;
&lt;p&gt;链接：&lt;a href=&#34;https://arxiv.org/abs/1711.07971&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/1711.07971&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;论文目标&#34;&gt;论文目标&lt;/h2&gt;
&lt;p&gt;当前的CNN或者RNN结构中的卷积等操作都是针对数据中的一个小区域（local neighborhood）处理和提取特征，并没有考虑到数据中的长期/远距离依赖关系。因此设计一个non-local操作，可以考虑到输入数据中的每一个位置上的特征和依赖关系，学习到全局信息。&lt;/p&gt;
&lt;h2 id=&#34;相关工作&#34;&gt;相关工作&lt;/h2&gt;
&lt;p&gt;在CNN中通过多次卷积扩大感受野，在RNN中通过设计迭代模型学习到序列中的远距离特征/信息，但是在实际的每一次处理过程中都只考虑到局部的信息。通过重复处理局部信息获得全局信息不仅计算效率低，而且引入了优化的困难。&lt;/p&gt;
&lt;p&gt;参考了传统的CV领域使用的non-local mean operation方法。其他相关的内容包括Graphical models、Feedforward modeling for sequences、self-attention、interaction networks、video classification architectures等。实际上self-attention可以看作是non-local的一种情况。&lt;/p&gt;
&lt;h2 id=&#34;本文思路解决方案&#34;&gt;本文思路/解决方案&lt;/h2&gt;
&lt;p&gt;通过考虑特征图中每一个位置的加权和来得到特定位置的响应。&lt;/p&gt;
&lt;p&gt;一方面可以直接学习到数据中远距离的信息，另一方面使用较浅层的网络也能实现比较好的结果，而且作者设计的non-local模块不改变数据的大小，可以方便的插在现有的网络中。&lt;/p&gt;
&lt;p&gt;简单来说，non-local的思路如下，其中$x_i$为输出位置，$x_j$为数据中的每一个点，使用$f(x_i,x_j)$计算两者之间的关系并作为权重计算输入数据特征$g(x)$的加权和。
$$
y_i = \frac{1}{C(x)}\sum\limits_{\forall j}f(x_i, x_j)g(x_j)
$$
在实际使用过程中，函数$f(\cdot)$和$g(\cdot)$有多种选择。后者常选用$1\times1\times1$的卷积实现。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Function Type&lt;/th&gt;
&lt;th&gt;Pairwise Function&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Gaussian&lt;/td&gt;
&lt;td&gt;$f(x_i,x_j) = e^{x_i^T x_j}$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Embedded Gaussian&lt;/td&gt;
&lt;td&gt;$f(x_i, x_j) =e^{\theta(x_i)^T\phi(x_j)}$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Dot product&lt;/td&gt;
&lt;td&gt;$f(x_i,x_j) = \theta(x_i)^T\phi(x_j)$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Concatenation&lt;/td&gt;
&lt;td&gt;$f(x_i, x_j) = ReLU(w_f^T[\theta(x_i),\phi(x_j)])$&lt;br /&gt;其中$[\cdot, \cdot]$表示连接得向量并经$w_f$变成标量&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;通过将non-local block定义为残差结构，使得模块输出的形状不发生变化，可以将non-local block插入到已有的预训练模型中，而不改变其性能（$W_z$初始化为0）。通过在较高的特征层加入该block，同时引入降采样，可以减小引入的计算量。
$$
z_i = W_z y_i + x_i
$$&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;non-local-block.png&#34; srcset=&#34;
               /post/paper-non-local/non-local-block_hub61eb3888fc09e20847c7442e0a9d14a_73071_a93e2641d4f6e406cc22ae567edbc5a0.webp 400w,
               /post/paper-non-local/non-local-block_hub61eb3888fc09e20847c7442e0a9d14a_73071_5351826cc5d1010ab1a072b78fd03b72.webp 760w,
               /post/paper-non-local/non-local-block_hub61eb3888fc09e20847c7442e0a9d14a_73071_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-non-local/non-local-block_hub61eb3888fc09e20847c7442e0a9d14a_73071_a93e2641d4f6e406cc22ae567edbc5a0.webp&#34;
               width=&#34;760&#34;
               height=&#34;584&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;结果&#34;&gt;结果&lt;/h2&gt;
&lt;p&gt;经过测试，添加了non-local block的模型具有更高的预测准确率，在non-local block中选择不同的函数计算数据之间的距离（即$f(x_i,x_j)$）对于最后的模型效果影响不大。而且在模型中添加了时空维度上的nonn-local block后效果相比单纯的时间或空间维度的效果更好。&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;提出了一种non-local的结构学习数据中距离比较远的特征的影响。并且实现了一种non-local block可以插入到现有的网络结构中并提升其性能。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>【论文】Feature Extraction for Visual Speaker Authentication against Computer-Generated Video Attacks</title>
      <link>https://yangleisx.github.io/post/paper-deanet/</link>
      <pubDate>Thu, 08 Oct 2020 16:18:37 +0800</pubDate>
      <guid>https://yangleisx.github.io/post/paper-deanet/</guid>
      <description>&lt;p&gt;论文题目：Feature Extraction for Visual Speaker Authentication against Computer-Generated Video Attacks&lt;/p&gt;
&lt;p&gt;作者：Jun Ma, Shilin Wang, Senior Member, IEEE, Aixin Zhang and Alan Wee-Chung Liew&lt;/p&gt;
&lt;p&gt;会议/时间：IEEE ICIP 2020&lt;/p&gt;
&lt;h2 id=&#34;论文目标&#34;&gt;论文目标&lt;/h2&gt;
&lt;p&gt;使用唇语特征进行身份认证具有一定的活体检测能力，但是容易受到使用DeepFake等技术构建的视频攻击，因此构建一个神经网络从视频中提取动态的说话习惯信息，同时尽量减少唇语特征身份认证对于唇部静态特征的依赖。&lt;/p&gt;
&lt;h2 id=&#34;相关工作&#34;&gt;相关工作&lt;/h2&gt;
&lt;p&gt;之前的工作中他人使用了唇部的图像和运动特征、纹理描述符等方式实现了比较不错的结果。作者所在 团队之前的工作中使用3D残差单元实现了唇部动态和静态特征的提取。&lt;/p&gt;
&lt;p&gt;近来使用DeepFake换脸技术可以很容易伪造讲话视频，甚至可以在单照片的数据集上实现。使用唇部特征的认证系统由于过多依赖静态特征，受到一定的威胁。&lt;/p&gt;
&lt;p&gt;本文提出的网络结构的基础包括frame difference、self-attention、non-local neural network&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;等。&lt;/p&gt;
&lt;h2 id=&#34;本文思路解决方案&#34;&gt;本文思路/解决方案&lt;/h2&gt;
&lt;p&gt;构建了一个深度神经网络结构提取用户唇部特征用于认证。包括两个模块：Difference block（Diff-block）和Dynamic Response block（DR-block）两者相互补用于提取用户动态讲话特征信息。&lt;/p&gt;
&lt;h3 id=&#34;diff-block&#34;&gt;Diff-block&lt;/h3&gt;
&lt;p&gt;给定长度为T帧的视频，通过计算每一帧图像与其他T-1帧图像的相关性来消除静态特征。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;diff-block.png&#34; srcset=&#34;
               /post/paper-deanet/diff-block_hu291c489b96324781e1b21136a49b4d65_233820_3977e3fe2713197caeb74b468bb9303b.webp 400w,
               /post/paper-deanet/diff-block_hu291c489b96324781e1b21136a49b4d65_233820_94b02e349033672c12bc9af8eb039084.webp 760w,
               /post/paper-deanet/diff-block_hu291c489b96324781e1b21136a49b4d65_233820_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-deanet/diff-block_hu291c489b96324781e1b21136a49b4d65_233820_3977e3fe2713197caeb74b468bb9303b.webp&#34;
               width=&#34;723&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;$$
Y_t = \sum\limits_{t\neq j}f(x_t,x_j)\times(x_t-x_j)\
f(x_t, x_j) = softmax(\theta(x_t)^T\varphi(x_j))
$$
其中$\theta(x_t)$和$\varphi(x_j)$为输入数据分别经由两组不同的Conv+Pool之后得到的，经过转置和相乘得到 $T*T$ 形状的张量，表示两帧数据之间的相关性。将原数据经过 &lt;strong&gt;D-value操作&lt;/strong&gt; 之后得到每一帧与其他帧的差值，并按照上述操作得到的相关性矩阵加权求和得到最终的输出。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;d-value.png&#34; srcset=&#34;
               /post/paper-deanet/d-value_hua6f719dde7ea88865cf20c8321514208_162201_399516e5f3504ecd25cfe4fed8fdf108.webp 400w,
               /post/paper-deanet/d-value_hua6f719dde7ea88865cf20c8321514208_162201_c1a3be6ff77511599a10554627cb520b.webp 760w,
               /post/paper-deanet/d-value_hua6f719dde7ea88865cf20c8321514208_162201_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-deanet/d-value_hua6f719dde7ea88865cf20c8321514208_162201_399516e5f3504ecd25cfe4fed8fdf108.webp&#34;
               width=&#34;760&#34;
               height=&#34;486&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;dr-block&#34;&gt;DR-block&lt;/h3&gt;
&lt;p&gt;用于提取像素级的全局动态信息，通过计算同一空间位置上的像素在不同时间位置的差异实现。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;dr-block.png&#34; srcset=&#34;
               /post/paper-deanet/dr-block_hu8a73e9c984a76df7ad2cc21c8523d37b_136337_d803eeea17a007dfbd835715b582c537.webp 400w,
               /post/paper-deanet/dr-block_hu8a73e9c984a76df7ad2cc21c8523d37b_136337_4ce0b47c9cfc0281bf769c919916f579.webp 760w,
               /post/paper-deanet/dr-block_hu8a73e9c984a76df7ad2cc21c8523d37b_136337_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-deanet/dr-block_hu8a73e9c984a76df7ad2cc21c8523d37b_136337_d803eeea17a007dfbd835715b582c537.webp&#34;
               width=&#34;760&#34;
               height=&#34;441&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;



$$\begin{aligned}
&amp;Y_{(t,h,w,c)} = \sum\limits_{t\neq j}g(p(t,h,w,c),p(j,h,w,c))\times p(j,h,w,c)\\
&amp;g(p(t,h,w,c),p(j,h,w,c)) = softmax(|p(t,h,w,c)-p(j,h,w,c)|)
\end{aligned}$$

&lt;p&gt;其中$g(p(t,h,w,c),p(j,h,w,c))$计算相同空间位置不同时间位置的像素差异。原数据首先经过 &lt;strong&gt;D_value操作&lt;/strong&gt; 和softmax后得到了像素值的差异。原数据经过 &lt;strong&gt;Select-T操作&lt;/strong&gt; 提取到特征，按照上述操作得到的差异矩阵加权求和得到了最终的输出。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;select-t.png&#34; srcset=&#34;
               /post/paper-deanet/select-t_hu22e5152dcf2c768f10b7925a575fc72c_124354_de08dd12a53478a4be2fc019adc2cc11.webp 400w,
               /post/paper-deanet/select-t_hu22e5152dcf2c768f10b7925a575fc72c_124354_5acd116136893aa1e3791315aa8c7b45.webp 760w,
               /post/paper-deanet/select-t_hu22e5152dcf2c768f10b7925a575fc72c_124354_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-deanet/select-t_hu22e5152dcf2c768f10b7925a575fc72c_124354_de08dd12a53478a4be2fc019adc2cc11.webp&#34;
               width=&#34;760&#34;
               height=&#34;284&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;dea_net&#34;&gt;DEA_Net&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;dea-net.png&#34; srcset=&#34;
               /post/paper-deanet/dea-net_hud4dd4a2ffc460d268e39d94c54e0a287_248058_359c7886ab6b2884bf1161519670f705.webp 400w,
               /post/paper-deanet/dea-net_hud4dd4a2ffc460d268e39d94c54e0a287_248058_298f02a5862c65d643133437a03eeb41.webp 760w,
               /post/paper-deanet/dea-net_hud4dd4a2ffc460d268e39d94c54e0a287_248058_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-deanet/dea-net_hud4dd4a2ffc460d268e39d94c54e0a287_248058_359c7886ab6b2884bf1161519670f705.webp&#34;
               width=&#34;760&#34;
               height=&#34;232&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

在上述两个处理单元的基础上得到了Dynamic Enhances Authentication Network（DEA_Net）用于分类任务。&lt;/p&gt;
&lt;h2 id=&#34;结果&#34;&gt;结果&lt;/h2&gt;
&lt;p&gt;使用GRID数据集用于模型的评估和测试，使用Faceswap工具生成攻击视频。经过测试，本文提出的模型与SOTA模型相比拥有更低的FAR和HTER，取得了比较好的结果。可以证明Diff-block和DR-block结合能够有效的消除数据中的静态特征，更好的对抗换脸攻击。&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;提出了两个网络单元用于消除数据中的静态特征，提取动态特征用于识别和认证。&lt;/li&gt;
&lt;li&gt;使用了non-local neural network的结构，增大了感知域，使得浅层网络可以学习到更多的全局信息。&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;X.L. Wang, R. Girshick, A. Gupta, and K.M. He, “Non-local neural networks,” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7794-7803, 2018.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>【论文】A Discriminative Feature Learning Approach for Deep Face Recognition</title>
      <link>https://yangleisx.github.io/post/paper-center-loss/</link>
      <pubDate>Sat, 13 Jun 2020 16:41:50 +0800</pubDate>
      <guid>https://yangleisx.github.io/post/paper-center-loss/</guid>
      <description>&lt;p&gt;论文题目：A Discriminative Feature Learning Approach for Deep Face Recognition&lt;/p&gt;
&lt;p&gt;作者：Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao&lt;/p&gt;
&lt;p&gt;会议/时间：ECCV2016&lt;/p&gt;
&lt;p&gt;链接：&lt;a href=&#34;https://ydwen.github.io/papers/WenECCV16.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;原文链接&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;论文目标&#34;&gt;论文目标&lt;/h2&gt;
&lt;p&gt;之前的工作中仅仅使用Softmax loss作为模型的监督信号，学到的模型具有一定判别能力(Separable)，本文通过介绍一种新的Centor Loss作为监督信号进行学习，使模型学到更具有判别能力(Discriminative)的特征。Centor Loss可以学习到一类特征的分类中心并减小类内距离，从而更具有更强的判别能力。&lt;/p&gt;
&lt;h2 id=&#34;相关工作&#34;&gt;相关工作&lt;/h2&gt;
&lt;p&gt;传统的度量学习(Metric Learning)中，由于识别样本在训练集中出现过，使用Softmax可以起到比较好的学习效果。但是在面识别任务中，很难预先搜集到所有可能的实体的数据用于学习，因此对于泛化能力的要求更高。这就要求学习到的特征具有更小的类内距离和更大的类间距离。&lt;/p&gt;
&lt;p&gt;使用contrastive loss或者triplet loss对于pair/triplet的取样要求比较高，使用精心设计的取样方法可以在一定程度上避免，但是引入了更高的计算复杂度。&lt;/p&gt;
&lt;h2 id=&#34;本文思路解决方案&#34;&gt;本文思路/解决方案&lt;/h2&gt;
&lt;p&gt;首先定义了centor loss函数



$$\mathcal{L}_c = \frac{1}{2}\sum\limits_{i=1}^{m}||\mathbb{x}_i-\mathbb{c}_{y_i}||^2_2$$

其中 $\mathbb{c}$表示对应的类的中心。&lt;/p&gt;
&lt;p&gt;直观的想法是在每一轮学习结束后将所有同类别数据的特征求均值，但是在大规模数据库中难以实现，因此采用每一批中同类别的数据的特征求均值用于类中心的更新。即令



$$\Delta\mathbb{c}_j = \frac{\sum\limits_{i=1}^{m}\delta(y_i=j)(\mathbb{c}_j - \mathbb{x}_i)}{1+\sum\limits_{i=1}^{m}\delta(y_j = j)}$$
&lt;/p&gt;
&lt;p&gt;其中 $\delta(condition) = condition\ is\ true ? 1: 0$，同时引入一个超参数$\alpha$作为类的中心更新时的“学习率”。&lt;/p&gt;
&lt;p&gt;完整的loss function为 $$\mathcal{L} = \mathcal{L}_s + \lambda\mathcal{L}_c$$，其中$\lambda$为权重系数。&lt;/p&gt;
&lt;p&gt;网络结构如下：可以看到引入了跨层连接和Joint Supervision Signal。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;center_loss_structure.png&#34; srcset=&#34;
               /post/paper-center-loss/center_loss_structure_hu032c9a6fd3e48b30db592e6b306c7bb2_87625_d07f4d4feb465adda6692fc9405f93c9.webp 400w,
               /post/paper-center-loss/center_loss_structure_hu032c9a6fd3e48b30db592e6b306c7bb2_87625_8fba953b0e7836831ed62c465366230a.webp 760w,
               /post/paper-center-loss/center_loss_structure_hu032c9a6fd3e48b30db592e6b306c7bb2_87625_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-center-loss/center_loss_structure_hu032c9a6fd3e48b30db592e6b306c7bb2_87625_d07f4d4feb465adda6692fc9405f93c9.webp&#34;
               width=&#34;760&#34;
               height=&#34;344&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;训练结果如下：可以看到在引入了Center Loss之后，类间距离显著减小，判别能力增强。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;center_loss_1.png&#34; srcset=&#34;
               /post/paper-center-loss/center_loss_1_hu7fef7fd8df29396650abe7a6457b7598_679185_b2c897f29eae92047c20a73faa0ae883.webp 400w,
               /post/paper-center-loss/center_loss_1_hu7fef7fd8df29396650abe7a6457b7598_679185_2614cceb9bab87c696f2803179f23e2c.webp 760w,
               /post/paper-center-loss/center_loss_1_hu7fef7fd8df29396650abe7a6457b7598_679185_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-center-loss/center_loss_1_hu7fef7fd8df29396650abe7a6457b7598_679185_b2c897f29eae92047c20a73faa0ae883.webp&#34;
               width=&#34;760&#34;
               height=&#34;547&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;结果&#34;&gt;结果&lt;/h2&gt;
&lt;p&gt;可以发现 $\lambda=0$时的学习效果较差，当 $\lambda$太大时学习效果也会下降，测试得到的参数为$\lambda=0.03$。&lt;/p&gt;
&lt;p&gt;可以发现$\alpha$的取值对结果影响不大（不为零时），测试得到的参数为$\alpha=0.5$。&lt;/p&gt;
&lt;p&gt;最终实现的模型在小数据库训练，LFW达到了99.28%泛化准确率，YTF达到了94.9%的泛化准确率。&lt;/p&gt;
&lt;p&gt;在MegaFace数据库上测试的结果中，本文的模型均达到了更好的性能。&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;提出了Center Loss，使用Center Loss结合Softmax Loss实现具有判别力的模型学习。&lt;/li&gt;
&lt;li&gt;使用了跨层连接的模型结构。&lt;/li&gt;
&lt;li&gt;使用Joint Supervision Signal，合理选择权重超参数。&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
    
    <item>
      <title>【论文】Deep Learning Face Representation by Joint Identification-Verification</title>
      <link>https://yangleisx.github.io/post/paper-deepid2/</link>
      <pubDate>Sat, 06 Jun 2020 15:48:29 +0800</pubDate>
      <guid>https://yangleisx.github.io/post/paper-deepid2/</guid>
      <description>&lt;p&gt;论文题目：Deep Learning Face Representation by Joint Identification-Verification&lt;/p&gt;
&lt;p&gt;作者：Yi Sun、 Xiaogang Wang、 Xiaoou Tang&lt;/p&gt;
&lt;p&gt;时间：2014&lt;/p&gt;
&lt;p&gt;链接：&lt;a href=&#34;https://arxiv.org/pdf/1406.4773.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;原文链接&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;论文目标&#34;&gt;论文目标&lt;/h2&gt;
&lt;p&gt;在面部识别中，关键任务是提取面部特征，并增大类间距离、减小类内距离。&lt;/p&gt;
&lt;h2 id=&#34;相关工作&#34;&gt;相关工作&lt;/h2&gt;
&lt;p&gt;在之前的工作中，本文作者提出的DeepID使用softmax作为输出用于身份验证，达到了比较好的效果。其他诸如LDA、metric learning等方法使用线性模型或使用浅层网络，具有一定的局限性。&lt;/p&gt;
&lt;h2 id=&#34;本文思路解决方案&#34;&gt;本文思路/解决方案&lt;/h2&gt;
&lt;p&gt;同时使用两种supervisory signal来指导模型的学习，即同时使用identification（给定输入，判断属于哪一个类别，多分类）和verification（给定两个输入，判断是否属于同一类别，二分类），前者可以增大类间距离，后者减小类内距离。&lt;/p&gt;
&lt;p&gt;深度神经网络的结构与本文作者&lt;a href=&#34;http://www.ee.cuhk.edu.hk/~xgwang/papers/sunWTcvpr14.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;之前的工作&lt;/a&gt;基本相同，即包括四层卷积和三层最大池化，经过跨层连接得到长度为160的特征向量。&lt;/p&gt;
&lt;p&gt;使用特征网络的输出向量分别用于identification和verification，反向传播时使用超参数 $\lambda$对两个损失函数的梯度加权。在前者使用softmax得到输出并优化cross-entropy loss。在后者使用基于L1/L2正则和cosine相似的损失函数，并对比较其效果。&lt;/p&gt;
&lt;p&gt;基于cosine相似:$$Verif(f_i,f_j, y_{ij}, \theta_{ve}) = \frac{1}{2}(y_{ij}-\sigma(wd+b))^2$$，其中$\sigma$为sigmoid函数，$$d = \frac{f_i · f_j}{||f_i||_2·||f_j||_2}$$。&lt;/p&gt;
&lt;p&gt;基于L2的:



$$Verif(f_i,f_j, y_{ij}, \theta_{ij}) = \begin{cases} \frac{1}{2}||f_i - f_j||^2 &amp;if\ y_{ij}=1\\ \frac{1}{2}\max(0, m-||f_i - f_j||^2)&amp;if\ y_{ij}=-1\end{cases}$$

，其中 $y_{ij}$表示输入是否为同一类别，$m$为指定的边界，在训练过程中手动调整。&lt;/p&gt;
&lt;p&gt;训练过程中，每一张图片选择了400个patch，包括不同位置、大小、通道（RGB or 灰度），训练得到200个网络。&lt;/p&gt;
&lt;p&gt;测试Verification时，通过前后向贪婪算法选择其中效果最好的25个得到 $25*160=4000$维度的特征向量，并通过PCA压缩，便于识别和预测。在预测时分别使用L2 Norm模型和联合贝叶斯模型来得到结果。&lt;/p&gt;
&lt;h2 id=&#34;结果&#34;&gt;结果&lt;/h2&gt;
&lt;p&gt;对于超参数 $\lambda$的选择，通过测试可以发现在0.05处达到最高。当 $\lambda=0$或者 $\lambda=+\infty$时，据无法得到比较好的效果，说明identification和verification对于特征提取都具有重要作用。&lt;/p&gt;
&lt;p&gt;与DeepID中的情况相同，训练时使用的类别越多，学习效果越好。&lt;/p&gt;
&lt;p&gt;使用联合贝叶斯模型实现预测可以得到更高的准确率。同时在训练过程中，使用L2正则的损失函数效果更好。&lt;/p&gt;
&lt;p&gt;多次选择效果比较好的patch组合，使用得到的结果结合SVM进行预测，整体系统的性能达到了99.15%的识别准确率。&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;使用identification和verification结合的方式，合理选择权重超参数，使得学习效果进一步提升。&lt;/li&gt;
&lt;li&gt;在verification中测试多种损失函数，最终选择了效果最好的基于L2的模型。&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
    
    <item>
      <title>【论文】FaceNet: A Unified Embedding for Face Recognition and Clustering</title>
      <link>https://yangleisx.github.io/post/paper-facenet/</link>
      <pubDate>Sat, 06 Jun 2020 15:48:29 +0800</pubDate>
      <guid>https://yangleisx.github.io/post/paper-facenet/</guid>
      <description>&lt;p&gt;论文题目：FaceNet: A Unified Embedding for Face Recognition and Clustering&lt;/p&gt;
&lt;p&gt;作者：Florian Schroff、Dmitry Kalenichenko、James Philbin&lt;/p&gt;
&lt;p&gt;会议/时间：2015&lt;/p&gt;
&lt;p&gt;链接：&lt;a href=&#34;https://arxiv.org/pdf/1503.03832.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;原文链接&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;论文目标&#34;&gt;论文目标&lt;/h2&gt;
&lt;p&gt;学习一种直接的从面部图片到欧氏空间向量的映射，用于人脸识别、分类和聚类等工作。欧氏空间向量之间的距离表示图片内人脸的相似度。&lt;/p&gt;
&lt;h2 id=&#34;相关工作&#34;&gt;相关工作&lt;/h2&gt;
&lt;p&gt;过去的人脸识别网络中往往通过人脸识别/分类等任务训练，最终将其中的某一层提取出来用作特征向量的生成。缺点在于泛化能力可能不够强，而且往往生成的特征向量长度比较大，效率较低。多数研究中对于较长的特征向量使用PCA简化。&lt;/p&gt;
&lt;h2 id=&#34;本文思路解决方案&#34;&gt;本文思路/解决方案&lt;/h2&gt;
&lt;p&gt;通过引入triplet loss和online triplet mining method，直接学习得到特征向量，具体是对于深度神经网络的输出，经过L2Norm后得到向量，并使用Triplet loss优化。本文作者使用的两种深度网络分别来自Zeiler&amp;amp;Fergus Model和Szegedy&amp;rsquo;s Inception Model，将其视作黑箱并训练。&lt;/p&gt;
&lt;p&gt;文中假设相同的人输入的数据分布在欧氏空间内的一个超平面，当给定一组triplet（包括锚anchor、正例positive、反例negative，其中anchor和positive是同一类）时，则有$$||f(x_i^a)-f(x_i^p)||^2_2+\alpha &amp;lt; ||f(x_i^a)-f(x_i^n)||^2_2$$，即正例的距离与反例距离的差值不小于给定的边界值$\alpha$。从而得到优化目标为$$triplet\ loss = \sum\limits_{i}^{N}\left[||f(x_i^a)-f(x_i^p)||^2_2+\alpha - ||f(x_i^a)-f(x_i^n)||^2_2 \right]_+$$。&lt;/p&gt;
&lt;p&gt;在训练过程中，需要选择合适的数据计算损失函数，如果选择简单的triplet会导致收敛缓慢，训练效果较差。因此在训练过程中，对于每一个给定的数据$$ x_i^a $$，选择距离最大的正例($$ \arg\max_{x_i^p}||f(x_i^a)-f(x_i^p)||^2_2 $$)和距离最小的反例($$ \arg\min||f(x_i^a)-f(x_i^n)||^2 $$)，称为hard取样。&lt;/p&gt;
&lt;p&gt;在实际的训练过程中，获得满足要求的triplet是不可能的，两种可行的方法包括：offline方式（每次训练N步后计算距离并选择triplet）、online方法（在每一个batch中选择满足要求的数据）。选择后者时需要注意保证每一个batch中都必须含有正例和反例。&lt;/p&gt;
&lt;p&gt;当在训练开始时就选择hard采样时，会导致模型进入局部最优，因此提出了semi-hard取样，即选择那些满足$$||f(x_i^a)-f(x_i^p)||^2_2 &amp;lt; ||f(x_i^a)-f(x_i^n)||^2_2$$的反例数据组成triplet进行训练。&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;提出了triplet loss，便于直接训练一个图像到向量的映射。&lt;/li&gt;
&lt;li&gt;分析了选择triplet的方式，使用semi-hard策略解决hard策略的缺点。&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
    
    <item>
      <title>【论文】Deep Learning Face Representation from Predicting 10000 Classes</title>
      <link>https://yangleisx.github.io/post/paper-deepid/</link>
      <pubDate>Sat, 06 Jun 2020 15:42:29 +0800</pubDate>
      <guid>https://yangleisx.github.io/post/paper-deepid/</guid>
      <description>&lt;p&gt;论文题目：Deep Learning Face Representation from Predicting 10,000 Classes&lt;/p&gt;
&lt;p&gt;会议：CVPR2014&lt;/p&gt;
&lt;p&gt;作者：Yi Sun、Xiaogang Wang、 Xiaoou Tang&lt;/p&gt;
&lt;p&gt;链接：&lt;a href=&#34;http://www.ee.cuhk.edu.hk/~xgwang/papers/sunWTcvpr14.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;原文链接&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;论文目标&#34;&gt;论文目标&lt;/h2&gt;
&lt;p&gt;通过深度神经网络学习高层次的图像特征并用于身份验证。&lt;/p&gt;
&lt;h2 id=&#34;相关工作&#34;&gt;相关工作&lt;/h2&gt;
&lt;p&gt;无限制条件下的面部识别和验证是近年来的研究热点，大多面部识别算法是通过浅层模型学习到的超完备的底层特征实现。&lt;/p&gt;
&lt;h2 id=&#34;本文思路解决方案&#34;&gt;本文思路/解决方案&lt;/h2&gt;
&lt;p&gt;通过深度卷积神经网络学习得到特征表示并用于面部识别。其中的特征表示通过深度神经网络的最后一层得到，称为DeepID（Deep Hidden Identity Features）。这一特征表示被用于最后的多分类识别任务，保证了卷积神经网络可以充分的学习到每个人的特征，具有更好的泛化能力。&lt;/p&gt;
&lt;p&gt;网络结构如下
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;deepid1_model.png&#34; srcset=&#34;
               /post/paper-deepid/deepid1_model_hu93d3bc334170f91615e77843006268af_294775_1df5b1798c6fb61f3ff2b6dcf14f584e.webp 400w,
               /post/paper-deepid/deepid1_model_hu93d3bc334170f91615e77843006268af_294775_df4b39c712eeb486453e6b35e8bf41f3.webp 760w,
               /post/paper-deepid/deepid1_model_hu93d3bc334170f91615e77843006268af_294775_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-deepid/deepid1_model_hu93d3bc334170f91615e77843006268af_294775_1df5b1798c6fb61f3ff2b6dcf14f584e.webp&#34;
               width=&#34;760&#34;
               height=&#34;339&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;通过跨层连接，保证DeepID可以获取到更多的信息，学习到更高级的特征（Conv4之后的特征更加高级/抽象）。跨层连接使用公式$$y_i = \max(0, \sum\limits_{i}x_i^1\omega_{i,j}^1+\sum\limits_{i}x_i^2\omega_{i,j}^2+b_j)$$实现，其中的$\omega$为权重。最终使用softmax层作为预测输出。&lt;/p&gt;
&lt;p&gt;特征提取部分的具体工作方式：每一张照片，划分为10个位置，每个位置选取三个不同的输入规模，每个照片得到RGB和灰度图，即每一条数据得到$10\times3\times2=60$条数据（patches）作为输入，训练60个网络。对于每一个卷积网络，给定数据，将其翻转后，得到两个向量作为输出，总的输出数据量为$160&lt;em&gt;2&lt;/em&gt;60$。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;deepid1_structure.png &#34; srcset=&#34;
               /post/paper-deepid/deepid1_structure_huaaf5607d461ca699e4cb99dcca8c39c3_244382_78f740e5da82d4c2b2dc4434cf337f45.webp 400w,
               /post/paper-deepid/deepid1_structure_huaaf5607d461ca699e4cb99dcca8c39c3_244382_1b9af34e1ec99211afc863790c46d6a3.webp 760w,
               /post/paper-deepid/deepid1_structure_huaaf5607d461ca699e4cb99dcca8c39c3_244382_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-deepid/deepid1_structure_huaaf5607d461ca699e4cb99dcca8c39c3_244382_78f740e5da82d4c2b2dc4434cf337f45.webp&#34;
               width=&#34;760&#34;
               height=&#34;485&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;在面部识别的预测中，使用联合贝叶斯方法。同时也测试了使用深度神经网络进行预测。&lt;/p&gt;
&lt;h2 id=&#34;结果&#34;&gt;结果&lt;/h2&gt;
&lt;p&gt;在CelebFaces上训练，在LFW上测试，达到了SOTA的效果。&lt;/p&gt;
&lt;p&gt;测试中可以发现添加了跨层连接的模型具有更低的验证错误率和更高的预测正确率。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;deepid1_test.png&#34; srcset=&#34;
               /post/paper-deepid/deepid1_test_hu864a7d803090d322729d88c64d3b9be9_132377_a793125c338309ec80c13aee20359348.webp 400w,
               /post/paper-deepid/deepid1_test_hu864a7d803090d322729d88c64d3b9be9_132377_3652c929af912866c88c580a539749c8.webp 760w,
               /post/paper-deepid/deepid1_test_hu864a7d803090d322729d88c64d3b9be9_132377_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-deepid/deepid1_test_hu864a7d803090d322729d88c64d3b9be9_132377_a793125c338309ec80c13aee20359348.webp&#34;
               width=&#34;760&#34;
               height=&#34;387&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;使用更多的patch与仅使用一张图片作为输入相比也具有更高的识别准确率。&lt;/p&gt;
&lt;p&gt;与现有的识别算法比较，具有更高的识别准确率（97.45%）。&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;提出了跨层连接（multi-scale），可以显著提高识别准确率。&lt;/li&gt;
&lt;li&gt;使用同一张照片的多个patch作为输入，包括不同的位置，不同的大小，不同的通道（RGB、灰度），并将60个网络的输出合并作为预测的依据，可以提高识别准确率。&lt;/li&gt;
&lt;li&gt;通过增加识别的人数（多分类的输出）可以使得特征网络学习到关键的信息。&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
    
    <item>
      <title>【论文】ABS: Scanning Neural Networks for Back-doors by Artificial Brain Stimulation</title>
      <link>https://yangleisx.github.io/post/paper-abs-nn/</link>
      <pubDate>Mon, 30 Mar 2020 21:34:37 +0800</pubDate>
      <guid>https://yangleisx.github.io/post/paper-abs-nn/</guid>
      <description>&lt;p&gt;论文题目：ABS: Scanning Neural Networks for Back-doors by Artificial Brain Stimulation&lt;/p&gt;
&lt;p&gt;会议：CCS2019&lt;/p&gt;
&lt;p&gt;作者：Y. Liu .etc from Purdue University&lt;/p&gt;
&lt;p&gt;链接：&lt;a href=&#34;https://doi.org/10.1145/3319535.3363216&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;原文链接&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;论文目标现存问题&#34;&gt;论文目标/现存问题&lt;/h2&gt;
&lt;p&gt;预训练的人工智能模型可能存在一些通过训练或更改模型参数而插入的后门（AI木马）。这些模型在处理普通的输入时会得到正确的结果，但是在遇到特殊的输入数据时会出现错误的分类结果，这里的特殊输入数据通常含有称为trojan trigger的特殊模式/特征。&lt;/p&gt;
&lt;p&gt;因此文章实现了一种技术，通过为神经元引入不同级别的激励来检测其输出的变化而逆向构建出trojan trigger，从而证明AI模型有没有收到AI木马攻击。&lt;/p&gt;
&lt;h2 id=&#34;相关工作现状&#34;&gt;相关工作/现状&lt;/h2&gt;
&lt;p&gt;由于训练模型在搜集数据和计算能力方面的要求，越来越多的预训练模型开始在网络上传播，这些模型往往经由大的服务商训练，但是也存在一些经过个人训练或者再训练(retrain)的模型。就像传统软件的木马或者后门一样，预训练的模型也可能被AI木马攻击或者插入后门。&lt;/p&gt;
&lt;p&gt;Gu et al.等&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;在2017年的研究表明通过污染训练数据可以得到藏有后门的模型，即“Data Poisoning”。此时需要攻击者可以参与训练的过程，接触到数据集并对数据进行相关操作，包括添加碎片（patch）或者扰动（perturbation）。&lt;/p&gt;
&lt;p&gt;Liu et al.等&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;的研究证明劫持部分神经元并使用精心构造的数据训练也可以藏入后门，即“Neuron HiJacking”。此时攻击者得到预训练的模型，从中选择一部分关键神经元并构造可以使这些神经元产生错误输出的触发器并再次训练部分模型。&lt;/p&gt;
&lt;p&gt;通常被藏入后门的模型在处理普通的输入数据时可以得到正常的结果（甚至更高的准确率），但是在处理添加了trojan trigger的数据时会得到错误的结果。&lt;/p&gt;
&lt;p&gt;现存的一些检查AI木马的算法具有一些局限性，例如需要提供trojan trigger，造成模型性能的下降，需要大量输入数据样本，或者无法解决特征域的攻击行为。其中比较好的是Nerual Cleanse&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;，但是仍然无法解决需要样本数量大、需要对trojan trigger的先验知识、无法处理尺寸较大的trojan trigger等缺点。&lt;/p&gt;
&lt;p&gt;这里的trojan trigger可以是输入域（像素级别）或者是特征域。目前的trojan trigger包括两类，分别为基于碎片（patch based trigger）和基于扰动（perturbations based trigger）的触发器。前者为插入图片或者覆盖原图片的一小块图像，后者为在原始数据上添加特定的数据扰动。这两种都属于输入域的攻击。常见的特征域的攻击包括为图片添加滤镜等方式。&lt;/p&gt;
&lt;h2 id=&#34;本文思路解决方案&#34;&gt;本文思路/解决方案&lt;/h2&gt;
&lt;p&gt;AI木马的实质是通过改变个别神经元的权重，使得某些特别的数据或行为出现时会激活并导致错误的分类结果。&lt;/p&gt;
&lt;p&gt;本文的算法主要从EBS（Electrical Brain Stimulation）技术中得到启发，该技术向神经元提供不同强度的电刺激并观察结果，从而研究特定神经元的功能。&lt;/p&gt;
&lt;p&gt;本文实现的算法中，通过为神经元提供不同程度的激励，被攻击的神经元就会在个别标签下产生错误的结果，通过这些选定的神经元可以逆向得到trojan trigger，如果该特征/输入可以使得正常的数据得到错误的结果，可以认为该网络存在后门或已被攻击。&lt;/p&gt;
&lt;p&gt;模型要点：1. 成功的AI木马攻击必定存在受损的神经元（错误地将trojan trigger看作是目标标签的特征之一）。2. 受损神经元在特征空间中的表示是目标标签的子空间。&lt;/p&gt;
&lt;p&gt;因此在具体的实现中，给定输入数据得到每一层神经元的激活信息，当选定神经元的激活信息改变时，检查输出信息的变化情况，可以判断该神经元的状态（当其他神经元激活信息改变时输出稳定于错误的标签时即可判断）。同时通过逆向工程的思想推测trojan trigger并检查消除假阳性的神经元。对每一个标签的检查中仅需要一张正常的照片即可用于神经元状态的分析。&lt;/p&gt;
&lt;p&gt;整体的流程包括三个部分：1. 激励测试并选择候选神经元。2. 对于候选神经元检测并推断trojan trigger。3. 使用trojan trigger对其他数据进行测试。&lt;/p&gt;
&lt;h2 id=&#34;结果&#34;&gt;结果&lt;/h2&gt;
&lt;p&gt;本文的主要结果为两个算法，分别用于选择候选神经元，逆向生成trojan trigger。经过测试，本文实现的技术可以检测到绝大多数的AI木马攻击，达到了90%甚至更高的正确率。同时所采用的逆向工程技术可以近似推断得到trojan trigger。&lt;/p&gt;
&lt;p&gt;本文的技术具有同时应对输入域和特征域的攻击行为、需要较少的输入数据、对trigger尺寸不敏感、高效的的特点。&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;本文实现的方法存在如下的改进方向：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;测试过程中可能误将目标标签的特征识别为trojan trigger。应当加以区分并避免这样的情况。&lt;/li&gt;
&lt;li&gt;对于复杂的基于特征域的攻击难以有效地识别和分析。&lt;/li&gt;
&lt;li&gt;对于标签特定的攻击方式难以有效识别和分析，只能识别将所有其他的数据转移为特定标签的攻击方式。&lt;/li&gt;
&lt;li&gt;算法效率仍需提升。&lt;/li&gt;
&lt;li&gt;仅针对单个神经元进行分析，不能处理多个神经元受更改的情况。&lt;/li&gt;
&lt;li&gt;攻击模型的调整。&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;TianyuGu,BrendanDolan-Gavitt,andSiddharthGarg.2017.Badnets:Identifying vulnerabilities in the machine learning model supply chain.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu Zhang. 2018. Trojaning Attack on Neural Networks.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;BolunWang,YuanshunYao,ShawnShan,HuiyingLi,BimalViswanath,Haitao Zheng, and Ben Y Zhao. 2019. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks.&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>【论文】CloudLeak: Large-Scale Deep Learning Models Stealing Through Adversarial Examples</title>
      <link>https://yangleisx.github.io/post/paper-cloudleak/</link>
      <pubDate>Tue, 17 Mar 2020 14:57:58 +0800</pubDate>
      <guid>https://yangleisx.github.io/post/paper-cloudleak/</guid>
      <description>&lt;p&gt;论文标题：CloudLeak：Large-Scale Deep Learning Models Stealing Through Adversarial Examples&lt;/p&gt;
&lt;p&gt;会议：Network and Distributed Systems Security (NDSS) Symposium 2020&lt;/p&gt;
&lt;h2 id=&#34;论文背景和目标&#34;&gt;论文背景和目标&lt;/h2&gt;
&lt;p&gt;基于云的机器学习服务（Cloud-Based MLaaS）是指将训练好的深度神经网络部署在云上并通过应用程序接口（API）提供分类和预测任务服务。尽管MLaaS被构建成基于云的黑盒服务，攻击者仍然可以通过精心构建的对抗样本来窃取网络参数。&lt;/p&gt;
&lt;p&gt;论文中实现了一种使用对抗样本窃取网络参数的方法“FeatureFool”，并且证明其相对于其他攻击模型具有更少的请求次数。论文中通过结合几种比较新颖的算法可以达到非常好的效果。&lt;/p&gt;
&lt;p&gt;相关的概念包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;迁移学习(Transfre Learning)：识别并将在一个任务上学到的能力应用于另一个不同的任务。文章中通过结合VGG19和DeepID的迁移学习，在具有较好的性能的同时加快了模型窃取的速度。详见Y. Sun等的&lt;em&gt;Deep Learning Face Represatation from Predictiing 1000 CLasses&lt;/em&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对抗攻击(Adversarial Attack)：在初始输入上增加一个小的扰动使得模型得到错误的预测结果。包括白盒攻击和黑盒攻击。文章中使用DNN内部特征来生成对抗样本的方法，主要通过恶意的特征构建对抗样本并求解可以减小置信度的网络参数。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;模型提取攻击(Model Extraction Attack)：通过目标模型的输入和对应的输出标签/置信度来提取参数并得到一个等效的模型。文章使用迁移学习和对抗样本结合的方法提取网络模型。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;主动学习(Active Learning)：模型选择并推送关键的样本（难以分类）要求用户打上标签，从而提高训练呢和预测效率。文章通过对抗样本的构建，解决了主动学习中边界样本趋于相同的问题，&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;基于云的机器学习服务平台(Cloud-based MLaaS Platform)：指为终端用户提供提供机器学习解决方案的平台，包括模型训练、预测分析等。文章重点介绍了五个MLaaS Platform即Microsoft、Face++、IBM、Google、Clarifai。这些服务通常要求用户提供已标注的数据，由平台训练后提供API用于预测。整个过程是黑盒服务，用户不能接触到训练好模型内部的细节。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;相关工作&#34;&gt;相关工作&lt;/h2&gt;
&lt;h3 id=&#34;其他人的成果&#34;&gt;其他人的成果&lt;/h3&gt;
&lt;p&gt;一方面通过攻击可以盗取网络内部参数，称为模型提取攻击（Model Extraction Attack）。&lt;/p&gt;
&lt;p&gt;例如2016年，F. Tramèr等实现了第一个模型提取攻击方法&lt;em&gt;Stealing Machine Learning Models vis Prediction APIs&lt;/em&gt;，通过模型预测结果和输入数据来学习模型内部参数。&lt;/p&gt;
&lt;p&gt;其他的工作还有B. Wang等的&lt;em&gt;Stealing Hyperparameters in Machine Learning&lt;/em&gt;、A. Dmitrenko等的&lt;em&gt;Dnn Model Extraction Attacks Using Prediction Interfaces&lt;/em&gt;、Y. Shi等的&lt;em&gt;How to Steal a Machine Learning Classifier with Deep Learning&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;另一方面通过攻击可以盗取网络的训练集，称为成员推理攻击（Membership Inference Attack）。&lt;/p&gt;
&lt;p&gt;2017年，R. Shokri等实现了一种成员推理攻击方法&lt;em&gt;Membership Inference Attacks against Machine Learning Models&lt;/em&gt;，可以判断目标模型的训练集是否包含某一条特定的数据。&lt;/p&gt;
&lt;p&gt;其他的工作还有Y. Long等的&lt;em&gt;Understanding Membership Inference on Well-generalized Learning Models&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;存在一些应对模型提取攻击的防御方案，大多都是在预测请求过程(query process)中加以处理，但是难以同时满足性能和效率的要求。例如M. Juuti等的&lt;em&gt;Prada: Protecting against DNN Model Stealing Attacks&lt;/em&gt;。&lt;/p&gt;
&lt;h3 id=&#34;局限性&#34;&gt;局限性&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;目前大多数模型攻击方法只能应用于比较简单的小规模机器学习模型中。&lt;/li&gt;
&lt;li&gt;目前的攻击方法的请求规模与模型参数成比例。在攻击含有百万量级参数的大型网络时效率低下。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;本文解决方案&#34;&gt;本文解决方案&lt;/h2&gt;
&lt;h3 id=&#34;模型假设&#34;&gt;模型假设&lt;/h3&gt;
&lt;p&gt;攻击者对网络模型的结构、参数、训练集没有先验的信息。但是可以使用任意输入并得到对应的预测结果。&lt;/p&gt;
&lt;h3 id=&#34;模型思路&#34;&gt;模型思路&lt;/h3&gt;
&lt;p&gt;通过使用对抗样本请求模型预测并得到结果，使用输入输出对（input-output pair）训练从候选库中选择出来的替代模型，从而实现对目标模型参数的窃取。实际上，通过使用预测结果可以大量减少标记成本并得到训练替代模型的数据集。基本流程为：1.生成对抗样本得到合成数据集。2.请求目标模型得到输出结果。3.根据模型输出标记对抗样本。4.使用合成数据集训练替代模型。&lt;/p&gt;
&lt;h3 id=&#34;符号定义&#34;&gt;符号定义&lt;/h3&gt;
&lt;p&gt;目标网络$f_v(x)$的输入为$x$，输出 $[f_v^1(x),f_v^2(x)&amp;hellip;]$，学习目标为在构造的数据集 $T={(x, f_v(x))}$上训练一个替代模型（网络）$f_s(x)$使得两者具由基本相同的功能。攻击者对于目标网络的结构$A$、参数$W$、训练集$D$没有任何先验的信息。&lt;/p&gt;
&lt;h3 id=&#34;模型实现&#34;&gt;模型实现&lt;/h3&gt;
&lt;h4 id=&#34;数据集生成&#34;&gt;数据集生成&lt;/h4&gt;
&lt;p&gt;基于边界的对抗性主动学习（Margin-based Adversarial AL）：从未标记的样本中选择一组“有用的”样本。即从生成的未标记样本中选择预测置信度最低、预测不确定性最大的样本



$$\mathit{Q}_{multiclass}^{LC}:\mathbb{x}_s^* \in arg\min\limits_{x&#39;\in D_u(x)}k(x&#39;,y,w)$$

注意这里得到的样本太多，预测时需要大量的请求次数，因此采用进一步采样得到更小的数据集。&lt;/p&gt;
&lt;p&gt;这里的采样方式包括：随机采样（Random Sample，RS）、投影梯度下降（Projected Gradient Descent）、Carlini &amp;amp; Wagner 攻击（Carlini and Wagner Attack，CW）、Feature Adversary（FA）、FeatureFool（FF， 本文提出的算法）。&lt;/p&gt;
&lt;p&gt;在使用上述FF算法求解时使用L-BFGS算法进行优化求解并使用平均测量误差（Average Test Error，ATE）方法衡量数据集质量。其优化流程主要为：1.确定原图像和目标图像。2.原图像加扰动后加入特征提取网络。3.选择网络中指定特征层的输出得到显著图。4.比较显著图与目标图像的显著图并加以优化。&lt;/p&gt;
&lt;h4 id=&#34;替代模型训练&#34;&gt;替代模型训练&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;随机选择一组初始化数据集，并使用上述各种算法得到一组小规模的对抗样本。这些样本可以很容易地使本地的替代模型得到错误的预测结果。&lt;/li&gt;
&lt;li&gt;使用对抗样本请求预测并得到构造数据集。使用几种训练好的模型（AlexNet、VGG19、VGGFace、ResNet50）构建迁移学习框架，使用构造数据集训练并更新上述框架的最后几层全联接的参数。&lt;/li&gt;
&lt;li&gt;重复使用本地模型构建对抗样本并请求预测，使用预测结果构建数据集进行训练的过程。最终得到性能足够逼近目标模型的替代模型。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;结果&#34;&gt;结果&lt;/h2&gt;
&lt;p&gt;作者选择了三个平台的四种模型进行了攻击演示。经测试，使用文章设计实现的FF算法在多数目标平台上可以实现比较好的效果。而且与现存的三种模型提取攻击相比，具有更高的准确度和相同准确率下更少的请求次数，从而具有更少的学习成本。&lt;/p&gt;
&lt;p&gt;使用现有的PRADA检测机制可以发现，文章实现的算法可以很好的逃避PRADA机制的检测，即相比其他的攻击方式，要想检测到本文实现的攻击需要更多的请求次数。文章推荐的检测方式为通过提取网络每一层的特征数据来区分恶意数据和合法数据的特征分布。&lt;/p&gt;
&lt;h2 id=&#34;总结分析-局限性&#34;&gt;总结分析-局限性&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;攻击请求方式的改进&lt;/strong&gt;：文章实现的方法中，通过对正常数据添加扰动构建攻击样本，造成了数据污染，从而降低了替代模型的预测准确率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;多标签任务的扩展&lt;/strong&gt;：文章实现的方法仅能用于单标签的多分类任务，因此需要在数据生成和替代样本结构选择上更加注意。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;其他学习领域的扩展&lt;/strong&gt;：文章实现的方法仅能用于图像分类任务，因此在处理音频或文字时需要对模型做相应的调整。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>【论文】Attention is All You Need</title>
      <link>https://yangleisx.github.io/post/paper-transformer/</link>
      <pubDate>Tue, 04 Feb 2020 16:40:41 +0800</pubDate>
      <guid>https://yangleisx.github.io/post/paper-transformer/</guid>
      <description>&lt;p&gt;论文题目: Attention is All You Need&lt;/p&gt;
&lt;p&gt;开源项目地址:&lt;a href=&#34;https://github.com/tensorflow/tensor2tensor&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;主要内容&#34;&gt;主要内容&lt;/h2&gt;
&lt;p&gt;文章实现了一种仅使用attention机制，完全去除卷积网络和循环网络的结构，称为Transformer网络。&lt;/p&gt;
&lt;p&gt;在实现比较高的准确性的基础上，具有较低的计算成本。&lt;/p&gt;
&lt;h2 id=&#34;背景&#34;&gt;背景&lt;/h2&gt;
&lt;p&gt;目前常用的序列处理模型通常使用一个编码器和一个解码器，在两者的连接中使用attention机制，而且编码器和解码器的实现使用复杂的CNN和RNN。&lt;/p&gt;
&lt;p&gt;由于RNN结构引入的时序逻辑不利于并行计算，引入比较高的计算成本。因此Transformer结构中除去了RNN而仅使用attention结构来学习序列前后的相关性。&lt;/p&gt;
&lt;p&gt;在transformer中仅使用self-attention机制来学习序列中不同元素之间的关系。&lt;/p&gt;
&lt;h2 id=&#34;实现&#34;&gt;实现&lt;/h2&gt;
&lt;p&gt;使用self-attention和full connect实现。&lt;/p&gt;
&lt;p&gt;编码器的实现中，使用若干个块，每个块包括一个multi-head attention层和一个全联接的feed foward层，其中每一层都引入了残差连接和正规化，构成$LayerNorm(x + Sublayer(x))$的结构。&lt;/p&gt;
&lt;p&gt;解码器的实现中，同样使用若干个块，每个块为编码器的基本块之前加上一个masked multi-head attention结构。同时将上一时刻的输出作为下一个时刻的输入，使得输出仅由之前的输入决定。&lt;/p&gt;
&lt;p&gt;Scaled Dot-Product Attention的结构为将Q(uery)、K(ey)、V(alue)映射得到输出。输入为$d_k$维的Key向量和$d_v$维的Value向量，计算方法为矩阵相乘的结果缩放后再Softmax得到Value元素对应的权重，即$Attention(Q,K,V) = Softmax(\frac{QK^T}{\sqrt{d_k}})V$。使用矩阵乘法的实现计算更快。&lt;/p&gt;
&lt;p&gt;Multi-head Attnetion为多个Attention层组合得到。通过将V、K、Q线性投影到h个attention网络，将其结果通过concat组合之后再线性投影得到结果，即$MultiHead(Q,K,V) = Concat(head_1,hed_2&amp;hellip;)W^O$，其中$head_i = Attention(QW^Q_i, KW^K_i, VW^V_i)$。&lt;/p&gt;
&lt;p&gt;注意解码器的Q来自解码器上一输出经过Masked Multi-Head Attention的结果，K、V来自编码器的输出。而编码器的Q、K、V来自上一层的输出。&lt;/p&gt;
&lt;p&gt;在Masked Multi-Head Attention中，SoftMax的输出被屏蔽（设置为$-\infty$）。&lt;/p&gt;
&lt;p&gt;在Attention之后的Feed Forward网络中，使用两次ReLU得到$FFN(x) = max(0, xW_1+b_1)W_2+b_2$。&lt;/p&gt;
&lt;p&gt;对于输入，还需要进行embedding和positional encoding操作。使用learned embeddings将输入转化为向量。后者引入顺序和位置信息，使用$PE_{(pos,2i)} = sin(pos/10000^{2i/d_{model}})$和$PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{model}})$，其中pos为位置，i为维数。&lt;/p&gt;
&lt;p&gt;对于输出，使用线性转换（learned linear transformation）和softmax将输出转变为概率。这里的线性转换和输入的embeddings使用相同的权重（embedding中乘$\sqrt{d_{model}}$,即向量规模的平方根）。&lt;/p&gt;
&lt;h2 id=&#34;评价&#34;&gt;评价&lt;/h2&gt;
&lt;p&gt;使用attention相比直接使用卷积或循环，计算复杂度比较低，同时更有利于并行计算。&lt;/p&gt;
&lt;p&gt;同时attention结构更有助于学习长句子，能学习到距离较远的信息。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>【论文】Deep Residual Learning for Image Recognition</title>
      <link>https://yangleisx.github.io/post/paper-resnet/</link>
      <pubDate>Mon, 03 Feb 2020 17:46:14 +0800</pubDate>
      <guid>https://yangleisx.github.io/post/paper-resnet/</guid>
      <description>&lt;p&gt;Deep Residual Learning for Image Recognition&lt;/p&gt;
&lt;h2 id=&#34;主要内容&#34;&gt;主要内容&lt;/h2&gt;
&lt;p&gt;构建了一种残差网络结构，有利于深度神经网络的训练，同时证明了该残差网络结构更容易优化并且在深度增加时仍能实现较高的准确率。同时训练了一个152层的网络用语图像识别。&lt;/p&gt;
&lt;h2 id=&#34;背景&#34;&gt;背景&lt;/h2&gt;
&lt;p&gt;传统的用于图像识别的DCNN通过增加深度可以提高识别性能。但是会出现梯度爆炸/消失的现象（vanishing/exploding gradient）。&lt;/p&gt;
&lt;p&gt;通过引入正则化等方法可以使深度网络实现收敛（SGD）。尽管可以收敛，但深度增加时会出现性能下降（degradation）。&lt;/p&gt;
&lt;p&gt;因此文中实现了一种残差网络deep residual learning，用来解决degradation问题。&lt;/p&gt;
&lt;p&gt;主要方式为：要学习得到H(x)，构造F(x)=H(x)-x并学习，最终得到F(x)+x即为所学习的目标。这里的+x可以使用短接（shortcut connection：跳过若干层的连接）来实现。这样的操作相当于恒等映射，没有引入新的参数或计算复杂度。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;残差网络模块&#34; srcset=&#34;
               /post/paper-resnet/resnet-block_huf1a93f8b83f0e5e2a0925eab0a837dff_35398_5a1ec9eebc7fac70b8d38d2a190c7f40.webp 400w,
               /post/paper-resnet/resnet-block_huf1a93f8b83f0e5e2a0925eab0a837dff_35398_c4597870b69958d71e12f64c296c880d.webp 760w,
               /post/paper-resnet/resnet-block_huf1a93f8b83f0e5e2a0925eab0a837dff_35398_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-resnet/resnet-block_huf1a93f8b83f0e5e2a0925eab0a837dff_35398_5a1ec9eebc7fac70b8d38d2a190c7f40.webp&#34;
               width=&#34;692&#34;
               height=&#34;322&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;实现&#34;&gt;实现&lt;/h2&gt;
&lt;p&gt;通过使用短接可以使得非线性的网络更好的拟合恒等映射，即令网络参数为0。&lt;/p&gt;
&lt;p&gt;因此构造的网络形如 $y = F(x,{W_i})+x$ 。使用两层网络和ReLU构造网络块得到 $F = W_2\sigma(W_1x)$，其中 $\sigma$表示ReLU，最终的输出为 $\sigma{y}$。可以添加一个矩阵 $W_s$ 用于将F的输出和x的规模对齐。&lt;/p&gt;
&lt;p&gt;这里F的结构可以使用两层或三层网络，但是不能只有一层（实际上构成一个线性单元）。同时每一层的结构可以是全联接层也可以是卷积层。&lt;/p&gt;
&lt;p&gt;类比VGG-19网络，构造一个34层的深度卷积神经网络，以及其对应的深度残差网络。其中后河通过在前者的网络结构中每隔两层添加一个短路连接得到。&lt;/p&gt;
&lt;h2 id=&#34;评价&#34;&gt;评价&lt;/h2&gt;
&lt;p&gt;相对于普通的深度网络，深度残差网络更容易训练。&lt;/p&gt;
&lt;p&gt;相对于相同深度的网络，深度残差网络可以得到更高的准确度。&lt;/p&gt;
&lt;p&gt;当网络深度增加时，使用深度残差网络可以缓减错误率上升的情况。&lt;/p&gt;
&lt;p&gt;短路连接使用不同的方式（直接映射/规模不同时使用矩阵/全部使用矩阵）可以带来轻微的性能提升，但直接映射的复杂度比较低。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>【论文】Effective Approaches to Attention-based Neural Machine Translation</title>
      <link>https://yangleisx.github.io/post/paper-attention/</link>
      <pubDate>Sun, 02 Feb 2020 22:29:36 +0800</pubDate>
      <guid>https://yangleisx.github.io/post/paper-attention/</guid>
      <description>&lt;p&gt;论文笔记:Effective Approaches to Attention-based Neural Machine Translation&lt;/p&gt;
&lt;p&gt;开源项目地址：&lt;a href=&#34;http://nlp.stanford.edu/projects/nmt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;stanford.edu&lt;/a&gt;或者&lt;a href=&#34;https://github.com/tensorflow/nmt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;主要内容&#34;&gt;主要内容&lt;/h2&gt;
&lt;p&gt;NMT使用RNN的架构来实现序列到序列的学习。Attention机制被广泛用于训练神经网络并提升其性能。&lt;/p&gt;
&lt;p&gt;因此可以使用attention机制通过聚焦于输入序列的某一部分来提高NMT的性能。&lt;/p&gt;
&lt;p&gt;文中构建了两种使用attention的NMT模型：global模型和locol模型。前者注意输入序列全体，后者每次针对输入序列的一个子集。两种模型的结构都很简单，而且后者的计算成本更低。&lt;/p&gt;
&lt;h2 id=&#34;背景&#34;&gt;背景&lt;/h2&gt;
&lt;p&gt;传统NMT模型包括一个编码器和一个解码器，前者将输入映射到固定长度的向量$\mathbf{S}$，后者将向量映射到输出序列。&lt;/p&gt;
&lt;p&gt;其具体实现有CNN-RNN结构，也有RNN-RNN结构（包括LSTM-LSTM，GRU-GRU等）。&lt;/p&gt;
&lt;p&gt;其他的实现中使用$\mathbf{S}$作为初始化解码器的工具，而使用attention机制的模型在整个翻译过程中都将$\mathbf{S}$用做参考。&lt;/p&gt;
&lt;h2 id=&#34;实现&#34;&gt;实现&lt;/h2&gt;
&lt;p&gt;使用深度LSTM作为编码器和解码器。&lt;/p&gt;
&lt;p&gt;目标函数为$J = \sum _{(x,y)\in \mathbb{D}}-logp(y|x)$其中$\mathbb{D}$为训练数据。&lt;/p&gt;
&lt;p&gt;globel和locol两种不同的模型区别在于内容向量$c_t$的生成方式不同。当内容向量$c_t$生成后，即可计算输出。&lt;/p&gt;
&lt;p&gt;即首先计算attentional hidden state:$\tilde{h}_t = tanh(W_c [c_t;h_t])$。&lt;/p&gt;
&lt;p&gt;然后得到预测向量 $p(y_t|y_{&amp;lt; t}, x) = softmax(W_s \tilde{h}_t)$ 。&lt;/p&gt;
&lt;h3 id=&#34;global-attention&#34;&gt;global attention&lt;/h3&gt;
&lt;p&gt;考虑到编码器的所有隐含状态，通过编码器状态$\bar{h}_s$和解码器状态$h_t$通过score()函数得到global align weights即$a_t$，这里的向量$a_t$为可变长度的。最终使用$a_t$对$\bar{h}_s$加权得到$c_t$从而得到$\hat{h}_t$.&lt;/p&gt;
&lt;p&gt;具体score函数见文献原文。&lt;/p&gt;
&lt;p&gt;global attention考虑到所有的输入符号，计算成本比较高，而且无法处理较长的序列。&lt;/p&gt;
&lt;h3 id=&#34;locol-attention&#34;&gt;locol attention&lt;/h3&gt;
&lt;p&gt;借鉴soft和hard attention的思想&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，使用加窗的方式，考虑一部分的隐状态。&lt;/p&gt;
&lt;p&gt;首先确定位置$p_t$，然后根据位于$[p_t-D,p_t+D]$的编码器状态使用相同的score()函数得到$a_t$。不同的是这里的$a_t$为固定长度($2D+1$)的向量。&lt;/p&gt;
&lt;p&gt;关于位置$p_t$的确定有不同的方法。&lt;/p&gt;
&lt;p&gt;locol-m方法：令$p_t = t$,使用前述方法计算$a_t$。&lt;/p&gt;
&lt;p&gt;locol-p方法：通过学习到的参数预测，即令$p_t = S·sigmoid(v_p^Ttanh(W_p h_t))$，其中 $v_p$ 和 $W_p$为学习得到的参数，S为输入序列长度。同时计算$a_t = align(h_t,\bar{h}_s) exp(- \frac{(s-p_t)^2}{2\sigma^2})$，其中$\sigma = \frac{D}{2}$。&lt;/p&gt;
&lt;h3 id=&#34;input-feeding&#34;&gt;input-feeding&lt;/h3&gt;
&lt;p&gt;将上一输出作为输入传入网络用于产生下一输出。&lt;/p&gt;
&lt;h3 id=&#34;具体实现&#34;&gt;具体实现&lt;/h3&gt;
&lt;p&gt;使用WMT的英语-德语双向翻译任务，共4.5M句子，包括116M英语单词110M德语单词。&lt;/p&gt;
&lt;p&gt;选择其中的50k单词作为训练的单词表。每个LSTM具有四层，每层1000单元，每个单元为1000维张量。&lt;/p&gt;
&lt;h2 id=&#34;评价&#34;&gt;评价&lt;/h2&gt;
&lt;p&gt;global attention和local attention相比其他人的实现&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;更加简单&lt;/p&gt;
&lt;p&gt;通过input-feeding使得之前学习得到的结果得到充分利用，这一方法也可以用于非attention的其他RNN架构的网络中。&lt;/p&gt;
&lt;p&gt;最终训练结果在WMT上得分23.0超过了最好的系统。&lt;/p&gt;
&lt;p&gt;论文实现的系统具有更低的test cost。可以更好的处理长句子。&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;http://proceedings.mlr.press/v37/xuc15.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Show,Attend and Tell:Neural Image Caption Generation with Visual Attention&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1409.0473&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Neural Machine Translation by Jointly Learning to Align and Translate&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>【论文】Sequence to Sequence Learning with Neural Networks</title>
      <link>https://yangleisx.github.io/post/paper-seq2seq/</link>
      <pubDate>Sun, 02 Feb 2020 22:26:13 +0800</pubDate>
      <guid>https://yangleisx.github.io/post/paper-seq2seq/</guid>
      <description>&lt;p&gt;论文笔记:Sequence to Sequence Learning with Neural Networks&lt;/p&gt;
&lt;h2 id=&#34;主要内容&#34;&gt;主要内容&lt;/h2&gt;
&lt;p&gt;使用一个深度LSTM网络将输入序列映射到固定长度的向量，再使用另一个深度LSTM网络将该向量映射（decode）到目标序列。&lt;/p&gt;
&lt;h2 id=&#34;背景&#34;&gt;背景&lt;/h2&gt;
&lt;p&gt;DNN在语音识别和视觉识别领域有非常好的效果，但是仅能用于输入和输出可以化为（encode）相同的固定规模（known and fixed）的向量。很多实际问题中输入和输出需要用未知长度的向量表示。&lt;/p&gt;
&lt;p&gt;LSTM可以在具有长期时间相关性（long range temporal dependencies）的数据中具有比较好的学习效果，因此用于在输入序列和相应的输出序列之间进行学习。&lt;/p&gt;
&lt;p&gt;同时通过在训练中反转输入序列引入短期依赖便于训练和优化。&lt;/p&gt;
&lt;h2 id=&#34;实现方式&#34;&gt;实现方式&lt;/h2&gt;
&lt;h3 id=&#34;原理&#34;&gt;原理&lt;/h3&gt;
&lt;p&gt;使用单个RNN由给定输入序列$(x_1,x_2,&amp;hellip;,x_t)$计算输出序列$(y_1,y_2,&amp;hellip;,y_t)$的方法如下：&lt;/p&gt;
&lt;p&gt;$h_t = sigm(W^{hx}x_t + W^{hh}h_{t-1})$、$y_t = W^{yh}h_t$。但是并不能用于不同长度的输入和输出序列。&lt;/p&gt;
&lt;p&gt;因此使用两个LSTM取代两个RNN来实现长期时间相关性的学习。&lt;/p&gt;
&lt;p&gt;在LSTM中使用条件概率的计算方法如下：&lt;/p&gt;
&lt;p&gt;$p(y_1,&amp;hellip;,y_{T&amp;rsquo;}|x_1,&amp;hellip;,x_T) = \prod_{t=1}^{T&amp;rsquo;}p(y_t|v,y_1,&amp;hellip;,y_{t-1})$，其中v是输入序列的一种表示。&lt;/p&gt;
&lt;p&gt;具体实现中，==使用两个深度LSTM网络，每个网络具有4层，用于训练的输入序列反转。==&lt;/p&gt;
&lt;p&gt;每个LSTM通过最大化对数几率进行训练，目标函数为$\frac{1}{|S|}\sum_{(T,S)\in \mathbb{S}} logp(T|S)$，其中$\mathbb{S}$为训练集。&lt;/p&gt;
&lt;p&gt;预测时依据$\hat{T} = arg max_{\mathbf{T}} p(T|S)$，使用自左向右的Beam Search算法&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;得到翻译结果。&lt;/p&gt;
&lt;p&gt;输入序列反转后，训练的性能提高（原文使用“最小时滞 minimal time lag”加以解释：输入序列和输出序列的开头几个单词的距离减小）。可以认为输出序列的前半部分正确率较高而后半部分表现比较差。&lt;/p&gt;
&lt;h3 id=&#34;实现&#34;&gt;实现&lt;/h3&gt;
&lt;p&gt;使用WMT&#39;14 英语-法语数据库进行训练。&lt;/p&gt;
&lt;p&gt;每个LSTM网络有4层，每层1000个单元，每个数据使用1000维张量表示。&lt;/p&gt;
&lt;p&gt;输入词汇量160000，输出词汇量80000。输出时使用8000输入的softmax。共有384M参数。&lt;/p&gt;
&lt;p&gt;为了防止梯度爆炸，每一个batch训练结束后对梯度正规化。&lt;/p&gt;
&lt;h2 id=&#34;评价&#34;&gt;评价&lt;/h2&gt;
&lt;h3 id=&#34;结果&#34;&gt;结果&lt;/h3&gt;
&lt;p&gt;训练结果使用BLEU打分，在WMT‘14数据库上得到37.0分&lt;/p&gt;
&lt;h3 id=&#34;结果评价&#34;&gt;结果评价&lt;/h3&gt;
&lt;p&gt;使用深度LSTM网络实现的sequence到sequence模型在机器翻译问题上取得了不错的效果。&lt;/p&gt;
&lt;p&gt;其在较长的句子上性能比较好。并且可以处理主动语态和被动语态。&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.jianshu.com/p/c7aab93b944d&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Beam Search&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
    </item>
    
  </channel>
</rss>
