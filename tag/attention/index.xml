<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Attention | YangLeiSX</title>
    <link>https://yangleisx.github.io/tag/attention/</link>
      <atom:link href="https://yangleisx.github.io/tag/attention/index.xml" rel="self" type="application/rss+xml" />
    <description>Attention</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Sun, 02 Feb 2020 22:29:36 +0800</lastBuildDate>
    <image>
      <url>https://yangleisx.github.io/media/icon_hu9d67daea6e408fd17d2331b8d809e90a_61652_512x512_fill_lanczos_center_3.png</url>
      <title>Attention</title>
      <link>https://yangleisx.github.io/tag/attention/</link>
    </image>
    
    <item>
      <title>【论文】Effective Approaches to Attention-based Neural Machine Translation</title>
      <link>https://yangleisx.github.io/post/paper-attention/</link>
      <pubDate>Sun, 02 Feb 2020 22:29:36 +0800</pubDate>
      <guid>https://yangleisx.github.io/post/paper-attention/</guid>
      <description>&lt;p&gt;论文笔记:Effective Approaches to Attention-based Neural Machine Translation&lt;/p&gt;
&lt;p&gt;开源项目地址：&lt;a href=&#34;http://nlp.stanford.edu/projects/nmt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;stanford.edu&lt;/a&gt;或者&lt;a href=&#34;https://github.com/tensorflow/nmt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;主要内容&#34;&gt;主要内容&lt;/h2&gt;
&lt;p&gt;NMT使用RNN的架构来实现序列到序列的学习。Attention机制被广泛用于训练神经网络并提升其性能。&lt;/p&gt;
&lt;p&gt;因此可以使用attention机制通过聚焦于输入序列的某一部分来提高NMT的性能。&lt;/p&gt;
&lt;p&gt;文中构建了两种使用attention的NMT模型：global模型和locol模型。前者注意输入序列全体，后者每次针对输入序列的一个子集。两种模型的结构都很简单，而且后者的计算成本更低。&lt;/p&gt;
&lt;h2 id=&#34;背景&#34;&gt;背景&lt;/h2&gt;
&lt;p&gt;传统NMT模型包括一个编码器和一个解码器，前者将输入映射到固定长度的向量$\mathbf{S}$，后者将向量映射到输出序列。&lt;/p&gt;
&lt;p&gt;其具体实现有CNN-RNN结构，也有RNN-RNN结构（包括LSTM-LSTM，GRU-GRU等）。&lt;/p&gt;
&lt;p&gt;其他的实现中使用$\mathbf{S}$作为初始化解码器的工具，而使用attention机制的模型在整个翻译过程中都将$\mathbf{S}$用做参考。&lt;/p&gt;
&lt;h2 id=&#34;实现&#34;&gt;实现&lt;/h2&gt;
&lt;p&gt;使用深度LSTM作为编码器和解码器。&lt;/p&gt;
&lt;p&gt;目标函数为$J = \sum _{(x,y)\in \mathbb{D}}-logp(y|x)$其中$\mathbb{D}$为训练数据。&lt;/p&gt;
&lt;p&gt;globel和locol两种不同的模型区别在于内容向量$c_t$的生成方式不同。当内容向量$c_t$生成后，即可计算输出。&lt;/p&gt;
&lt;p&gt;即首先计算attentional hidden state:$\tilde{h}_t = tanh(W_c [c_t;h_t])$。&lt;/p&gt;
&lt;p&gt;然后得到预测向量 $p(y_t|y_{&amp;lt; t}, x) = softmax(W_s \tilde{h}_t)$ 。&lt;/p&gt;
&lt;h3 id=&#34;global-attention&#34;&gt;global attention&lt;/h3&gt;
&lt;p&gt;考虑到编码器的所有隐含状态，通过编码器状态$\bar{h}_s$和解码器状态$h_t$通过score()函数得到global align weights即$a_t$，这里的向量$a_t$为可变长度的。最终使用$a_t$对$\bar{h}_s$加权得到$c_t$从而得到$\hat{h}_t$.&lt;/p&gt;
&lt;p&gt;具体score函数见文献原文。&lt;/p&gt;
&lt;p&gt;global attention考虑到所有的输入符号，计算成本比较高，而且无法处理较长的序列。&lt;/p&gt;
&lt;h3 id=&#34;locol-attention&#34;&gt;locol attention&lt;/h3&gt;
&lt;p&gt;借鉴soft和hard attention的思想&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，使用加窗的方式，考虑一部分的隐状态。&lt;/p&gt;
&lt;p&gt;首先确定位置$p_t$，然后根据位于$[p_t-D,p_t+D]$的编码器状态使用相同的score()函数得到$a_t$。不同的是这里的$a_t$为固定长度($2D+1$)的向量。&lt;/p&gt;
&lt;p&gt;关于位置$p_t$的确定有不同的方法。&lt;/p&gt;
&lt;p&gt;locol-m方法：令$p_t = t$,使用前述方法计算$a_t$。&lt;/p&gt;
&lt;p&gt;locol-p方法：通过学习到的参数预测，即令$p_t = S·sigmoid(v_p^Ttanh(W_p h_t))$，其中 $v_p$ 和 $W_p$为学习得到的参数，S为输入序列长度。同时计算$a_t = align(h_t,\bar{h}_s) exp(- \frac{(s-p_t)^2}{2\sigma^2})$，其中$\sigma = \frac{D}{2}$。&lt;/p&gt;
&lt;h3 id=&#34;input-feeding&#34;&gt;input-feeding&lt;/h3&gt;
&lt;p&gt;将上一输出作为输入传入网络用于产生下一输出。&lt;/p&gt;
&lt;h3 id=&#34;具体实现&#34;&gt;具体实现&lt;/h3&gt;
&lt;p&gt;使用WMT的英语-德语双向翻译任务，共4.5M句子，包括116M英语单词110M德语单词。&lt;/p&gt;
&lt;p&gt;选择其中的50k单词作为训练的单词表。每个LSTM具有四层，每层1000单元，每个单元为1000维张量。&lt;/p&gt;
&lt;h2 id=&#34;评价&#34;&gt;评价&lt;/h2&gt;
&lt;p&gt;global attention和local attention相比其他人的实现&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;更加简单&lt;/p&gt;
&lt;p&gt;通过input-feeding使得之前学习得到的结果得到充分利用，这一方法也可以用于非attention的其他RNN架构的网络中。&lt;/p&gt;
&lt;p&gt;最终训练结果在WMT上得分23.0超过了最好的系统。&lt;/p&gt;
&lt;p&gt;论文实现的系统具有更低的test cost。可以更好的处理长句子。&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;http://proceedings.mlr.press/v37/xuc15.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Show,Attend and Tell:Neural Image Caption Generation with Visual Attention&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1409.0473&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Neural Machine Translation by Jointly Learning to Align and Translate&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
    </item>
    
  </channel>
</rss>
