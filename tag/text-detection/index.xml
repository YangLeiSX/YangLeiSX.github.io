<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Text Detection | YangLeiSX</title>
    <link>https://yangleisx.github.io/tag/text-detection/</link>
      <atom:link href="https://yangleisx.github.io/tag/text-detection/index.xml" rel="self" type="application/rss+xml" />
    <description>Text Detection</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Fri, 15 Oct 2021 16:19:26 +0800</lastBuildDate>
    <image>
      <url>https://yangleisx.github.io/media/icon_hu9d67daea6e408fd17d2331b8d809e90a_61652_512x512_fill_lanczos_center_3.png</url>
      <title>Text Detection</title>
      <link>https://yangleisx.github.io/tag/text-detection/</link>
    </image>
    
    <item>
      <title>【论文】Look More Than Once: An Accurate Detector for Text of Arbitrary Shapes</title>
      <link>https://yangleisx.github.io/post/paper-lomo/</link>
      <pubDate>Fri, 15 Oct 2021 16:19:26 +0800</pubDate>
      <guid>https://yangleisx.github.io/post/paper-lomo/</guid>
      <description>&lt;p&gt;论文题目：Look More Than Once: An Accurate Detector for Text of Arbitrary Shapes&lt;/p&gt;
&lt;p&gt;作者：Chengquan Zhang， Borong Liang， Zuming Huang， Mengyi En，Junyu Han，Errui Ding， Xinghao Ding&lt;/p&gt;
&lt;p&gt;会议/时间：CVPR2019&lt;/p&gt;
&lt;p&gt;链接: &lt;a href=&#34;https://ieeexplore.ieee.org/document/8953775&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ieeexplore&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;论文目标&#34;&gt;论文目标&lt;/h2&gt;
&lt;p&gt;现有模型受到感受野的限制，很难实现对于长文本和弯曲文本的准确识别。论文中设计实现了一个LOMO网络，首先通过DR获得粗检测结果，再使用IRM迭代优化结果，最后使用SEM得到任意多边形的文本检测结果。&lt;/p&gt;
&lt;h2 id=&#34;相关工作&#34;&gt;相关工作&lt;/h2&gt;
&lt;p&gt;目前的文本检测有三种思路：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;基于Component，检测到文本部分然后通过后处理合并。例如CTPN、SegLink、WordSup等。&lt;/li&gt;
&lt;li&gt;基于Detection，类似通用的目标检测。例如TextBoxes、RRD、RRPN、EAST等。&lt;/li&gt;
&lt;li&gt;基于Segmentation，类似语义分割。例如TextSnake、PSENet等。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;本文方法&#34;&gt;本文方法&lt;/h2&gt;
&lt;p&gt;整体的模型结构如下。基本的特征提取部分使用ResNet50和FPN实现。最终得到1/4大小的128通道的特征图。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;lomo_structure&#34; srcset=&#34;
               /post/paper-lomo/lomo_structure_hue5e8f1c01a26b4b030fc1d74546bd49e_76805_90fa04cb99324adf5342fc0c2d8bb44f.webp 400w,
               /post/paper-lomo/lomo_structure_hue5e8f1c01a26b4b030fc1d74546bd49e_76805_8154e8a1b9b69e022ffc9331f28fdd46.webp 760w,
               /post/paper-lomo/lomo_structure_hue5e8f1c01a26b4b030fc1d74546bd49e_76805_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-lomo/lomo_structure_hue5e8f1c01a26b4b030fc1d74546bd49e_76805_90fa04cb99324adf5342fc0c2d8bb44f.webp&#34;
               width=&#34;760&#34;
               height=&#34;222&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;提取到的图像特征首先经过Direct Regressor得到粗检测结果。这里DR的设计与EAST基本相同，包括test/non-text分类结果和四个边界点的偏移值。在训练分类结果时使用了一种改进的Dice-Loss。其中的权重$w$被设置为与文本框的短边长度称反比。
$$L_{cls} = 1 - \frac{2 * sum(y * \hat{y} * w)}{sum(y*w) + sum(\hat{y} * w)}$$&lt;/p&gt;
&lt;p&gt;IRM的结构如下。首先根据DR的结果，从FPN的特征中使用ROI Transform提取特征，并使用卷积和Sigmoid激活得到四个通道的注意力图（分别为四个边角）。通过Reduce-Sum和卷积得到四个边角的偏移量。对DR的结果进行修正。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;lomo_irm&#34; srcset=&#34;
               /post/paper-lomo/lomo_irm_hua6a8cc7fafa855021711f1ab194f25b8_224730_030a8c2816d9869d78572a4437806822.webp 400w,
               /post/paper-lomo/lomo_irm_hua6a8cc7fafa855021711f1ab194f25b8_224730_1de9dc4e39bd20eab806ddac74fa802e.webp 760w,
               /post/paper-lomo/lomo_irm_hua6a8cc7fafa855021711f1ab194f25b8_224730_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-lomo/lomo_irm_hua6a8cc7fafa855021711f1ab194f25b8_224730_030a8c2816d9869d78572a4437806822.webp&#34;
               width=&#34;760&#34;
               height=&#34;408&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;IRM部分的损失函数为使用Smoothed-L1来监督。
$$L_{irm} = \frac{1}{K*8}\sum\limits_{k=1}^{K}\sum\limits_{j=1}^{8}smooth_{L1}(c_k^j - \hat{c}_k^j)$$&lt;/p&gt;
&lt;p&gt;SEM的结构如下。根据IRM修正之后的结果，进一步得到多边形框的结果。除去上述检测框中的背景部分。首先同样是使用ROI Transform提取特征，然后经过两次上采样之后，再生成预测结果，包括文本区域、文本中心线（收缩后的文本区域）、边界偏置。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;lomo_sem&#34; srcset=&#34;
               /post/paper-lomo/lomo_sem_hu197b4a28fc238235005fe279f206a35c_463791_dcab21e3b7cb6bbd41effc2d60e9e701.webp 400w,
               /post/paper-lomo/lomo_sem_hu197b4a28fc238235005fe279f206a35c_463791_c500b747ebe86e80595781b39993d294.webp 760w,
               /post/paper-lomo/lomo_sem_hu197b4a28fc238235005fe279f206a35c_463791_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-lomo/lomo_sem_hu197b4a28fc238235005fe279f206a35c_463791_dcab21e3b7cb6bbd41effc2d60e9e701.webp&#34;
               width=&#34;760&#34;
               height=&#34;334&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;要想得到文本检测结果，需要首先在文本中心线上采样N个点，然后根据边界偏置得到文本行的上下边界上的控制点，相连接得到多边形框。&lt;/p&gt;
&lt;p&gt;在训练时，首先使用生成数据对DR部分进行训练，然后才使用真实数据同时训练三个分支。为了防止训练时DR产生的错误结果影响另外两分支的训练，将DR结果中的50%随机替换为GT。在预测的时候DR的结果经过NMS之后再经过多次IRM，最后通过SEM得到结果。&lt;/p&gt;
&lt;h2 id=&#34;结果分析&#34;&gt;结果分析&lt;/h2&gt;
&lt;p&gt;ablation study显示，IRM迭代次数增加可以提升模型性能，但是相应的处理时间增加，因此权衡之后设置为2。在IRM中，添加四个角点的注意力图也可以提升模型性能。&lt;/p&gt;
&lt;p&gt;ablation study显示，添加SEM可以显著提升模型性能（7.17%）。对于SEM结果的处理中，采样点数量增加效果提升并逐渐收敛，因此选择为7。&lt;/p&gt;
&lt;p&gt;在长文本数据集ICDAR2017-RCTW上的效果如下。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;lomo_exp&#34; srcset=&#34;
               /post/paper-lomo/lomo_exp_hue6cf532ef9bb217ee46ab7eb14485d58_87274_c25b22cd1421810eef90d6091023cd11.webp 400w,
               /post/paper-lomo/lomo_exp_hue6cf532ef9bb217ee46ab7eb14485d58_87274_013795f36ed5a8900db90377dbb84780.webp 760w,
               /post/paper-lomo/lomo_exp_hue6cf532ef9bb217ee46ab7eb14485d58_87274_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-lomo/lomo_exp_hue6cf532ef9bb217ee46ab7eb14485d58_87274_c25b22cd1421810eef90d6091023cd11.webp&#34;
               width=&#34;760&#34;
               height=&#34;369&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;在弯曲文本数据集ICDAR2015上的效果如下。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;lomo_exp_ic15&#34; srcset=&#34;
               /post/paper-lomo/lomo_exp_ic15_hu6ba3ca2b944b614b37850df321849ae3_197529_368999009f1fcda3df853bd004c83b20.webp 400w,
               /post/paper-lomo/lomo_exp_ic15_hu6ba3ca2b944b614b37850df321849ae3_197529_550e6de229affa5bc7a23a582e8b28b9.webp 760w,
               /post/paper-lomo/lomo_exp_ic15_hu6ba3ca2b944b614b37850df321849ae3_197529_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-lomo/lomo_exp_ic15_hu6ba3ca2b944b614b37850df321849ae3_197529_368999009f1fcda3df853bd004c83b20.webp&#34;
               width=&#34;760&#34;
               height=&#34;735&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;在多语言数据集ICDAR2017-MLT上的效果如下。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;lomo_exp_ic17&#34; srcset=&#34;
               /post/paper-lomo/lomo_exp_ic17_hu48cfa2842547b2094ad750e232ad9020_114588_31408eaa332c57853f96034bb042dbfa.webp 400w,
               /post/paper-lomo/lomo_exp_ic17_hu48cfa2842547b2094ad750e232ad9020_114588_12907b4145ded085e0c5feb464cdef92.webp 760w,
               /post/paper-lomo/lomo_exp_ic17_hu48cfa2842547b2094ad750e232ad9020_114588_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-lomo/lomo_exp_ic17_hu48cfa2842547b2094ad750e232ad9020_114588_31408eaa332c57853f96034bb042dbfa.webp&#34;
               width=&#34;760&#34;
               height=&#34;550&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;设计了IRM在粗检测结果上精调。
设计了SEM实现文本框形状的优化，从四边形变换为任意形状。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>【论文】Shape Robust Text Detection with Progressive Scale Expansion Network</title>
      <link>https://yangleisx.github.io/post/paper-psenet/</link>
      <pubDate>Thu, 19 Nov 2020 10:55:55 +0800</pubDate>
      <guid>https://yangleisx.github.io/post/paper-psenet/</guid>
      <description>&lt;p&gt;论文题目：Shape Robust Text Detection with Progressive Scale Expansion Network&lt;/p&gt;
&lt;p&gt;作者：Wenhai Wang, Enze Xie, Xiang Li, Wenbo Hou, Tong Lu, Gang Yu, Shuai Shao&lt;/p&gt;
&lt;p&gt;会议/时间：CVPR2019&lt;/p&gt;
&lt;p&gt;链接：&lt;a href=&#34;https://arxiv.org/pdf/1903.12473.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arxiv&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;论文目标&#34;&gt;论文目标&lt;/h2&gt;
&lt;p&gt;在当前的文字检测算法在应用时有两个问题：当前的算法通常得到一个四边形边界框，难以检测任意形状的文本；如果两行文本距离比较近，有可能会被框选为同一个边界框。因此提出了PSENet，可以有效的解决上述的两个问题。&lt;/p&gt;
&lt;h2 id=&#34;相关工作&#34;&gt;相关工作&lt;/h2&gt;
&lt;p&gt;现有的基于CNN的文本检测模型可以分为两类：基于回归的方案和基于分割的方案。前者生成四边形的边界框，无法处理任意形状的文本，后者使用像素级的分类得到目标区域，但是很难区分开相聚比较近的目标。&lt;/p&gt;
&lt;p&gt;基于回归的文本检测方案大多是基于通用的目标检测模型，包括Faster R-CNN等。其他的文本检测模型还有TextBoxes、EAST等。大多数这一类的模型都需要设计Anchor而且由多个处理阶段组成，可能会导致性能比较差。基于分割的文本检测方案主要使用FCN，例如通过FCN获得热力图等，再进行后处理获得文本位置。&lt;/p&gt;
&lt;h2 id=&#34;本文思路解决方案&#34;&gt;本文思路/解决方案&lt;/h2&gt;
&lt;p&gt;本文提出的PSENet是基于分割的方案，每一个预测的分割称为kernel，形状相似但是大小不同，最后设计了一个基于BFS的渐进扩展算法，将原本的kernel扩展得到最终的预测分割。由于使用渐近扩展式的算法，对于最小尺寸的kernel，可以区分开距离较近的文本，同时也可以解决小尺寸分割难以覆盖完整文本的问题。&lt;/p&gt;
&lt;p&gt;模型结构是基于ResNet的FPN结构，从中选取不同大小的特征图连接得到混合特征图，最后通过卷积等操作得到不同尺寸的多个分割图，再经过尺寸扩展得到预测结果。&lt;/p&gt;
&lt;!-- ![pipeline](pipeline.png) --&gt;
&lt;p&gt;在这个尺寸扩展算法中，首先选择了尺寸最小的分割图进行连通域分析作为kernel，然后将其他的分割图作为输入，通过Scale Expansion算法计算新的扩展后的kernel，最终得到结果。作者提到对于位于多个文本之间的像素，使用先来先服务的方式合并到不同的标签中，同时由于使用了渐进式的方法，这些边界上的重合并不会影响最终的处理结果，这可能也是作者选择多个不同尺寸的分割图渐近处理的原因。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;algorithm&#34; srcset=&#34;
               /post/paper-psenet/algorithm_hud4cda9c211fdf528e67360bca4084d03_138031_d0c1d0e711bbff73edf17c946f5196ad.webp 400w,
               /post/paper-psenet/algorithm_hud4cda9c211fdf528e67360bca4084d03_138031_6a4e39df643c92d65b747a3533f377a0.webp 760w,
               /post/paper-psenet/algorithm_hud4cda9c211fdf528e67360bca4084d03_138031_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://yangleisx.github.io/post/paper-psenet/algorithm_hud4cda9c211fdf528e67360bca4084d03_138031_d0c1d0e711bbff73edf17c946f5196ad.webp&#34;
               width=&#34;760&#34;
               height=&#34;564&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;在训练过程中，为了获得不同尺寸的分割图，需要提供对应的标签供学习，作者使用了Vatti clipping algorithm来将最初的文本框标签收缩一定的像素得到这些标签。算法中使用到的像素值通过下面的公式计算得到。其中m为最小的缩放比例，n为不同尺寸的分割图的个数。&lt;/p&gt;



$$\begin{aligned}
d_i = \frac{\mathrm{Area}(p_n)\times(1-r_i^2)}{\mathrm{Perimeter}(p_n)}\notag\\
r_i = 1 - \frac{(1-m)\times(n-i)}{n-1}
\end{aligned}$$

&lt;p&gt;在实验中作者使用了Dice Coefficient作为模型的评价指标，使用了完整尺寸的标签和缩放后的标签上的Dice系数作为损失函数来指导模型的学习。其中对于完整尺寸的标签，使用Online Hard Example Mining(OHEM)来获得一个mask协助训练。



$$\begin{aligned}
L &amp;= \lambda L_c + (1-\lambda)L_s\notag\\
L_c &amp;= 1 - D(S_n\cdot M, G_n \cdot M)\notag\\
L_s &amp;= 1 - \frac{\sum\limits^{n-1} D(S_i\cdot W, G_i \cdot W)}{n-1}\notag\\
W_{x,y} &amp;= \left\{
\begin{align}
1,&amp; \quad if\ S_{n,x,y} \geq0.5;\notag\\
0,&amp; \quad otherwise\notag\\
\end{align}
\right.
\end{aligned}$$
&lt;/p&gt;
&lt;h2 id=&#34;结果&#34;&gt;结果&lt;/h2&gt;
&lt;p&gt;实验包括了四个数据集：CTW1500、Total-Text、ICDAR2015和ICDAR2017MLT。模型使用预训练好的ResNet，在IC17-MLT上训练，而且没有使用另外的人造数据集。实验讨论的结果包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;最小尺寸的kernel并不能直接作为模型的输出，模型的F-measure结果比较差，而且文本框内容的识别结果也比较差。&lt;/li&gt;
&lt;li&gt;对于最小缩放比例m的选择，选择太大或者太小都会导致性能的下降。&lt;/li&gt;
&lt;li&gt;分割图的个数n增加时，性能会有一定的上升，但是不能无限制增大，在n大于5之后性能提升不大。&lt;/li&gt;
&lt;li&gt;修改模型骨架，例如增加ResNet的层数也会提升模型的性能。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;使用了基于FPN的结构，将不同尺度的特征图上采样并连接在一起。&lt;/p&gt;
&lt;p&gt;获得不同尺度下的分割图，再从小到大渐进式的合并，不仅可以检测到任意形状的文本，也可以避免将距离比较近的文本识别为同一对象。&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
